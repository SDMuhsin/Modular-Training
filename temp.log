Generating data for encoder 0
BertModel loaded from: transformers.models.bert.modeling_bert
Transformers module loaded from: /media/a40-ko-lab/secondary/sayed/Sayed/torch_sayed/lib/python3.10/site-packages/transformers/__init__.py
12/04/2024 11:51:58 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False
12/04/2024 11:51:58 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=no,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/tmp/cb/runs/Dec04_11-51-58_a40kolab,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1.0,
optim=adamw_torch,
optim_args=None,
optim_target_modules=None,
output_dir=/tmp/cb/,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=/tmp/cb/,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
Loaded dataset from disk
Data augmentation: 0/250Memory Usage: 572.7265625 MB
Data augmentation: 1/250Data augmentation: 2/250Data augmentation: 3/250Data augmentation: 4/250Data augmentation: 5/250Data augmentation: 6/250Data augmentation: 7/250Data augmentation: 8/250Data augmentation: 9/250Data augmentation: 10/250Data augmentation: 11/250Data augmentation: 12/250Data augmentation: 13/250Data augmentation: 14/250Data augmentation: 15/250Data augmentation: 16/250Data augmentation: 17/250Data augmentation: 18/250Data augmentation: 19/250Data augmentation: 20/250Data augmentation: 21/250Data augmentation: 22/250Data augmentation: 23/250Data augmentation: 24/250Data augmentation: 25/250Data augmentation: 26/250Data augmentation: 27/250Data augmentation: 28/250Data augmentation: 29/250Data augmentation: 30/250Data augmentation: 31/250Data augmentation: 32/250Data augmentation: 33/250Data augmentation: 34/250Data augmentation: 35/250Data augmentation: 36/250Data augmentation: 37/250Data augmentation: 38/250Data augmentation: 39/250Data augmentation: 40/250Data augmentation: 41/250Data augmentation: 42/250Data augmentation: 43/250Data augmentation: 44/250Data augmentation: 45/250Data augmentation: 46/250Data augmentation: 47/250Data augmentation: 48/250Data augmentation: 49/250Data augmentation: 50/250Data augmentation: 51/250Data augmentation: 52/250Data augmentation: 53/250Data augmentation: 54/250Data augmentation: 55/250Data augmentation: 56/250Data augmentation: 57/250Data augmentation: 58/250Data augmentation: 59/250Data augmentation: 60/250Data augmentation: 61/250Data augmentation: 62/250Data augmentation: 63/250Data augmentation: 64/250Data augmentation: 65/250Data augmentation: 66/250Data augmentation: 67/250Data augmentation: 68/250Data augmentation: 69/250Data augmentation: 70/250Data augmentation: 71/250Data augmentation: 72/250Data augmentation: 73/250Data augmentation: 74/250Data augmentation: 75/250Data augmentation: 76/250Data augmentation: 77/250Data augmentation: 78/250Data augmentation: 79/250Data augmentation: 80/250Data augmentation: 81/250Data augmentation: 82/250Data augmentation: 83/250Data augmentation: 84/250Data augmentation: 85/250Data augmentation: 86/250Data augmentation: 87/250Data augmentation: 88/250Data augmentation: 89/250Data augmentation: 90/250Data augmentation: 91/250Data augmentation: 92/250Data augmentation: 93/250Data augmentation: 94/250Data augmentation: 95/250Data augmentation: 96/250Data augmentation: 97/250Data augmentation: 98/250Data augmentation: 99/250Data augmentation: 100/250Memory Usage: 802.17578125 MB
Data augmentation: 101/250Data augmentation: 102/250Data augmentation: 103/250Data augmentation: 104/250Data augmentation: 105/250Data augmentation: 106/250Data augmentation: 107/250Data augmentation: 108/250Data augmentation: 109/250Data augmentation: 110/250Data augmentation: 111/250Data augmentation: 112/250Data augmentation: 113/250Data augmentation: 114/250Data augmentation: 115/250Data augmentation: 116/250Data augmentation: 117/250Data augmentation: 118/250Data augmentation: 119/250Data augmentation: 120/250Data augmentation: 121/250Data augmentation: 122/250Data augmentation: 123/250Data augmentation: 124/250Data augmentation: 125/250Data augmentation: 126/250Data augmentation: 127/250Data augmentation: 128/250Data augmentation: 129/250Data augmentation: 130/250Data augmentation: 131/250Data augmentation: 132/250Data augmentation: 133/250Data augmentation: 134/250Data augmentation: 135/250Data augmentation: 136/250Data augmentation: 137/250Data augmentation: 138/250Data augmentation: 139/250Data augmentation: 140/250Data augmentation: 141/250Data augmentation: 142/250Data augmentation: 143/250Data augmentation: 144/250Data augmentation: 145/250Data augmentation: 146/250Data augmentation: 147/250Data augmentation: 148/250Data augmentation: 149/250Data augmentation: 150/250Data augmentation: 151/250Data augmentation: 152/250Data augmentation: 153/250Data augmentation: 154/250Data augmentation: 155/250Data augmentation: 156/250Data augmentation: 157/250Data augmentation: 158/250Data augmentation: 159/250Data augmentation: 160/250Data augmentation: 161/250Data augmentation: 162/250Data augmentation: 163/250Data augmentation: 164/250Data augmentation: 165/250Data augmentation: 166/250Data augmentation: 167/250Data augmentation: 168/250Data augmentation: 169/250Data augmentation: 170/250Data augmentation: 171/250Data augmentation: 172/250Data augmentation: 173/250Data augmentation: 174/250Data augmentation: 175/250Data augmentation: 176/250Data augmentation: 177/250Data augmentation: 178/250Data augmentation: 179/250Data augmentation: 180/250Data augmentation: 181/250Data augmentation: 182/250Data augmentation: 183/250Data augmentation: 184/250Data augmentation: 185/250Data augmentation: 186/250Data augmentation: 187/250Data augmentation: 188/250Data augmentation: 189/250Data augmentation: 190/250Data augmentation: 191/250Data augmentation: 192/250Data augmentation: 193/250Data augmentation: 194/250Data augmentation: 195/250Data augmentation: 196/250Data augmentation: 197/250Data augmentation: 198/250Data augmentation: 199/250Data augmentation: 200/250Memory Usage: 802.55078125 MB
Data augmentation: 201/250Data augmentation: 202/250Data augmentation: 203/250Data augmentation: 204/250Data augmentation: 205/250Data augmentation: 206/250Data augmentation: 207/250Data augmentation: 208/250Data augmentation: 209/250Data augmentation: 210/250Data augmentation: 211/250Data augmentation: 212/250Data augmentation: 213/250Data augmentation: 214/250Data augmentation: 215/250Data augmentation: 216/250Data augmentation: 217/250Data augmentation: 218/250Data augmentation: 219/250Data augmentation: 220/250Data augmentation: 221/250Data augmentation: 222/250Data augmentation: 223/250Data augmentation: 224/250Data augmentation: 225/250Data augmentation: 226/250Data augmentation: 227/250Data augmentation: 228/250Data augmentation: 229/250Data augmentation: 230/250Data augmentation: 231/250Data augmentation: 232/250Data augmentation: 233/250Data augmentation: 234/250Data augmentation: 235/250Data augmentation: 236/250Data augmentation: 237/250Data augmentation: 238/250Data augmentation: 239/250Data augmentation: 240/250Data augmentation: 241/250Data augmentation: 242/250Data augmentation: 243/250Data augmentation: 244/250Data augmentation: 245/250Data augmentation: 246/250Data augmentation: 247/250Data augmentation: 248/250Data augmentation: 249/250Augmented data saved to file.
LOAD FROM SAVE
12/04/2024 11:52:36 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /media/a40-ko-lab/secondary/sayed/Sayed/transformers/weight_sharing/modular_superglue/downloads/cb_distilbert-base-uncased/validation/cache-5eb9c8a59a7c060e.arrow
12/04/2024 11:52:36 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /media/a40-ko-lab/secondary/sayed/Sayed/transformers/weight_sharing/modular_superglue/downloads/cb_distilbert-base-uncased/test/cache-349186c77b2f4601.arrow
12/04/2024 11:52:36 - INFO - __main__ - Sample 25 of the training set: {'input_ids': [101, 1036, 1036, 2017, 1005, 2310, 2657, 2242, 2843, 2008, 2077, 1012, 4033, 1005, 1056, 2725, 1010, 5261, 1029, 1005, 1005, 5261, 2018, 2042, 16697, 2011, 2216, 4165, 2062, 16697, 2084, 1996, 2500, 2005, 2204, 3114, 2021, 10149, 2936, 2156, 2008, 2002, 2001, 4895, 28139, 19362, 2098, 2000, 2069, 2009, 2004, 2002, 3724, 2370, 2185, 2013, 1996, 7684, 4675, 1012, 102, 5261, 2001, 4895, 28139, 19362, 2098, 2000, 2265, 2002, 2018, 2042, 16697, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
HERE AS INTENDED
12/04/2024 11:52:36 - INFO - __main__ - *** Evaluate ***
Directory './saves/distilbert/distilbert-base-uncased/cb/mha/outputs/encoder_0' created successfully.
Directory './saves/distilbert/distilbert-base-uncased/cb/ffn/inputs/encoder_0' created successfully.
Directory './saves/distilbert/distilbert-base-uncased/cb/ffn/outputs/encoder_0' created successfully.
Directory './saves/distilbert/distilbert-base-uncased/cb/mha/inputs/encoder_0' created successfully.
***** eval metrics *****
  eval_model_preparation_time =     0.0014
  eval_runtime                = 0:00:01.52
  eval_samples                =        499
  eval_samples_per_second     =    326.204
  eval_steps_per_second       =     41.184
Data generated succesfully

 
 
 Beginning modular training of SA and BL modules
Encoder compression: 2
Modular training for SA module, encoder 0
LOAD CONFIG FROM SAVE
Encoder idx = 0
Batch count :  63
Memory Usage: 673.796875 MB
INPUT SHAPE torch.Size([8, 128, 768]) torch.Size([8, 1, 128, 128])
Modular training for BL module, encoder 0
LOAD CONFIG FROM SAVE
Model :  distilbert/distilbert-base-uncased
