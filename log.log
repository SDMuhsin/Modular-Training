rm: cannot remove './saves/cb_augmented_dataset.pkl': No such file or directory
rm: cannot remove './saves/distilbert/distilbert-base-uncased/cb/*': No such file or directory
Generating data for encoder 0
[nltk_data] Downloading package stopwords to /home/a40-ko-
[nltk_data]     lab/nltk_data...
[nltk_data]   Package stopwords is already up-to-date!
BertModel loaded from: transformers.models.bert.modeling_bert
Transformers module loaded from: /home/a40-ko-lab/miniconda3/envs/jiantenv/lib/python3.9/site-packages/transformers/__init__.py
06/12/2024 13:38:11 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False
06/12/2024 13:38:11 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
batch_eval_metrics=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_steps=None,
eval_strategy=no,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/tmp/cb/runs/Jun12_13-38-11_a40kolab,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=100.0,
optim=adamw_torch,
optim_args=None,
optim_target_modules=None,
output_dir=/tmp/cb/,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=/tmp/cb/,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
[INFO|configuration_utils.py:731] 2024-06-12 13:38:11,686 >> loading configuration file ./downloads/cb_config/config.json
[INFO|configuration_utils.py:800] 2024-06-12 13:38:11,687 >> Model config DistilBertConfig {
  "_name_or_path": "./downloads/cb_config",
  "activation": "gelu",
  "architectures": [
    "DistilBertForMaskedLM"
  ],
  "attention_dropout": 0.1,
  "dim": 768,
  "dropout": 0.1,
  "finetuning_task": "cb",
  "hidden_dim": 3072,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 512,
  "model_type": "distilbert",
  "n_heads": 12,
  "n_layers": 6,
  "pad_token_id": 0,
  "qa_dropout": 0.1,
  "seq_classif_dropout": 0.2,
  "sinusoidal_pos_embds": false,
  "tie_weights_": true,
  "transformers_version": "4.42.0.dev0",
  "vocab_size": 30522
}

[INFO|tokenization_utils_base.py:2107] 2024-06-12 13:38:11,688 >> loading file vocab.txt
[INFO|tokenization_utils_base.py:2107] 2024-06-12 13:38:11,688 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2107] 2024-06-12 13:38:11,688 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2107] 2024-06-12 13:38:11,688 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2107] 2024-06-12 13:38:11,688 >> loading file tokenizer_config.json
[INFO|configuration_utils.py:731] 2024-06-12 13:38:11,703 >> loading configuration file ./downloads/cb_model/config.json
[INFO|configuration_utils.py:800] 2024-06-12 13:38:11,704 >> Model config DistilBertConfig {
  "_name_or_path": "./downloads/cb_model",
  "activation": "gelu",
  "architectures": [
    "DistilBertForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "dim": 768,
  "dropout": 0.1,
  "finetuning_task": "cb",
  "hidden_dim": 3072,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 512,
  "model_type": "distilbert",
  "n_heads": 12,
  "n_layers": 6,
  "pad_token_id": 0,
  "qa_dropout": 0.1,
  "seq_classif_dropout": 0.2,
  "sinusoidal_pos_embds": false,
  "tie_weights_": true,
  "torch_dtype": "float32",
  "transformers_version": "4.42.0.dev0",
  "vocab_size": 30522
}

[INFO|modeling_utils.py:3474] 2024-06-12 13:38:11,729 >> loading weights file ./downloads/cb_model/model.safetensors
[INFO|modeling_utils.py:4283] 2024-06-12 13:38:11,766 >> All model checkpoint weights were used when initializing DistilBertForSequenceClassification.

[INFO|modeling_utils.py:4291] 2024-06-12 13:38:11,767 >> All the weights of DistilBertForSequenceClassification were initialized from the model checkpoint at ./downloads/cb_model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForSequenceClassification for predictions without further training.
Loading cached processed dataset at /media/a40-ko-lab/c61d4ea0-b8fe-4205-9815-f7d3d054043c/sayed/Sayed/transformers/weight_sharing/modular_superglue/downloads/cb/train/cache-ec9a28bf6cd38919.arrow
Loaded dataset from disk
06/12/2024 13:38:11 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /media/a40-ko-lab/c61d4ea0-b8fe-4205-9815-f7d3d054043c/sayed/Sayed/transformers/weight_sharing/modular_superglue/downloads/cb/train/cache-ec9a28bf6cd38919.arrow
Loading cached processed dataset at /media/a40-ko-lab/c61d4ea0-b8fe-4205-9815-f7d3d054043c/sayed/Sayed/transformers/weight_sharing/modular_superglue/downloads/cb/validation/cache-462a1cd6bd454452.arrow
06/12/2024 13:38:11 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /media/a40-ko-lab/c61d4ea0-b8fe-4205-9815-f7d3d054043c/sayed/Sayed/transformers/weight_sharing/modular_superglue/downloads/cb/validation/cache-462a1cd6bd454452.arrow
Loading cached processed dataset at /media/a40-ko-lab/c61d4ea0-b8fe-4205-9815-f7d3d054043c/sayed/Sayed/transformers/weight_sharing/modular_superglue/downloads/cb/test/cache-f44364b0f28c5bd9.arrow
06/12/2024 13:38:11 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /media/a40-ko-lab/c61d4ea0-b8fe-4205-9815-f7d3d054043c/sayed/Sayed/transformers/weight_sharing/modular_superglue/downloads/cb/test/cache-f44364b0f28c5bd9.arrow
06/12/2024 13:38:11 - INFO - __main__ - Sample 163 of the training set: {'input_ids': [101, 1037, 1024, 2031, 2017, 2628, 2008, 2200, 2172, 2030, 1010, 1038, 1024, 7910, 1010, 2025, 2428, 1012, 1045, 2123, 1005, 1056, 2228, 2505, 2097, 2412, 2202, 2058, 1996, 5088, 1012, 102, 2242, 2097, 2202, 2058, 1996, 5088, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
06/12/2024 13:38:11 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:3753] 2024-06-12 13:38:11,998 >> ***** Running Evaluation *****
[INFO|trainer.py:3755] 2024-06-12 13:38:11,998 >>   Num examples = 250
[INFO|trainer.py:3758] 2024-06-12 13:38:11,998 >>   Batch size = 8
Directory './saves/distilbert/distilbert-base-uncased/cb/mha/outputs/encoder_0' created successfully.
Directory './saves/distilbert/distilbert-base-uncased/cb/ffn/inputs/encoder_0' created successfully.
Directory './saves/distilbert/distilbert-base-uncased/cb/ffn/outputs/encoder_0' created successfully.
Directory './saves/distilbert/distilbert-base-uncased/cb/mha/inputs/encoder_0' created successfully.
Batch Idx  1
  0%|          | 0/32 [00:00<?, ?it/s] 16%|█▌        | 5/32 [00:00<00:00, 46.95it/s] 31%|███▏      | 10/32 [00:00<00:00, 43.25it/s] 47%|████▋     | 15/32 [00:00<00:00, 41.98it/s] 62%|██████▎   | 20/32 [00:00<00:00, 41.75it/s] 78%|███████▊  | 25/32 [00:00<00:00, 41.76it/s] 94%|█████████▍| 30/32 [00:00<00:00, 41.79it/s]100%|██████████| 32/32 [00:00<00:00, 42.89it/s]
Batch Idx  2
Batch Idx  3
Batch Idx  4
Batch Idx  5
Batch Idx  6
Batch Idx  7
Batch Idx  8
Batch Idx  9
Batch Idx  10
Batch Idx  11
Batch Idx  12
Batch Idx  13
Batch Idx  14
Batch Idx  15
Batch Idx  16
Batch Idx  17
Batch Idx  18
Batch Idx  19
Batch Idx  20
Batch Idx  21
Batch Idx  22
Batch Idx  23
Batch Idx  24
Batch Idx  25
Batch Idx  26
Batch Idx  27
Batch Idx  28
Batch Idx  29
Batch Idx  30
Batch Idx  31
Batch Idx  32
***** eval metrics *****
  eval_runtime            = 0:00:00.86
  eval_samples            =        250
  eval_samples_per_second =    290.326
  eval_steps_per_second   =     37.162
Data generated succesfully

 
 
 Beginning modular training of SA and BL modules
Encoder compression: 2
Modular training for BL module, encoder 0
LOAD FROM SAVE
Encoder idx = 0
Compression = 2
[FFN] Epoch 1, Loss: 129.42950469255447 = 77.05173683166504 (normal) + 52.377768114209175 (augmented)  
[FFN] Epoch 2, Loss: 68.81933563947678 = 40.77201247215271 (normal) + 28.04732248187065 (augmented)  
[FFN] Epoch 3, Loss: 39.89170449972153 = 22.32439061999321 (normal) + 17.567313492298126 (augmented)  
[FFN] Epoch 4, Loss: 33.194395542144775 = 18.521724551916122 (normal) + 14.672671139240265 (augmented)  
[FFN] Epoch 5, Loss: 30.8129985332489 = 17.27885267138481 (normal) + 13.534146189689636 (augmented)  
[FFN] Epoch 6, Loss: 29.56459927558899 = 16.50253063440323 (normal) + 13.062068283557892 (augmented)  
[FFN] Epoch 7, Loss: 27.98804199695587 = 16.073400795459747 (normal) + 11.914640873670578 (augmented)  
[FFN] Epoch 8, Loss: 26.686600118875504 = 15.211931973695755 (normal) + 11.474667966365814 (augmented)  
[FFN] Epoch 9, Loss: 26.283439248800278 = 15.080172836780548 (normal) + 11.203266337513924 (augmented)  
[FFN] Epoch 10, Loss: 25.41560998558998 = 14.734341830015182 (normal) + 10.681267976760864 (augmented)  
[FFN] Epoch 11, Loss: 24.740198254585266 = 14.762259751558304 (normal) + 9.977938309311867 (augmented)  
[FFN] Epoch 12, Loss: 23.8624125123024 = 14.227492272853851 (normal) + 9.634920194745064 (augmented)  
[FFN] Epoch 13, Loss: 23.46468037366867 = 14.04636025428772 (normal) + 9.418319910764694 (augmented)  
[FFN] Epoch 14, Loss: 23.255942225456238 = 14.091200739145279 (normal) + 9.164741523563862 (augmented)  
[FFN] Epoch 15, Loss: 22.703130275011063 = 13.74185959994793 (normal) + 8.961270593106747 (augmented)  
[FFN] Epoch 16, Loss: 22.327528923749924 = 13.629991054534912 (normal) + 8.697537697851658 (augmented)  
[FFN] Epoch 17, Loss: 22.293012976646423 = 13.753617033362389 (normal) + 8.539395771920681 (augmented)  
[FFN] Epoch 18, Loss: 21.732681095600128 = 13.361189037561417 (normal) + 8.371491901576519 (augmented)  
[FFN] Epoch 19, Loss: 21.77712470293045 = 13.418856799602509 (normal) + 8.358267828822136 (augmented)  
[FFN] Epoch 20, Loss: 21.311523526906967 = 13.247621804475784 (normal) + 8.063901610672474 (augmented)  
[FFN] Epoch 21, Loss: 21.29410746693611 = 13.132390454411507 (normal) + 8.161716714501381 (augmented)  
[FFN] Epoch 22, Loss: 20.88348960876465 = 12.94844065606594 (normal) + 7.935048773884773 (augmented)  
[FFN] Epoch 23, Loss: 20.95240905880928 = 12.979066967964172 (normal) + 7.973342061042786 (augmented)  
[FFN] Epoch 24, Loss: 20.853995084762573 = 12.803801357746124 (normal) + 8.050193816423416 (augmented)  
[FFN] Epoch 25, Loss: 20.680239260196686 = 12.840988472104073 (normal) + 7.839250758290291 (augmented)  
[FFN] Epoch 26, Loss: 20.813032746315002 = 12.945084810256958 (normal) + 7.8679478615522385 (augmented)  
[FFN] Epoch 27, Loss: 20.736449509859085 = 12.832794606685638 (normal) + 7.903654865920544 (augmented)  
[FFN] Epoch 28, Loss: 20.46631094813347 = 12.803630590438843 (normal) + 7.66268016397953 (augmented)  
[FFN] Epoch 29, Loss: 20.651607424020767 = 12.816188871860504 (normal) + 7.835418365895748 (augmented)  
[FFN] Epoch 30, Loss: 20.37329238653183 = 12.655654892325401 (normal) + 7.717637352645397 (augmented)  
[FFN] Epoch 31, Loss: 20.830873876810074 = 13.09434188902378 (normal) + 7.736531853675842 (augmented)  
[FFN] Epoch 32, Loss: 20.39597874879837 = 12.618247464299202 (normal) + 7.777731202542782 (augmented)  
[FFN] Epoch 33, Loss: 20.454594641923904 = 12.771375104784966 (normal) + 7.683219626545906 (augmented)  
[FFN] Epoch 34, Loss: 20.31783202290535 = 12.700286135077477 (normal) + 7.617545880377293 (augmented)  
[FFN] Epoch 35, Loss: 19.91686150431633 = 12.27012850344181 (normal) + 7.646733067929745 (augmented)  
[FFN] Epoch 36, Loss: 20.07351067662239 = 12.333456814289093 (normal) + 7.740053683519363 (augmented)  
[FFN] Epoch 37, Loss: 20.165366291999817 = 12.489546194672585 (normal) + 7.6758201122283936 (augmented)  
[FFN] Epoch 38, Loss: 20.21614983677864 = 12.644745111465454 (normal) + 7.571404792368412 (augmented)  
[FFN] Epoch 39, Loss: 19.806128978729248 = 12.589588955044746 (normal) + 7.216540031135082 (augmented)  
[FFN] Epoch 40, Loss: 19.859720468521118 = 12.491547122597694 (normal) + 7.368173189461231 (augmented)  
[FFN] Epoch 41, Loss: 19.854479014873505 = 12.27528291940689 (normal) + 7.579196080565453 (augmented)  
[FFN] Epoch 42, Loss: 19.874446630477905 = 12.47545638680458 (normal) + 7.398989923298359 (augmented)  
[FFN] Epoch 43, Loss: 19.7977397441864 = 12.472490951418877 (normal) + 7.325248874723911 (augmented)  
[FFN] Epoch 44, Loss: 19.45911979675293 = 12.554066777229309 (normal) + 6.905053071677685 (augmented)  
[FFN] Epoch 45, Loss: 19.995905220508575 = 12.604529559612274 (normal) + 7.391375556588173 (augmented)  
[FFN] Epoch 46, Loss: 19.843094915151596 = 12.553232073783875 (normal) + 7.289863001555204 (augmented)  
[FFN] Epoch 47, Loss: 19.455705225467682 = 12.375819683074951 (normal) + 7.079885467886925 (augmented)  
[FFN] Epoch 48, Loss: 19.665035128593445 = 12.195030987262726 (normal) + 7.470004133880138 (augmented)  
[FFN] Epoch 49, Loss: 19.498396396636963 = 12.17755974829197 (normal) + 7.320836655795574 (augmented)  
[FFN] Epoch 50, Loss: 19.72636952996254 = 12.463799193501472 (normal) + 7.262570232152939 (augmented)  
[FFN] Epoch 51, Loss: 19.41310714185238 = 12.192824840545654 (normal) + 7.220282144844532 (augmented)  
[FFN] Epoch 52, Loss: 19.161360919475555 = 12.096014082431793 (normal) + 7.065346717834473 (augmented)  
[FFN] Epoch 53, Loss: 19.687606900930405 = 12.61797308921814 (normal) + 7.069633737206459 (augmented)  
[FFN] Epoch 54, Loss: 19.668410301208496 = 12.373720601201057 (normal) + 7.29468959569931 (augmented)  
[FFN] Epoch 55, Loss: 19.410815864801407 = 12.321050494909286 (normal) + 7.089765198528767 (augmented)  
[FFN] Epoch 56, Loss: 19.968371361494064 = 12.640890255570412 (normal) + 7.327480986714363 (augmented)  
[FFN] Epoch 57, Loss: 19.34487220644951 = 12.258708462119102 (normal) + 7.086163766682148 (augmented)  
[FFN] Epoch 58, Loss: 19.53736288845539 = 12.517678588628769 (normal) + 7.019684325903654 (augmented)  
[FFN] Epoch 59, Loss: 19.125818699598312 = 12.034117326140404 (normal) + 7.091701567173004 (augmented)  
[FFN] Epoch 60, Loss: 19.410818070173264 = 12.262484058737755 (normal) + 7.148333717137575 (augmented)  
[FFN] Epoch 61, Loss: 19.07573403418064 = 12.16567350924015 (normal) + 6.910060454159975 (augmented)  
[FFN] Epoch 62, Loss: 19.170934319496155 = 12.094566330313683 (normal) + 7.076367896050215 (augmented)  
[FFN] Epoch 63, Loss: 19.580991566181183 = 12.456788256764412 (normal) + 7.124203190207481 (augmented)  
[FFN] Epoch 64, Loss: 19.318961322307587 = 12.352029755711555 (normal) + 6.966931667178869 (augmented)  
[FFN] Epoch 65, Loss: 19.241622000932693 = 12.255996480584145 (normal) + 6.985625263303518 (augmented)  
[FFN] Epoch 66, Loss: 19.24437004327774 = 12.254827558994293 (normal) + 6.989542447030544 (augmented)  
[FFN] Epoch 67, Loss: 19.495235323905945 = 12.344371408224106 (normal) + 7.150863654911518 (augmented)  
[FFN] Epoch 68, Loss: 19.20645758509636 = 12.248575553297997 (normal) + 6.957881938666105 (augmented)  
[FFN] Epoch 69, Loss: 19.196895390748978 = 12.175967678427696 (normal) + 7.020927630364895 (augmented)  
[FFN] Epoch 70, Loss: 19.271138668060303 = 12.16861043870449 (normal) + 7.102528244256973 (augmented)  
[FFN] Epoch 71, Loss: 19.184533774852753 = 12.125224649906158 (normal) + 7.0593093521893024 (augmented)  
[FFN] Epoch 72, Loss: 19.335849344730377 = 12.393181696534157 (normal) + 6.942667655646801 (augmented)  
[FFN] Epoch 73, Loss: 18.89253304898739 = 12.048012360930443 (normal) + 6.844520624727011 (augmented)  
[FFN] Epoch 74, Loss: 18.935065358877182 = 12.307493075728416 (normal) + 6.627572178840637 (augmented)  
[FFN] Epoch 75, Loss: 19.579239428043365 = 12.602558314800262 (normal) + 6.976681120693684 (augmented)  
[FFN] Epoch 76, Loss: 19.099234730005264 = 12.145414993166924 (normal) + 6.95381960645318 (augmented)  
[FFN] Epoch 77, Loss: 18.934919968247414 = 12.216465249657631 (normal) + 6.718454737216234 (augmented)  
[FFN] Epoch 78, Loss: 19.163054779171944 = 12.225907295942307 (normal) + 6.937147580087185 (augmented)  
[FFN] Epoch 79, Loss: 19.16735675930977 = 12.296815991401672 (normal) + 6.870540767908096 (augmented)  
[FFN] Epoch 80, Loss: 19.449463814496994 = 12.499484837055206 (normal) + 6.949978936463594 (augmented)  
[FFN] Epoch 81, Loss: 19.254396438598633 = 12.44758489727974 (normal) + 6.806811474263668 (augmented)  
[FFN] Epoch 82, Loss: 19.494257271289825 = 12.725782215595245 (normal) + 6.768475040793419 (augmented)  
[FFN] Epoch 83, Loss: 19.572197139263153 = 12.476230174303055 (normal) + 7.095966927707195 (augmented)  
[FFN] Epoch 84, Loss: 19.013288766145706 = 12.091005966067314 (normal) + 6.922282937914133 (augmented)  
[FFN] Epoch 85, Loss: 19.156621247529984 = 12.19181041419506 (normal) + 6.964810650795698 (augmented)  
[FFN] Epoch 86, Loss: 19.12583714723587 = 12.36219447851181 (normal) + 6.763642478734255 (augmented)  
[FFN] Epoch 87, Loss: 18.9329996407032 = 12.315510883927345 (normal) + 6.6174886003136635 (augmented)  
[FFN] Epoch 88, Loss: 19.22229364514351 = 12.384819954633713 (normal) + 6.83747361600399 (augmented)  
[FFN] Epoch 89, Loss: 19.308050990104675 = 12.515744015574455 (normal) + 6.792306959629059 (augmented)  
[FFN] Epoch 90, Loss: 19.612622916698456 = 12.79428818821907 (normal) + 6.818334560841322 (augmented)  
[FFN] Epoch 91, Loss: 19.298325568437576 = 12.381547838449478 (normal) + 6.916777595877647 (augmented)  
[FFN] Epoch 92, Loss: 19.163802802562714 = 12.317201882600784 (normal) + 6.846600726246834 (augmented)  
[FFN] Epoch 93, Loss: 19.117332354187965 = 12.234492436051369 (normal) + 6.882839851081371 (augmented)  
[FFN] Epoch 94, Loss: 19.180076718330383 = 12.383937925100327 (normal) + 6.796138741075993 (augmented)  
[FFN] Epoch 95, Loss: 18.99706181883812 = 12.260284185409546 (normal) + 6.736777562648058 (augmented)  
[FFN] Epoch 96, Loss: 19.216679364442825 = 12.366789877414703 (normal) + 6.849889308214188 (augmented)  
[FFN] Epoch 97, Loss: 19.20869952440262 = 12.436152532696724 (normal) + 6.772547010332346 (augmented)  
[FFN] Epoch 98, Loss: 19.42450785636902 = 12.512922704219818 (normal) + 6.911585196852684 (augmented)  
[FFN] Epoch 99, Loss: 19.315738081932068 = 12.455197721719742 (normal) + 6.860540397465229 (augmented)  
[FFN] Epoch 100, Loss: 19.08160635828972 = 12.391349717974663 (normal) + 6.6902567483484745 (augmented)  
Training complete.
Modular training for SA module, encoder 0
LOAD FROM SAVE
Encoder idx = 0
Batch count :  32
[MHA]Epoch 1, Loss: 14.014581173658371 = 8.42152114212513 (normal) + 5.593060076236725 (augmented)  
[MHA]Epoch 2, Loss: 10.421929627656937 = 6.560435816645622 (normal) + 3.8614938855171204 (augmented)  
[MHA]Epoch 3, Loss: 8.577285811305046 = 5.575310543179512 (normal) + 3.0019751712679863 (augmented)  
[MHA]Epoch 4, Loss: 7.7538551688194275 = 5.1557761952281 (normal) + 2.5980789735913277 (augmented)  
[MHA]Epoch 5, Loss: 7.374854758381844 = 4.981738835573196 (normal) + 2.3931158520281315 (augmented)  
[MHA]Epoch 6, Loss: 7.132525682449341 = 4.861375354230404 (normal) + 2.271150339394808 (augmented)  
[MHA]Epoch 7, Loss: 6.918399587273598 = 4.7448834255337715 (normal) + 2.1735161170363426 (augmented)  
[MHA]Epoch 8, Loss: 6.711354494094849 = 4.621245443820953 (normal) + 2.0901090279221535 (augmented)  
[MHA]Epoch 9, Loss: 6.509556204080582 = 4.495786912739277 (normal) + 2.0137692652642727 (augmented)  
[MHA]Epoch 10, Loss: 6.311186835169792 = 4.369410790503025 (normal) + 1.9417760223150253 (augmented)  
[MHA]Epoch 11, Loss: 6.126254662871361 = 4.250947490334511 (normal) + 1.875307124108076 (augmented)  
[MHA]Epoch 12, Loss: 5.955055221915245 = 4.143085844814777 (normal) + 1.8119693845510483 (augmented)  
[MHA]Epoch 13, Loss: 5.803425952792168 = 4.045356221497059 (normal) + 1.7580696679651737 (augmented)  
[MHA]Epoch 14, Loss: 5.6619973331689835 = 3.958309717476368 (normal) + 1.7036876007914543 (augmented)  
[MHA]Epoch 15, Loss: 5.5359018594026566 = 3.8794196620583534 (normal) + 1.6564822010695934 (augmented)  
[MHA]Epoch 16, Loss: 5.427556127309799 = 3.8135400637984276 (normal) + 1.6140160448849201 (augmented)  
[MHA]Epoch 17, Loss: 5.326303735375404 = 3.7541871070861816 (normal) + 1.5721165649592876 (augmented)  
[MHA]Epoch 18, Loss: 5.230005726218224 = 3.699712887406349 (normal) + 1.5302927903831005 (augmented)  
[MHA]Epoch 19, Loss: 5.150941289961338 = 3.6509023681282997 (normal) + 1.5000389255583286 (augmented)  
[MHA]Epoch 20, Loss: 5.077020414173603 = 3.607244111597538 (normal) + 1.4697762690484524 (augmented)  
[MHA]Epoch 21, Loss: 5.002823665738106 = 3.5668848752975464 (normal) + 1.4359388090670109 (augmented)  
[MHA]Epoch 22, Loss: 4.938504047691822 = 3.5276180133223534 (normal) + 1.4108860082924366 (augmented)  
[MHA]Epoch 23, Loss: 4.87610137462616 = 3.4949395954608917 (normal) + 1.381161779165268 (augmented)  
[MHA]Epoch 24, Loss: 4.811208017170429 = 3.4573008865118027 (normal) + 1.3539070896804333 (augmented)  
[MHA]Epoch 25, Loss: 4.760398708283901 = 3.426530100405216 (normal) + 1.3338685631752014 (augmented)  
[MHA]Epoch 26, Loss: 4.711921088397503 = 3.399625487625599 (normal) + 1.3122956231236458 (augmented)  
[MHA]Epoch 27, Loss: 4.657596543431282 = 3.3706380277872086 (normal) + 1.2869584932923317 (augmented)  
[MHA]Epoch 28, Loss: 4.6084598153829575 = 3.3412946090102196 (normal) + 1.2671651989221573 (augmented)  
[MHA]Epoch 29, Loss: 4.560759015381336 = 3.3158523365855217 (normal) + 1.2449066303670406 (augmented)  
[MHA]Epoch 30, Loss: 4.515381582081318 = 3.291514627635479 (normal) + 1.2238669525831938 (augmented)  
[MHA]Epoch 31, Loss: 4.4736442267894745 = 3.2696003764867783 (normal) + 1.2040438670665026 (augmented)  
[MHA]Epoch 32, Loss: 4.43026215583086 = 3.2468251734972 (normal) + 1.1834369916468859 (augmented)  
[MHA]Epoch 33, Loss: 4.386312462389469 = 3.2232344821095467 (normal) + 1.1630779411643744 (augmented)  
[MHA]Epoch 34, Loss: 4.3507450968027115 = 3.2031919211149216 (normal) + 1.1475531868636608 (augmented)  
[MHA]Epoch 35, Loss: 4.313245482742786 = 3.1833505406975746 (normal) + 1.1298949122428894 (augmented)  
[MHA]Epoch 36, Loss: 4.273486472666264 = 3.1639833375811577 (normal) + 1.1095031052827835 (augmented)  
[MHA]Epoch 37, Loss: 4.243233315646648 = 3.147518038749695 (normal) + 1.0957152415066957 (augmented)  
[MHA]Epoch 38, Loss: 4.203279674053192 = 3.126072071492672 (normal) + 1.0772075895220041 (augmented)  
[MHA]Epoch 39, Loss: 4.171062298119068 = 3.1102793514728546 (normal) + 1.0607829205691814 (augmented)  
[MHA]Epoch 40, Loss: 4.138493657112122 = 3.0936652943491936 (normal) + 1.0448283571749926 (augmented)  
[MHA]Epoch 41, Loss: 4.100036032497883 = 3.074048288166523 (normal) + 1.0259877126663923 (augmented)  
[MHA]Epoch 42, Loss: 4.074240081012249 = 3.060953199863434 (normal) + 1.0132868606597185 (augmented)  
[MHA]Epoch 43, Loss: 4.042621031403542 = 3.043866142630577 (normal) + 0.9987548869103193 (augmented)  
[MHA]Epoch 44, Loss: 4.017801992595196 = 3.0304824113845825 (normal) + 0.9873195178806782 (augmented)  
[MHA]Epoch 45, Loss: 3.9860978946089745 = 3.016328290104866 (normal) + 0.9697695914655924 (augmented)  
[MHA]Epoch 46, Loss: 3.961316503584385 = 2.9988382533192635 (normal) + 0.9624782241880894 (augmented)  
[MHA]Epoch 47, Loss: 3.929618813097477 = 2.9853330180048943 (normal) + 0.9442857820540667 (augmented)  
[MHA]Epoch 48, Loss: 3.9008454978466034 = 2.9705507084727287 (normal) + 0.9302947670221329 (augmented)  
[MHA]Epoch 49, Loss: 3.874460719525814 = 2.9551442861557007 (normal) + 0.9193164333701134 (augmented)  
[MHA]Epoch 50, Loss: 3.848350875079632 = 2.939893275499344 (normal) + 0.9084575790911913 (augmented)  
[MHA]Epoch 51, Loss: 3.8162539824843407 = 2.9241378605365753 (normal) + 0.8921161033213139 (augmented)  
[MHA]Epoch 52, Loss: 3.7907349690794945 = 2.9117686823010445 (normal) + 0.8789662439376116 (augmented)  
[MHA]Epoch 53, Loss: 3.766107015311718 = 2.8968622758984566 (normal) + 0.8692447245121002 (augmented)  
[MHA]Epoch 54, Loss: 3.7405601888895035 = 2.8841964453458786 (normal) + 0.8563637211918831 (augmented)  
[MHA]Epoch 55, Loss: 3.7143194898962975 = 2.8700047582387924 (normal) + 0.8443146850913763 (augmented)  
[MHA]Epoch 56, Loss: 3.687693051993847 = 2.8561547696590424 (normal) + 0.8315382674336433 (augmented)  
[MHA]Epoch 57, Loss: 3.6580739319324493 = 2.8409274369478226 (normal) + 0.8171464763581753 (augmented)  
[MHA]Epoch 58, Loss: 3.6357400864362717 = 2.828019432723522 (normal) + 0.8077206648886204 (augmented)  
[MHA]Epoch 59, Loss: 3.6071224361658096 = 2.8133175298571587 (normal) + 0.7938048765063286 (augmented)  
[MHA]Epoch 60, Loss: 3.5811752006411552 = 2.796678990125656 (normal) + 0.7844961974769831 (augmented)  
[MHA]Epoch 61, Loss: 3.5561499670147896 = 2.783810742199421 (normal) + 0.7723391856998205 (augmented)  
[MHA]Epoch 62, Loss: 3.5257172137498856 = 2.7690734788775444 (normal) + 0.7566437534987926 (augmented)  
[MHA]Epoch 63, Loss: 3.4945530891418457 = 2.753732778131962 (normal) + 0.7408203016966581 (augmented)  
[MHA]Epoch 64, Loss: 3.4741768836975098 = 2.738035425543785 (normal) + 0.7361414488404989 (augmented)  
[MHA]Epoch 65, Loss: 3.446247309446335 = 2.7197430580854416 (normal) + 0.7265042327344418 (augmented)  
[MHA]Epoch 66, Loss: 3.4227100536227226 = 2.704386167228222 (normal) + 0.7183238621801138 (augmented)  
[MHA]Epoch 67, Loss: 3.389100879430771 = 2.687499091029167 (normal) + 0.7016017884016037 (augmented)  
[MHA]Epoch 68, Loss: 3.3590820133686066 = 2.668138526380062 (normal) + 0.6909434627741575 (augmented)  
[MHA]Epoch 69, Loss: 3.336134396493435 = 2.6530017256736755 (normal) + 0.6831326205283403 (augmented)  
[MHA]Epoch 70, Loss: 3.3018432334065437 = 2.6333052441477776 (normal) + 0.6685379687696695 (augmented)  
[MHA]Epoch 71, Loss: 3.2756478264927864 = 2.6178000159561634 (normal) + 0.6578477900475264 (augmented)  
[MHA]Epoch 72, Loss: 3.244478575885296 = 2.5977573841810226 (normal) + 0.6467211823910475 (augmented)  
[MHA]Epoch 73, Loss: 3.213847003877163 = 2.5790591686964035 (normal) + 0.6347878342494369 (augmented)  
[MHA]Epoch 74, Loss: 3.184162311255932 = 2.558575563132763 (normal) + 0.6255867350846529 (augmented)  
[MHA]Epoch 75, Loss: 3.151342310011387 = 2.5382947772741318 (normal) + 0.6130475252866745 (augmented)  
[MHA]Epoch 76, Loss: 3.1211449950933456 = 2.5175408720970154 (normal) + 0.603604101575911 (augmented)  
[MHA]Epoch 77, Loss: 3.0938761457800865 = 2.498850390315056 (normal) + 0.5950257359072566 (augmented)  
[MHA]Epoch 78, Loss: 3.061556987464428 = 2.4774035438895226 (normal) + 0.5841534286737442 (augmented)  
[MHA]Epoch 79, Loss: 3.0329719856381416 = 2.4588683024048805 (normal) + 0.574103681370616 (augmented)  
[MHA]Epoch 80, Loss: 3.005917467176914 = 2.4411389268934727 (normal) + 0.5647785337641835 (augmented)  
[MHA]Epoch 81, Loss: 2.97214824706316 = 2.4175991639494896 (normal) + 0.5545490896329284 (augmented)  
[MHA]Epoch 82, Loss: 2.9427849277853966 = 2.398631267249584 (normal) + 0.5441536279395223 (augmented)  
[MHA]Epoch 83, Loss: 2.913725785911083 = 2.3771781995892525 (normal) + 0.5365476105362177 (augmented)  
[MHA]Epoch 84, Loss: 2.884047582745552 = 2.3557608984410763 (normal) + 0.528286699205637 (augmented)  
[MHA]Epoch 85, Loss: 2.8487352952361107 = 2.332417570054531 (normal) + 0.5163177251815796 (augmented)  
[MHA]Epoch 86, Loss: 2.8199454247951508 = 2.3122887276113033 (normal) + 0.5076567167416215 (augmented)  
[MHA]Epoch 87, Loss: 2.785344682633877 = 2.2890439070761204 (normal) + 0.49630075227469206 (augmented)  
[MHA]Epoch 88, Loss: 2.757505029439926 = 2.2695855796337128 (normal) + 0.48791944701224566 (augmented)  
[MHA]Epoch 89, Loss: 2.7245322912931442 = 2.245877031236887 (normal) + 0.47865527868270874 (augmented)  
[MHA]Epoch 90, Loss: 2.693574123084545 = 2.221581604331732 (normal) + 0.47199251409620047 (augmented)  
[MHA]Epoch 91, Loss: 2.6607393622398376 = 2.2020577117800713 (normal) + 0.4586816718801856 (augmented)  
[MHA]Epoch 92, Loss: 2.63157869130373 = 2.1791741997003555 (normal) + 0.4524045158177614 (augmented)  
[MHA]Epoch 93, Loss: 2.601968787610531 = 2.1561492905020714 (normal) + 0.44581949803978205 (augmented)  
[MHA]Epoch 94, Loss: 2.5702325850725174 = 2.132854040712118 (normal) + 0.4373785676434636 (augmented)  
[MHA]Epoch 95, Loss: 2.542164895683527 = 2.1115432493388653 (normal) + 0.43062165938317776 (augmented)  
[MHA]Epoch 96, Loss: 2.5158908888697624 = 2.0936122611165047 (normal) + 0.4222786361351609 (augmented)  
[MHA]Epoch 97, Loss: 2.4840239956974983 = 2.0656544864177704 (normal) + 0.4183695176616311 (augmented)  
[MHA]Epoch 98, Loss: 2.459295693784952 = 2.0505132004618645 (normal) + 0.408782497048378 (augmented)  
[MHA]Epoch 99, Loss: 2.4316476471722126 = 2.0276830419898033 (normal) + 0.4039646126329899 (augmented)  
[MHA]Epoch 100, Loss: 2.404793232679367 = 2.0075829550623894 (normal) + 0.39721029717475176 (augmented)  
EC  0
Training complete.
Generating data for encoder 1
[nltk_data] Downloading package stopwords to /home/a40-ko-
[nltk_data]     lab/nltk_data...
[nltk_data]   Package stopwords is already up-to-date!
BertModel loaded from: transformers.models.bert.modeling_bert
Transformers module loaded from: /home/a40-ko-lab/miniconda3/envs/jiantenv/lib/python3.9/site-packages/transformers/__init__.py
06/12/2024 13:39:00 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False
06/12/2024 13:39:00 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
batch_eval_metrics=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_steps=None,
eval_strategy=no,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/tmp/cb/runs/Jun12_13-39-00_a40kolab,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=100.0,
optim=adamw_torch,
optim_args=None,
optim_target_modules=None,
output_dir=/tmp/cb/,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=/tmp/cb/,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
[INFO|configuration_utils.py:731] 2024-06-12 13:39:00,154 >> loading configuration file ./downloads/cb_config/config.json
[INFO|configuration_utils.py:800] 2024-06-12 13:39:00,156 >> Model config DistilBertConfig {
  "_name_or_path": "./downloads/cb_config",
  "activation": "gelu",
  "architectures": [
    "DistilBertForMaskedLM"
  ],
  "attention_dropout": 0.1,
  "dim": 768,
  "dropout": 0.1,
  "finetuning_task": "cb",
  "hidden_dim": 3072,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 512,
  "model_type": "distilbert",
  "n_heads": 12,
  "n_layers": 6,
  "pad_token_id": 0,
  "qa_dropout": 0.1,
  "seq_classif_dropout": 0.2,
  "sinusoidal_pos_embds": false,
  "tie_weights_": true,
  "transformers_version": "4.42.0.dev0",
  "vocab_size": 30522
}

[INFO|tokenization_utils_base.py:2107] 2024-06-12 13:39:00,157 >> loading file vocab.txt
[INFO|tokenization_utils_base.py:2107] 2024-06-12 13:39:00,157 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2107] 2024-06-12 13:39:00,157 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2107] 2024-06-12 13:39:00,157 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2107] 2024-06-12 13:39:00,157 >> loading file tokenizer_config.json
[INFO|configuration_utils.py:731] 2024-06-12 13:39:00,172 >> loading configuration file ./downloads/cb_model/config.json
[INFO|configuration_utils.py:800] 2024-06-12 13:39:00,173 >> Model config DistilBertConfig {
  "_name_or_path": "./downloads/cb_model",
  "activation": "gelu",
  "architectures": [
    "DistilBertForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "dim": 768,
  "dropout": 0.1,
  "finetuning_task": "cb",
  "hidden_dim": 3072,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 512,
  "model_type": "distilbert",
  "n_heads": 12,
  "n_layers": 6,
  "pad_token_id": 0,
  "qa_dropout": 0.1,
  "seq_classif_dropout": 0.2,
  "sinusoidal_pos_embds": false,
  "tie_weights_": true,
  "torch_dtype": "float32",
  "transformers_version": "4.42.0.dev0",
  "vocab_size": 30522
}

[INFO|modeling_utils.py:3474] 2024-06-12 13:39:00,198 >> loading weights file ./downloads/cb_model/model.safetensors
[INFO|modeling_utils.py:4283] 2024-06-12 13:39:00,235 >> All model checkpoint weights were used when initializing DistilBertForSequenceClassification.

[INFO|modeling_utils.py:4291] 2024-06-12 13:39:00,235 >> All the weights of DistilBertForSequenceClassification were initialized from the model checkpoint at ./downloads/cb_model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForSequenceClassification for predictions without further training.
Loading cached processed dataset at /media/a40-ko-lab/c61d4ea0-b8fe-4205-9815-f7d3d054043c/sayed/Sayed/transformers/weight_sharing/modular_superglue/downloads/cb/train/cache-ec9a28bf6cd38919.arrow
Loaded dataset from disk
06/12/2024 13:39:00 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /media/a40-ko-lab/c61d4ea0-b8fe-4205-9815-f7d3d054043c/sayed/Sayed/transformers/weight_sharing/modular_superglue/downloads/cb/train/cache-ec9a28bf6cd38919.arrow
Loading cached processed dataset at /media/a40-ko-lab/c61d4ea0-b8fe-4205-9815-f7d3d054043c/sayed/Sayed/transformers/weight_sharing/modular_superglue/downloads/cb/validation/cache-462a1cd6bd454452.arrow
06/12/2024 13:39:00 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /media/a40-ko-lab/c61d4ea0-b8fe-4205-9815-f7d3d054043c/sayed/Sayed/transformers/weight_sharing/modular_superglue/downloads/cb/validation/cache-462a1cd6bd454452.arrow
Loading cached processed dataset at /media/a40-ko-lab/c61d4ea0-b8fe-4205-9815-f7d3d054043c/sayed/Sayed/transformers/weight_sharing/modular_superglue/downloads/cb/test/cache-f44364b0f28c5bd9.arrow
06/12/2024 13:39:00 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /media/a40-ko-lab/c61d4ea0-b8fe-4205-9815-f7d3d054043c/sayed/Sayed/transformers/weight_sharing/modular_superglue/downloads/cb/test/cache-f44364b0f28c5bd9.arrow
06/12/2024 13:39:00 - INFO - __main__ - Sample 163 of the training set: {'input_ids': [101, 1037, 1024, 2031, 2017, 2628, 2008, 2200, 2172, 2030, 1010, 1038, 1024, 7910, 1010, 2025, 2428, 1012, 1045, 2123, 1005, 1056, 2228, 2505, 2097, 2412, 2202, 2058, 1996, 5088, 1012, 102, 2242, 2097, 2202, 2058, 1996, 5088, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
06/12/2024 13:39:00 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:3753] 2024-06-12 13:39:00,464 >> ***** Running Evaluation *****
[INFO|trainer.py:3755] 2024-06-12 13:39:00,464 >>   Num examples = 250
[INFO|trainer.py:3758] 2024-06-12 13:39:00,464 >>   Batch size = 8
Directory './saves/distilbert/distilbert-base-uncased/cb/mha/outputs/encoder_1' created successfully.
Directory './saves/distilbert/distilbert-base-uncased/cb/ffn/inputs/encoder_1' created successfully.
Directory './saves/distilbert/distilbert-base-uncased/cb/ffn/outputs/encoder_1' created successfully.
Directory './saves/distilbert/distilbert-base-uncased/cb/mha/inputs/encoder_1' created successfully.
Batch Idx  1
  0%|          | 0/32 [00:00<?, ?it/s] 16%|█▌        | 5/32 [00:00<00:00, 47.55it/s] 31%|███▏      | 10/32 [00:00<00:00, 43.53it/s] 47%|████▋     | 15/32 [00:00<00:00, 43.16it/s] 62%|██████▎   | 20/32 [00:00<00:00, 42.01it/s] 78%|███████▊  | 25/32 [00:00<00:00, 41.96it/s] 94%|█████████▍| 30/32 [00:00<00:00, 42.04it/s]100%|██████████| 32/32 [00:00<00:00, 43.24it/s]
Batch Idx  2
Batch Idx  3
Batch Idx  4
Batch Idx  5
Batch Idx  6
Batch Idx  7
Batch Idx  8
Batch Idx  9
Batch Idx  10
Batch Idx  11
Batch Idx  12
Batch Idx  13
Batch Idx  14
Batch Idx  15
Batch Idx  16
Batch Idx  17
Batch Idx  18
Batch Idx  19
Batch Idx  20
Batch Idx  21
Batch Idx  22
Batch Idx  23
Batch Idx  24
Batch Idx  25
Batch Idx  26
Batch Idx  27
Batch Idx  28
Batch Idx  29
Batch Idx  30
Batch Idx  31
Batch Idx  32
***** eval metrics *****
  eval_runtime            = 0:00:00.85
  eval_samples            =        250
  eval_samples_per_second =     292.68
  eval_steps_per_second   =     37.463
Data generated succesfully

 
 
 Beginning modular training of SA and BL modules
Encoder compression: 2
Modular training for BL module, encoder 1
LOAD FROM SAVE
Encoder idx = 1
Compression = 2
[FFN] Epoch 1, Loss: 34.81395077705383 = 27.046795189380646 (normal) + 7.767155483365059 (augmented)  
[FFN] Epoch 2, Loss: 30.460924685001373 = 24.20168101787567 (normal) + 6.259243652224541 (augmented)  
[FFN] Epoch 3, Loss: 25.915898382663727 = 20.814388513565063 (normal) + 5.101509675383568 (augmented)  
[FFN] Epoch 4, Loss: 21.35025644302368 = 17.337422728538513 (normal) + 4.012833669781685 (augmented)  
[FFN] Epoch 5, Loss: 16.422374308109283 = 13.451017826795578 (normal) + 2.9713564664125443 (augmented)  
[FFN] Epoch 6, Loss: 12.882350206375122 = 10.432544484734535 (normal) + 2.449805650860071 (augmented)  
[FFN] Epoch 7, Loss: 10.800539880990982 = 8.74326029419899 (normal) + 2.057279624044895 (augmented)  
[FFN] Epoch 8, Loss: 9.128872200846672 = 6.943488225340843 (normal) + 2.1853839233517647 (augmented)  
[FFN] Epoch 9, Loss: 8.353987127542496 = 6.119818300008774 (normal) + 2.2341688200831413 (augmented)  
[FFN] Epoch 10, Loss: 7.486416876316071 = 5.314424157142639 (normal) + 2.171992737799883 (augmented)  
[FFN] Epoch 11, Loss: 7.362959951162338 = 5.284860864281654 (normal) + 2.0780990161001682 (augmented)  
[FFN] Epoch 12, Loss: 7.500629171729088 = 5.414607286453247 (normal) + 2.08602187409997 (augmented)  
[FFN] Epoch 13, Loss: 7.260118529200554 = 5.264628380537033 (normal) + 1.9954900965094566 (augmented)  
[FFN] Epoch 14, Loss: 7.055686846375465 = 5.108383342623711 (normal) + 1.94730344414711 (augmented)  
[FFN] Epoch 15, Loss: 6.5514549016952515 = 4.650095097720623 (normal) + 1.901359785348177 (augmented)  
[FFN] Epoch 16, Loss: 6.577333495020866 = 4.756892651319504 (normal) + 1.8204407915472984 (augmented)  
[FFN] Epoch 17, Loss: 6.9923694133758545 = 4.99187171459198 (normal) + 2.000497702509165 (augmented)  
[FFN] Epoch 18, Loss: 6.163633644580841 = 4.257291853427887 (normal) + 1.9063418172299862 (augmented)  
[FFN] Epoch 19, Loss: 6.187474757432938 = 4.373465299606323 (normal) + 1.814009454101324 (augmented)  
[FFN] Epoch 20, Loss: 6.51729578524828 = 4.878939256072044 (normal) + 1.6383565440773964 (augmented)  
[FFN] Epoch 21, Loss: 6.413573391735554 = 4.778063148260117 (normal) + 1.6355101950466633 (augmented)  
[FFN] Epoch 22, Loss: 5.591037638485432 = 3.8798873126506805 (normal) + 1.7111502476036549 (augmented)  
[FFN] Epoch 23, Loss: 6.098385497927666 = 4.326743021607399 (normal) + 1.7716424576938152 (augmented)  
[FFN] Epoch 24, Loss: 5.89016979932785 = 4.296772073954344 (normal) + 1.5933976881206036 (augmented)  
[FFN] Epoch 25, Loss: 6.331825099885464 = 4.807578191161156 (normal) + 1.5242469031363726 (augmented)  
[FFN] Epoch 26, Loss: 5.5107297748327255 = 3.960249349474907 (normal) + 1.5504803601652384 (augmented)  
[FFN] Epoch 27, Loss: 6.127017870545387 = 4.4306387305259705 (normal) + 1.6963791027665138 (augmented)  
[FFN] Epoch 28, Loss: 6.069118790328503 = 4.495691880583763 (normal) + 1.5734269432723522 (augmented)  
[FFN] Epoch 29, Loss: 5.965660735964775 = 4.437142807990313 (normal) + 1.5285178981721401 (augmented)  
[FFN] Epoch 30, Loss: 5.955208644270897 = 4.316030669957399 (normal) + 1.6391779705882072 (augmented)  
[FFN] Epoch 31, Loss: 6.253665030002594 = 4.754243426024914 (normal) + 1.4994215928018093 (augmented)  
[FFN] Epoch 32, Loss: 5.808444648981094 = 4.2333248518407345 (normal) + 1.5751197785139084 (augmented)  
[FFN] Epoch 33, Loss: 6.00555981695652 = 4.538904808461666 (normal) + 1.4666549991816282 (augmented)  
[FFN] Epoch 34, Loss: 5.7285387590527534 = 4.29765160381794 (normal) + 1.4308871775865555 (augmented)  
[FFN] Epoch 35, Loss: 5.517796158790588 = 3.954929307103157 (normal) + 1.562866823747754 (augmented)  
[FFN] Epoch 36, Loss: 6.044765204191208 = 4.575228054076433 (normal) + 1.4695371259003878 (augmented)  
[FFN] Epoch 37, Loss: 5.605710476636887 = 4.107960429042578 (normal) + 1.4977500047534704 (augmented)  
[FFN] Epoch 38, Loss: 5.591442935168743 = 4.05236079543829 (normal) + 1.539082108065486 (augmented)  
[FFN] Epoch 39, Loss: 6.269120946526527 = 4.850745916366577 (normal) + 1.4183749966323376 (augmented)  
[FFN] Epoch 40, Loss: 5.493290297687054 = 4.035051479935646 (normal) + 1.4582387935370207 (augmented)  
[FFN] Epoch 41, Loss: 5.9912117421627045 = 4.419799663126469 (normal) + 1.5714120138436556 (augmented)  
[FFN] Epoch 42, Loss: 5.325455442070961 = 3.94934706017375 (normal) + 1.3761083781719208 (augmented)  
[FFN] Epoch 43, Loss: 5.667303159832954 = 4.2075449377298355 (normal) + 1.459758223965764 (augmented)  
[FFN] Epoch 44, Loss: 5.5055175721645355 = 4.1280878856778145 (normal) + 1.377429684624076 (augmented)  
[FFN] Epoch 45, Loss: 5.280832402408123 = 3.931544091552496 (normal) + 1.3492883313447237 (augmented)  
[FFN] Epoch 46, Loss: 5.829452112317085 = 4.510423690080643 (normal) + 1.3190284259617329 (augmented)  
[FFN] Epoch 47, Loss: 5.288969159126282 = 3.953367568552494 (normal) + 1.3356015719473362 (augmented)  
[FFN] Epoch 48, Loss: 5.285302892327309 = 3.8484754264354706 (normal) + 1.4368274696171284 (augmented)  
[FFN] Epoch 49, Loss: 5.556372374296188 = 4.0276037603616714 (normal) + 1.5287686251103878 (augmented)  
[FFN] Epoch 50, Loss: 6.004094533622265 = 4.605855997651815 (normal) + 1.3982385024428368 (augmented)  
[FFN] Epoch 51, Loss: 5.599657244980335 = 4.196918275207281 (normal) + 1.4027390480041504 (augmented)  
[FFN] Epoch 52, Loss: 5.824839383363724 = 4.413767967373133 (normal) + 1.4110713712871075 (augmented)  
[FFN] Epoch 53, Loss: 5.896989844739437 = 4.495652355253696 (normal) + 1.4013374857604504 (augmented)  
[FFN] Epoch 54, Loss: 5.4798755422234535 = 4.073829084634781 (normal) + 1.4060464203357697 (augmented)  
[FFN] Epoch 55, Loss: 5.959252089262009 = 4.483446005731821 (normal) + 1.47580611333251 (augmented)  
[FFN] Epoch 56, Loss: 5.605754099786282 = 4.253091473132372 (normal) + 1.352662643417716 (augmented)  
[FFN] Epoch 57, Loss: 5.462658703327179 = 3.9681383334100246 (normal) + 1.4945204071700573 (augmented)  
[FFN] Epoch 58, Loss: 5.361627876758575 = 3.9553322829306126 (normal) + 1.4062955603003502 (augmented)  
[FFN] Epoch 59, Loss: 5.479866661131382 = 4.18346368893981 (normal) + 1.2964029647409916 (augmented)  
[FFN] Epoch 60, Loss: 5.445946902036667 = 4.03890386223793 (normal) + 1.4070430397987366 (augmented)  
[FFN] Epoch 61, Loss: 5.427366942167282 = 4.083375457674265 (normal) + 1.3439914658665657 (augmented)  
[FFN] Epoch 62, Loss: 5.142535537481308 = 3.848743975162506 (normal) + 1.29379153996706 (augmented)  
[FFN] Epoch 63, Loss: 5.299387753009796 = 3.844808302819729 (normal) + 1.4545794483274221 (augmented)  
[FFN] Epoch 64, Loss: 5.56277197599411 = 4.239196702837944 (normal) + 1.323575234040618 (augmented)  
[FFN] Epoch 65, Loss: 5.828402452170849 = 4.517259772866964 (normal) + 1.3111426811665297 (augmented)  
[FFN] Epoch 66, Loss: 4.657214090228081 = 3.3794045262038708 (normal) + 1.2778094969689846 (augmented)  
[FFN] Epoch 67, Loss: 5.075786359608173 = 3.729431364685297 (normal) + 1.34635497815907 (augmented)  
[FFN] Epoch 68, Loss: 5.1272002607584 = 3.7729137390851974 (normal) + 1.3542865328490734 (augmented)  
[FFN] Epoch 69, Loss: 5.668491967022419 = 4.356703508645296 (normal) + 1.311788396909833 (augmented)  
[FFN] Epoch 70, Loss: 5.190888285636902 = 3.9042392522096634 (normal) + 1.2866490203887224 (augmented)  
[FFN] Epoch 71, Loss: 4.980708867311478 = 3.6646929904818535 (normal) + 1.3160158544778824 (augmented)  
[FFN] Epoch 72, Loss: 5.336523957550526 = 4.123500771820545 (normal) + 1.2130231857299805 (augmented)  
[FFN] Epoch 73, Loss: 5.251150414347649 = 3.929297111928463 (normal) + 1.3218533098697662 (augmented)  
[FFN] Epoch 74, Loss: 5.58770652115345 = 4.2054175697267056 (normal) + 1.3822889477014542 (augmented)  
[FFN] Epoch 75, Loss: 5.276619799435139 = 4.025667171925306 (normal) + 1.2509525921195745 (augmented)  
[FFN] Epoch 76, Loss: 5.492101468145847 = 4.076191607862711 (normal) + 1.415909893810749 (augmented)  
[FFN] Epoch 77, Loss: 5.60283736884594 = 4.148593500256538 (normal) + 1.454243890941143 (augmented)  
[FFN] Epoch 78, Loss: 5.281271427869797 = 3.972176242619753 (normal) + 1.309095174074173 (augmented)  
[FFN] Epoch 79, Loss: 5.698432341217995 = 4.379870988428593 (normal) + 1.3185613844543695 (augmented)  
[FFN] Epoch 80, Loss: 5.486290670931339 = 4.192669861018658 (normal) + 1.2936208304017782 (augmented)  
[FFN] Epoch 81, Loss: 5.334761671721935 = 4.050813127309084 (normal) + 1.2839484997093678 (augmented)  
[FFN] Epoch 82, Loss: 6.241044893860817 = 4.861231457442045 (normal) + 1.3798134215176105 (augmented)  
[FFN] Epoch 83, Loss: 5.001247838139534 = 3.5888416320085526 (normal) + 1.4124061819165945 (augmented)  
[FFN] Epoch 84, Loss: 5.188590280711651 = 3.8649915866553783 (normal) + 1.3235986903309822 (augmented)  
[FFN] Epoch 85, Loss: 5.815416917204857 = 4.544966541230679 (normal) + 1.270450359210372 (augmented)  
[FFN] Epoch 86, Loss: 5.838426522910595 = 4.352557323873043 (normal) + 1.4858691543340683 (augmented)  
[FFN] Epoch 87, Loss: 5.50617640465498 = 4.174913953989744 (normal) + 1.3312624264508486 (augmented)  
[FFN] Epoch 88, Loss: 5.543367139995098 = 4.214693367481232 (normal) + 1.3286737520247698 (augmented)  
[FFN] Epoch 89, Loss: 5.303355164825916 = 3.988444097340107 (normal) + 1.3149110190570354 (augmented)  
[FFN] Epoch 90, Loss: 5.454018950462341 = 4.067261587828398 (normal) + 1.3867573458701372 (augmented)  
[FFN] Epoch 91, Loss: 5.2118073627352715 = 3.801379807293415 (normal) + 1.4104275312274694 (augmented)  
[FFN] Epoch 92, Loss: 5.553733453154564 = 4.213883072137833 (normal) + 1.3398504294455051 (augmented)  
[FFN] Epoch 93, Loss: 5.520297594368458 = 4.1154184974730015 (normal) + 1.4048790223896503 (augmented)  
[FFN] Epoch 94, Loss: 5.360565580427647 = 4.113340612500906 (normal) + 1.247224947437644 (augmented)  
[FFN] Epoch 95, Loss: 5.1368118822574615 = 3.7497182860970497 (normal) + 1.3870935961604118 (augmented)  
[FFN] Epoch 96, Loss: 5.265378139913082 = 3.789417441934347 (normal) + 1.4759606681764126 (augmented)  
[FFN] Epoch 97, Loss: 5.330401286482811 = 3.929790250957012 (normal) + 1.4006110057234764 (augmented)  
[FFN] Epoch 98, Loss: 5.052685409784317 = 3.6983457021415234 (normal) + 1.3543397281318903 (augmented)  
[FFN] Epoch 99, Loss: 5.822002574801445 = 4.458282079547644 (normal) + 1.363720465451479 (augmented)  
[FFN] Epoch 100, Loss: 5.895865499973297 = 4.496357399970293 (normal) + 1.3995081204921007 (augmented)  
Training complete.
Modular training for SA module, encoder 1
LOAD FROM SAVE
Encoder idx = 1
Batch count :  32
[MHA]Epoch 1, Loss: 8.6428182721138 = 5.6854377835989 (normal) + 2.9573804885149 (augmented)  
[MHA]Epoch 2, Loss: 6.832529544830322 = 4.675012156367302 (normal) + 2.1575173921883106 (augmented)  
[MHA]Epoch 3, Loss: 6.457722902297974 = 4.449121095240116 (normal) + 2.0086018182337284 (augmented)  
[MHA]Epoch 4, Loss: 6.225633651018143 = 4.302328310906887 (normal) + 1.9233052805066109 (augmented)  
[MHA]Epoch 5, Loss: 5.9235586524009705 = 4.098277121782303 (normal) + 1.8252815268933773 (augmented)  
[MHA]Epoch 6, Loss: 5.587498977780342 = 3.8611841797828674 (normal) + 1.7263147309422493 (augmented)  
[MHA]Epoch 7, Loss: 5.246602728962898 = 3.6173235401511192 (normal) + 1.6292791850864887 (augmented)  
[MHA]Epoch 8, Loss: 4.939016133546829 = 3.400148421525955 (normal) + 1.5388677269220352 (augmented)  
[MHA]Epoch 9, Loss: 4.680626846849918 = 3.220279335975647 (normal) + 1.4603475257754326 (augmented)  
[MHA]Epoch 10, Loss: 4.44631289690733 = 3.0675605088472366 (normal) + 1.3787523470818996 (augmented)  
[MHA]Epoch 11, Loss: 4.248688384890556 = 2.9377951100468636 (normal) + 1.3108932226896286 (augmented)  
[MHA]Epoch 12, Loss: 4.066410966217518 = 2.82189654558897 (normal) + 1.2445144020020962 (augmented)  
[MHA]Epoch 13, Loss: 3.907764807343483 = 2.7214964404702187 (normal) + 1.186268374323845 (augmented)  
[MHA]Epoch 14, Loss: 3.77071962505579 = 2.6407350599765778 (normal) + 1.1299845669418573 (augmented)  
[MHA]Epoch 15, Loss: 3.6682145819067955 = 2.5676368549466133 (normal) + 1.1005777213722467 (augmented)  
[MHA]Epoch 16, Loss: 3.562673658132553 = 2.5062324181199074 (normal) + 1.0564412269741297 (augmented)  
[MHA]Epoch 17, Loss: 3.4733091667294502 = 2.457263119518757 (normal) + 1.0160460248589516 (augmented)  
[MHA]Epoch 18, Loss: 3.4081238955259323 = 2.4132261350750923 (normal) + 0.9948977362364531 (augmented)  
[MHA]Epoch 19, Loss: 3.3447761461138725 = 2.372821554541588 (normal) + 0.9719545766711235 (augmented)  
[MHA]Epoch 20, Loss: 3.291798308491707 = 2.3393232077360153 (normal) + 0.9524750839918852 (augmented)  
[MHA]Epoch 21, Loss: 3.24014263600111 = 2.308531057089567 (normal) + 0.9316115491092205 (augmented)  
[MHA]Epoch 22, Loss: 3.197618745267391 = 2.279454719275236 (normal) + 0.9181640055030584 (augmented)  
[MHA]Epoch 23, Loss: 3.147217310965061 = 2.2523831874132156 (normal) + 0.8948341030627489 (augmented)  
[MHA]Epoch 24, Loss: 3.106622725725174 = 2.2271088883280754 (normal) + 0.8795137982815504 (augmented)  
[MHA]Epoch 25, Loss: 3.0851310342550278 = 2.202878460288048 (normal) + 0.88225257396698 (augmented)  
[MHA]Epoch 26, Loss: 3.040605515241623 = 2.1823772378265858 (normal) + 0.8582282569259405 (augmented)  
[MHA]Epoch 27, Loss: 3.0224515944719315 = 2.1612153872847557 (normal) + 0.8612361717969179 (augmented)  
[MHA]Epoch 28, Loss: 2.9744567796587944 = 2.141527432948351 (normal) + 0.8329293169081211 (augmented)  
[MHA]Epoch 29, Loss: 2.9541667476296425 = 2.1244083046913147 (normal) + 0.8297584280371666 (augmented)  
[MHA]Epoch 30, Loss: 2.9243233650922775 = 2.1076602563261986 (normal) + 0.8166630808264017 (augmented)  
[MHA]Epoch 31, Loss: 2.896333262324333 = 2.0926678627729416 (normal) + 0.8036653734743595 (augmented)  
[MHA]Epoch 32, Loss: 2.882979691028595 = 2.078413315117359 (normal) + 0.8045663554221392 (augmented)  
[MHA]Epoch 33, Loss: 2.857014447450638 = 2.062516927719116 (normal) + 0.7944974973797798 (augmented)  
[MHA]Epoch 34, Loss: 2.8413571193814278 = 2.0504794642329216 (normal) + 0.7908776327967644 (augmented)  
[MHA]Epoch 35, Loss: 2.8157389760017395 = 2.037238698452711 (normal) + 0.7785002365708351 (augmented)  
[MHA]Epoch 36, Loss: 2.792103037238121 = 2.025015752762556 (normal) + 0.7670872639864683 (augmented)  
[MHA]Epoch 37, Loss: 2.771728068590164 = 2.0134771913290024 (normal) + 0.7582508493214846 (augmented)  
[MHA]Epoch 38, Loss: 2.7653346434235573 = 2.0032689794898033 (normal) + 0.7620656583458185 (augmented)  
[MHA]Epoch 39, Loss: 2.7367730364203453 = 1.9924558773636818 (normal) + 0.7443171497434378 (augmented)  
[MHA]Epoch 40, Loss: 2.730268180370331 = 1.9832212589681149 (normal) + 0.7470468953251839 (augmented)  
[MHA]Epoch 41, Loss: 2.7180370688438416 = 1.9711730033159256 (normal) + 0.7468640524893999 (augmented)  
[MHA]Epoch 42, Loss: 2.6990931928157806 = 1.9630717188119888 (normal) + 0.7360214572399855 (augmented)  
[MHA]Epoch 43, Loss: 2.6771300360560417 = 1.9527189061045647 (normal) + 0.7244111206382513 (augmented)  
[MHA]Epoch 44, Loss: 2.6587791815400124 = 1.9441860616207123 (normal) + 0.7145931143313646 (augmented)  
[MHA]Epoch 45, Loss: 2.6489800065755844 = 1.936500322073698 (normal) + 0.7124796733260155 (augmented)  
[MHA]Epoch 46, Loss: 2.637570843100548 = 1.9290113858878613 (normal) + 0.7085594478994608 (augmented)  
[MHA]Epoch 47, Loss: 2.627993956208229 = 1.921856451779604 (normal) + 0.7061375100165606 (augmented)  
[MHA]Epoch 48, Loss: 2.62344753742218 = 1.9128213003277779 (normal) + 0.7106262035667896 (augmented)  
[MHA]Epoch 49, Loss: 2.6040530279278755 = 1.9078569933772087 (normal) + 0.6961960289627314 (augmented)  
[MHA]Epoch 50, Loss: 2.5953396186232567 = 1.8987698778510094 (normal) + 0.6965697314590216 (augmented)  
[MHA]Epoch 51, Loss: 2.5725844129920006 = 1.8911396823823452 (normal) + 0.6814447250217199 (augmented)  
[MHA]Epoch 52, Loss: 2.561837039887905 = 1.8859768360853195 (normal) + 0.6758601758629084 (augmented)  
[MHA]Epoch 53, Loss: 2.56579352915287 = 1.8821792677044868 (normal) + 0.6836142763495445 (augmented)  
[MHA]Epoch 54, Loss: 2.5437165200710297 = 1.8750523664057255 (normal) + 0.6686641350388527 (augmented)  
[MHA]Epoch 55, Loss: 2.537718191742897 = 1.8724791891872883 (normal) + 0.6652389820665121 (augmented)  
[MHA]Epoch 56, Loss: 2.532864212989807 = 1.8683748133480549 (normal) + 0.6644893996417522 (augmented)  
[MHA]Epoch 57, Loss: 2.51889930665493 = 1.862929455935955 (normal) + 0.655969837680459 (augmented)  
[MHA]Epoch 58, Loss: 2.511937916278839 = 1.8552509061992168 (normal) + 0.6566869840025902 (augmented)  
[MHA]Epoch 59, Loss: 2.511477842926979 = 1.8501735217869282 (normal) + 0.661304323002696 (augmented)  
[MHA]Epoch 60, Loss: 2.483671687543392 = 1.8389644101262093 (normal) + 0.6447072774171829 (augmented)  
[MHA]Epoch 61, Loss: 2.4725961834192276 = 1.8335141949355602 (normal) + 0.6390819661319256 (augmented)  
[MHA]Epoch 62, Loss: 2.460287742316723 = 1.8240209482610226 (normal) + 0.6362667698413134 (augmented)  
[MHA]Epoch 63, Loss: 2.446019694209099 = 1.8182177022099495 (normal) + 0.6278019864112139 (augmented)  
[MHA]Epoch 64, Loss: 2.436926230788231 = 1.8128084018826485 (normal) + 0.6241178419440985 (augmented)  
[MHA]Epoch 65, Loss: 2.4359138682484627 = 1.8083163537085056 (normal) + 0.6275974996387959 (augmented)  
[MHA]Epoch 66, Loss: 2.4231288135051727 = 1.8019866310060024 (normal) + 0.6211421769112349 (augmented)  
[MHA]Epoch 67, Loss: 2.4109342098236084 = 1.799883659929037 (normal) + 0.6110505554825068 (augmented)  
[MHA]Epoch 68, Loss: 2.4075194597244263 = 1.793268159031868 (normal) + 0.6142512895166874 (augmented)  
[MHA]Epoch 69, Loss: 2.400859422981739 = 1.7882499881088734 (normal) + 0.6126094274222851 (augmented)  
[MHA]Epoch 70, Loss: 2.389053598046303 = 1.7842644304037094 (normal) + 0.6047891899943352 (augmented)  
[MHA]Epoch 71, Loss: 2.381368551403284 = 1.7806638702750206 (normal) + 0.6007046718150377 (augmented)  
[MHA]Epoch 72, Loss: 2.3827977254986763 = 1.780044425278902 (normal) + 0.6027532843872905 (augmented)  
[MHA]Epoch 73, Loss: 2.3885360546410084 = 1.7821358181536198 (normal) + 0.6064002150669694 (augmented)  
[MHA]Epoch 74, Loss: 2.380563087761402 = 1.7788541167974472 (normal) + 0.601708959788084 (augmented)  
[MHA]Epoch 75, Loss: 2.3807862401008606 = 1.7788880616426468 (normal) + 0.6018981710076332 (augmented)  
[MHA]Epoch 76, Loss: 2.3692280538380146 = 1.7766150534152985 (normal) + 0.5926130143925548 (augmented)  
[MHA]Epoch 77, Loss: 2.362895779311657 = 1.7711029574275017 (normal) + 0.5917928097769618 (augmented)  
[MHA]Epoch 78, Loss: 2.3502903282642365 = 1.7637888230383396 (normal) + 0.5865015089511871 (augmented)  
[MHA]Epoch 79, Loss: 2.336862273514271 = 1.7570730783045292 (normal) + 0.5797891905531287 (augmented)  
[MHA]Epoch 80, Loss: 2.3258118629455566 = 1.7470275945961475 (normal) + 0.5787842432036996 (augmented)  
[MHA]Epoch 81, Loss: 2.3085703551769257 = 1.7408416233956814 (normal) + 0.5677287112921476 (augmented)  
[MHA]Epoch 82, Loss: 2.2977400980889797 = 1.7343516200780869 (normal) + 0.5633884435519576 (augmented)  
[MHA]Epoch 83, Loss: 2.2971587255597115 = 1.7299481742084026 (normal) + 0.5672105411067605 (augmented)  
[MHA]Epoch 84, Loss: 2.2853691317141056 = 1.7265650779008865 (normal) + 0.5588040528818965 (augmented)  
[MHA]Epoch 85, Loss: 2.273101285099983 = 1.7199810668826103 (normal) + 0.5531202014535666 (augmented)  
[MHA]Epoch 86, Loss: 2.267226852476597 = 1.7162250503897667 (normal) + 0.5510018048807979 (augmented)  
[MHA]Epoch 87, Loss: 2.265901144593954 = 1.7156918346881866 (normal) + 0.5502093192189932 (augmented)  
[MHA]Epoch 88, Loss: 2.256021123379469 = 1.7084493599832058 (normal) + 0.5475717475637794 (augmented)  
[MHA]Epoch 89, Loss: 2.2443379275500774 = 1.7026393041014671 (normal) + 0.5416985861957073 (augmented)  
[MHA]Epoch 90, Loss: 2.2369708195328712 = 1.6971764862537384 (normal) + 0.5397943388670683 (augmented)  
[MHA]Epoch 91, Loss: 2.2257968224585056 = 1.6913633197546005 (normal) + 0.5344335157424212 (augmented)  
[MHA]Epoch 92, Loss: 2.21602139249444 = 1.6893631517887115 (normal) + 0.5266582388430834 (augmented)  
[MHA]Epoch 93, Loss: 2.20435219630599 = 1.6836831830441952 (normal) + 0.5206690188497305 (augmented)  
[MHA]Epoch 94, Loss: 2.201252520084381 = 1.67839877307415 (normal) + 0.5228537442162633 (augmented)  
[MHA]Epoch 95, Loss: 2.1989701129496098 = 1.675147108733654 (normal) + 0.5238229911774397 (augmented)  
[MHA]Epoch 96, Loss: 2.1891839019954205 = 1.6728877760469913 (normal) + 0.5162961408495903 (augmented)  
[MHA]Epoch 97, Loss: 2.1855302080512047 = 1.6679964400827885 (normal) + 0.5175337679684162 (augmented)  
[MHA]Epoch 98, Loss: 2.1732177808880806 = 1.6628244146704674 (normal) + 0.5103933652862906 (augmented)  
[MHA]Epoch 99, Loss: 2.168043915182352 = 1.6595558039844036 (normal) + 0.5084881270304322 (augmented)  
[MHA]Epoch 100, Loss: 2.1622788198292255 = 1.653500884771347 (normal) + 0.5087779015302658 (augmented)  
EC  0
Training complete.
Generating data for encoder 2
[nltk_data] Downloading package stopwords to /home/a40-ko-
[nltk_data]     lab/nltk_data...
[nltk_data]   Package stopwords is already up-to-date!
BertModel loaded from: transformers.models.bert.modeling_bert
Transformers module loaded from: /home/a40-ko-lab/miniconda3/envs/jiantenv/lib/python3.9/site-packages/transformers/__init__.py
06/12/2024 13:39:45 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False
06/12/2024 13:39:45 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
batch_eval_metrics=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_steps=None,
eval_strategy=no,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/tmp/cb/runs/Jun12_13-39-45_a40kolab,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=100.0,
optim=adamw_torch,
optim_args=None,
optim_target_modules=None,
output_dir=/tmp/cb/,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=/tmp/cb/,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
[INFO|configuration_utils.py:731] 2024-06-12 13:39:45,969 >> loading configuration file ./downloads/cb_config/config.json
[INFO|configuration_utils.py:800] 2024-06-12 13:39:45,970 >> Model config DistilBertConfig {
  "_name_or_path": "./downloads/cb_config",
  "activation": "gelu",
  "architectures": [
    "DistilBertForMaskedLM"
  ],
  "attention_dropout": 0.1,
  "dim": 768,
  "dropout": 0.1,
  "finetuning_task": "cb",
  "hidden_dim": 3072,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 512,
  "model_type": "distilbert",
  "n_heads": 12,
  "n_layers": 6,
  "pad_token_id": 0,
  "qa_dropout": 0.1,
  "seq_classif_dropout": 0.2,
  "sinusoidal_pos_embds": false,
  "tie_weights_": true,
  "transformers_version": "4.42.0.dev0",
  "vocab_size": 30522
}

[INFO|tokenization_utils_base.py:2107] 2024-06-12 13:39:45,971 >> loading file vocab.txt
[INFO|tokenization_utils_base.py:2107] 2024-06-12 13:39:45,971 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2107] 2024-06-12 13:39:45,971 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2107] 2024-06-12 13:39:45,971 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2107] 2024-06-12 13:39:45,972 >> loading file tokenizer_config.json
[INFO|configuration_utils.py:731] 2024-06-12 13:39:45,987 >> loading configuration file ./downloads/cb_model/config.json
[INFO|configuration_utils.py:800] 2024-06-12 13:39:45,988 >> Model config DistilBertConfig {
  "_name_or_path": "./downloads/cb_model",
  "activation": "gelu",
  "architectures": [
    "DistilBertForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "dim": 768,
  "dropout": 0.1,
  "finetuning_task": "cb",
  "hidden_dim": 3072,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 512,
  "model_type": "distilbert",
  "n_heads": 12,
  "n_layers": 6,
  "pad_token_id": 0,
  "qa_dropout": 0.1,
  "seq_classif_dropout": 0.2,
  "sinusoidal_pos_embds": false,
  "tie_weights_": true,
  "torch_dtype": "float32",
  "transformers_version": "4.42.0.dev0",
  "vocab_size": 30522
}

[INFO|modeling_utils.py:3474] 2024-06-12 13:39:46,013 >> loading weights file ./downloads/cb_model/model.safetensors
[INFO|modeling_utils.py:4283] 2024-06-12 13:39:46,049 >> All model checkpoint weights were used when initializing DistilBertForSequenceClassification.

[INFO|modeling_utils.py:4291] 2024-06-12 13:39:46,049 >> All the weights of DistilBertForSequenceClassification were initialized from the model checkpoint at ./downloads/cb_model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForSequenceClassification for predictions without further training.
Loading cached processed dataset at /media/a40-ko-lab/c61d4ea0-b8fe-4205-9815-f7d3d054043c/sayed/Sayed/transformers/weight_sharing/modular_superglue/downloads/cb/train/cache-ec9a28bf6cd38919.arrow
Loaded dataset from disk
06/12/2024 13:39:46 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /media/a40-ko-lab/c61d4ea0-b8fe-4205-9815-f7d3d054043c/sayed/Sayed/transformers/weight_sharing/modular_superglue/downloads/cb/train/cache-ec9a28bf6cd38919.arrow
Loading cached processed dataset at /media/a40-ko-lab/c61d4ea0-b8fe-4205-9815-f7d3d054043c/sayed/Sayed/transformers/weight_sharing/modular_superglue/downloads/cb/validation/cache-462a1cd6bd454452.arrow
06/12/2024 13:39:46 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /media/a40-ko-lab/c61d4ea0-b8fe-4205-9815-f7d3d054043c/sayed/Sayed/transformers/weight_sharing/modular_superglue/downloads/cb/validation/cache-462a1cd6bd454452.arrow
Loading cached processed dataset at /media/a40-ko-lab/c61d4ea0-b8fe-4205-9815-f7d3d054043c/sayed/Sayed/transformers/weight_sharing/modular_superglue/downloads/cb/test/cache-f44364b0f28c5bd9.arrow
06/12/2024 13:39:46 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /media/a40-ko-lab/c61d4ea0-b8fe-4205-9815-f7d3d054043c/sayed/Sayed/transformers/weight_sharing/modular_superglue/downloads/cb/test/cache-f44364b0f28c5bd9.arrow
06/12/2024 13:39:46 - INFO - __main__ - Sample 163 of the training set: {'input_ids': [101, 1037, 1024, 2031, 2017, 2628, 2008, 2200, 2172, 2030, 1010, 1038, 1024, 7910, 1010, 2025, 2428, 1012, 1045, 2123, 1005, 1056, 2228, 2505, 2097, 2412, 2202, 2058, 1996, 5088, 1012, 102, 2242, 2097, 2202, 2058, 1996, 5088, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
06/12/2024 13:39:46 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:3753] 2024-06-12 13:39:46,272 >> ***** Running Evaluation *****
[INFO|trainer.py:3755] 2024-06-12 13:39:46,272 >>   Num examples = 250
[INFO|trainer.py:3758] 2024-06-12 13:39:46,272 >>   Batch size = 8
Directory './saves/distilbert/distilbert-base-uncased/cb/mha/outputs/encoder_2' created successfully.
Directory './saves/distilbert/distilbert-base-uncased/cb/ffn/inputs/encoder_2' created successfully.
Directory './saves/distilbert/distilbert-base-uncased/cb/ffn/outputs/encoder_2' created successfully.
Directory './saves/distilbert/distilbert-base-uncased/cb/mha/inputs/encoder_2' created successfully.
Batch Idx  1
  0%|          | 0/32 [00:00<?, ?it/s] 16%|█▌        | 5/32 [00:00<00:00, 45.32it/s] 31%|███▏      | 10/32 [00:00<00:00, 43.53it/s] 47%|████▋     | 15/32 [00:00<00:00, 42.92it/s] 62%|██████▎   | 20/32 [00:00<00:00, 41.64it/s] 78%|███████▊  | 25/32 [00:00<00:00, 41.64it/s] 94%|█████████▍| 30/32 [00:00<00:00, 41.64it/s]100%|██████████| 32/32 [00:00<00:00, 42.84it/s]
Batch Idx  2
Batch Idx  3
Batch Idx  4
Batch Idx  5
Batch Idx  6
Batch Idx  7
Batch Idx  8
Batch Idx  9
Batch Idx  10
Batch Idx  11
Batch Idx  12
Batch Idx  13
Batch Idx  14
Batch Idx  15
Batch Idx  16
Batch Idx  17
Batch Idx  18
Batch Idx  19
Batch Idx  20
Batch Idx  21
Batch Idx  22
Batch Idx  23
Batch Idx  24
Batch Idx  25
Batch Idx  26
Batch Idx  27
Batch Idx  28
Batch Idx  29
Batch Idx  30
Batch Idx  31
Batch Idx  32
***** eval metrics *****
  eval_runtime            = 0:00:00.86
  eval_samples            =        250
  eval_samples_per_second =     290.44
  eval_steps_per_second   =     37.176
Data generated succesfully

 
 
 Beginning modular training of SA and BL modules
Encoder compression: 2
Modular training for BL module, encoder 2
LOAD FROM SAVE
Encoder idx = 2
Compression = 2
[FFN] Epoch 1, Loss: 49.39770328998566 = 42.24932646751404 (normal) + 7.1483767330646515 (augmented)  
[FFN] Epoch 2, Loss: 44.26372480392456 = 38.44339573383331 (normal) + 5.820329003036022 (augmented)  
[FFN] Epoch 3, Loss: 39.54459738731384 = 34.65740442276001 (normal) + 4.887193329632282 (augmented)  
[FFN] Epoch 4, Loss: 34.979078233242035 = 30.88458275794983 (normal) + 4.0944954007864 (augmented)  
[FFN] Epoch 5, Loss: 29.59649568796158 = 26.368731558322906 (normal) + 3.2277641855180264 (augmented)  
[FFN] Epoch 6, Loss: 24.986785173416138 = 22.276064157485962 (normal) + 2.710721082985401 (augmented)  
[FFN] Epoch 7, Loss: 21.069168090820312 = 18.79582017660141 (normal) + 2.2733478993177414 (augmented)  
[FFN] Epoch 8, Loss: 17.22537511587143 = 14.875476509332657 (normal) + 2.349898785352707 (augmented)  
[FFN] Epoch 9, Loss: 14.716221243143082 = 12.131456643342972 (normal) + 2.5847645550966263 (augmented)  
[FFN] Epoch 10, Loss: 12.30150842666626 = 9.585377424955368 (normal) + 2.7161308899521828 (augmented)  
[FFN] Epoch 11, Loss: 11.69908556342125 = 8.832448810338974 (normal) + 2.866636745631695 (augmented)  
[FFN] Epoch 12, Loss: 11.778179734945297 = 8.76366400718689 (normal) + 3.014515742659569 (augmented)  
[FFN] Epoch 13, Loss: 10.879061341285706 = 8.010103449225426 (normal) + 2.86895789206028 (augmented)  
[FFN] Epoch 14, Loss: 10.209464520215988 = 7.443758934736252 (normal) + 2.7657055258750916 (augmented)  
[FFN] Epoch 15, Loss: 9.234358578920364 = 6.7131300419569016 (normal) + 2.5212284587323666 (augmented)  
[FFN] Epoch 16, Loss: 9.487176015973091 = 7.039257638156414 (normal) + 2.4479183070361614 (augmented)  
[FFN] Epoch 17, Loss: 9.599693477153778 = 7.059057556092739 (normal) + 2.5406359992921352 (augmented)  
[FFN] Epoch 18, Loss: 8.517339184880257 = 6.223226763308048 (normal) + 2.29411244019866 (augmented)  
[FFN] Epoch 19, Loss: 8.364412821829319 = 6.146990895271301 (normal) + 2.2174220010638237 (augmented)  
[FFN] Epoch 20, Loss: 9.391131184995174 = 7.512479208409786 (normal) + 1.878651898354292 (augmented)  
[FFN] Epoch 21, Loss: 8.915403261780739 = 7.102775312960148 (normal) + 1.8126279413700104 (augmented)  
[FFN] Epoch 22, Loss: 7.795233488082886 = 5.868324175477028 (normal) + 1.9269093908369541 (augmented)  
[FFN] Epoch 23, Loss: 8.499311536550522 = 6.45255708694458 (normal) + 2.0467544198036194 (augmented)  
[FFN] Epoch 24, Loss: 8.240238107740879 = 6.502160862088203 (normal) + 1.7380772307515144 (augmented)  
[FFN] Epoch 25, Loss: 8.78153520822525 = 7.090223960578442 (normal) + 1.6913112811744213 (augmented)  
[FFN] Epoch 26, Loss: 7.421294763684273 = 5.677902482450008 (normal) + 1.7433922737836838 (augmented)  
[FFN] Epoch 27, Loss: 8.761681318283081 = 6.861560769379139 (normal) + 1.9001205749809742 (augmented)  
[FFN] Epoch 28, Loss: 8.767196983098984 = 7.071755804121494 (normal) + 1.695441085845232 (augmented)  
[FFN] Epoch 29, Loss: 8.300750605762005 = 6.639778986573219 (normal) + 1.6609715819358826 (augmented)  
[FFN] Epoch 30, Loss: 8.057215012609959 = 6.283095471560955 (normal) + 1.7741195037961006 (augmented)  
[FFN] Epoch 31, Loss: 9.137152403593063 = 7.517552629113197 (normal) + 1.6195998191833496 (augmented)  
[FFN] Epoch 32, Loss: 8.17266345769167 = 6.520319849252701 (normal) + 1.6523435898125172 (augmented)  
[FFN] Epoch 33, Loss: 8.597095489501953 = 6.975644141435623 (normal) + 1.6214514076709747 (augmented)  
[FFN] Epoch 34, Loss: 7.951300762593746 = 6.403664469718933 (normal) + 1.5476363208144903 (augmented)  
[FFN] Epoch 35, Loss: 7.900326281785965 = 6.094210550189018 (normal) + 1.806115783751011 (augmented)  
[FFN] Epoch 36, Loss: 8.810261711478233 = 7.2630644626915455 (normal) + 1.5471972487866879 (augmented)  
[FFN] Epoch 37, Loss: 7.910108484327793 = 6.335181262344122 (normal) + 1.574927182868123 (augmented)  
[FFN] Epoch 38, Loss: 8.105365701019764 = 6.355888903141022 (normal) + 1.7494767419993877 (augmented)  
[FFN] Epoch 39, Loss: 9.552547559142113 = 7.997045993804932 (normal) + 1.5555015038698912 (augmented)  
[FFN] Epoch 40, Loss: 8.507044285535812 = 6.899378456175327 (normal) + 1.6076658219099045 (augmented)  
[FFN] Epoch 41, Loss: 8.714723825454712 = 6.922984514385462 (normal) + 1.7917393520474434 (augmented)  
[FFN] Epoch 42, Loss: 7.763393960893154 = 6.315577615052462 (normal) + 1.447816390544176 (augmented)  
[FFN] Epoch 43, Loss: 7.913701266050339 = 6.320525292307138 (normal) + 1.5931760258972645 (augmented)  
[FFN] Epoch 44, Loss: 8.426967650651932 = 6.931699778884649 (normal) + 1.4952678829431534 (augmented)  
[FFN] Epoch 45, Loss: 7.9806626588106155 = 6.583697408437729 (normal) + 1.396965304389596 (augmented)  
[FFN] Epoch 46, Loss: 8.65983460098505 = 7.2403435334563255 (normal) + 1.4194911494851112 (augmented)  
[FFN] Epoch 47, Loss: 7.8681831657886505 = 6.380821704864502 (normal) + 1.487361392006278 (augmented)  
[FFN] Epoch 48, Loss: 7.573106676340103 = 5.927589874714613 (normal) + 1.6455168016254902 (augmented)  
[FFN] Epoch 49, Loss: 8.092846147716045 = 6.340740967541933 (normal) + 1.7521051596850157 (augmented)  
[FFN] Epoch 50, Loss: 8.853324487805367 = 7.353352144360542 (normal) + 1.4999723099172115 (augmented)  
[FFN] Epoch 51, Loss: 7.9211646020412445 = 6.3146992437541485 (normal) + 1.60646540671587 (augmented)  
[FFN] Epoch 52, Loss: 8.721651390194893 = 7.099824704229832 (normal) + 1.621826745569706 (augmented)  
[FFN] Epoch 53, Loss: 8.56783775985241 = 6.998872697353363 (normal) + 1.5689650420099497 (augmented)  
[FFN] Epoch 54, Loss: 7.653368324041367 = 6.110851917415857 (normal) + 1.5425164252519608 (augmented)  
[FFN] Epoch 55, Loss: 8.678370475769043 = 7.064062960445881 (normal) + 1.614307539537549 (augmented)  
[FFN] Epoch 56, Loss: 8.081564731895924 = 6.580624025315046 (normal) + 1.5009407922625542 (augmented)  
[FFN] Epoch 57, Loss: 7.739203780889511 = 6.016361430287361 (normal) + 1.722842339426279 (augmented)  
[FFN] Epoch 58, Loss: 7.628098174929619 = 5.983503378927708 (normal) + 1.6445948220789433 (augmented)  
[FFN] Epoch 59, Loss: 7.809056535363197 = 6.376015357673168 (normal) + 1.4330411478877068 (augmented)  
[FFN] Epoch 60, Loss: 7.604614146053791 = 6.005808010697365 (normal) + 1.5988061428070068 (augmented)  
[FFN] Epoch 61, Loss: 7.633787676692009 = 6.123964332044125 (normal) + 1.5098234340548515 (augmented)  
[FFN] Epoch 62, Loss: 7.175523608922958 = 5.712043661624193 (normal) + 1.4634799472987652 (augmented)  
[FFN] Epoch 63, Loss: 7.38325235247612 = 5.701198033988476 (normal) + 1.6820542961359024 (augmented)  
[FFN] Epoch 64, Loss: 7.952879540622234 = 6.435249593108892 (normal) + 1.5176299884915352 (augmented)  
[FFN] Epoch 65, Loss: 8.366875298321247 = 6.8430071249604225 (normal) + 1.5238682385534048 (augmented)  
[FFN] Epoch 66, Loss: 6.272073514759541 = 4.856627926230431 (normal) + 1.4154456220567226 (augmented)  
[FFN] Epoch 67, Loss: 7.112013518810272 = 5.471736371517181 (normal) + 1.640277050435543 (augmented)  
[FFN] Epoch 68, Loss: 7.135308593511581 = 5.485838055610657 (normal) + 1.6494705006480217 (augmented)  
[FFN] Epoch 69, Loss: 8.066430382430553 = 6.532135244458914 (normal) + 1.5342951565980911 (augmented)  
[FFN] Epoch 70, Loss: 7.188036665320396 = 5.656393464654684 (normal) + 1.531643209978938 (augmented)  
[FFN] Epoch 71, Loss: 6.791474372148514 = 5.234565112739801 (normal) + 1.556909240782261 (augmented)  
[FFN] Epoch 72, Loss: 7.433862939476967 = 6.008923403918743 (normal) + 1.4249395970255136 (augmented)  
[FFN] Epoch 73, Loss: 7.239317536354065 = 5.667002286761999 (normal) + 1.5723152160644531 (augmented)  
[FFN] Epoch 74, Loss: 7.751548893749714 = 6.151451509445906 (normal) + 1.6000973880290985 (augmented)  
[FFN] Epoch 75, Loss: 7.245485849678516 = 5.786034949123859 (normal) + 1.4594508819282055 (augmented)  
[FFN] Epoch 76, Loss: 7.663305163383484 = 5.908028174191713 (normal) + 1.7552769649773836 (augmented)  
[FFN] Epoch 77, Loss: 7.734455466270447 = 6.025152254849672 (normal) + 1.7093032114207745 (augmented)  
[FFN] Epoch 78, Loss: 7.2029067277908325 = 5.6548136211931705 (normal) + 1.5480930544435978 (augmented)  
[FFN] Epoch 79, Loss: 7.925217971205711 = 6.378210820257664 (normal) + 1.5470071882009506 (augmented)  
[FFN] Epoch 80, Loss: 7.614616625010967 = 6.050478104501963 (normal) + 1.5641385111957788 (augmented)  
[FFN] Epoch 81, Loss: 7.364678472280502 = 5.805435825139284 (normal) + 1.55924266949296 (augmented)  
[FFN] Epoch 82, Loss: 8.892547816038132 = 7.179862376302481 (normal) + 1.7126853875815868 (augmented)  
[FFN] Epoch 83, Loss: 6.779293201863766 = 5.020713157951832 (normal) + 1.7585800141096115 (augmented)  
[FFN] Epoch 84, Loss: 7.015918388962746 = 5.476920410990715 (normal) + 1.5389980375766754 (augmented)  
[FFN] Epoch 85, Loss: 8.107184156775475 = 6.609476838260889 (normal) + 1.4977072793990374 (augmented)  
[FFN] Epoch 86, Loss: 8.115478992462158 = 6.302803099155426 (normal) + 1.8126758821308613 (augmented)  
[FFN] Epoch 87, Loss: 7.6512420400977135 = 5.979116551578045 (normal) + 1.6721254158765078 (augmented)  
[FFN] Epoch 88, Loss: 7.570375092327595 = 6.055956408381462 (normal) + 1.5144186280667782 (augmented)  
[FFN] Epoch 89, Loss: 7.2244879230856895 = 5.646649140864611 (normal) + 1.5778388045728207 (augmented)  
[FFN] Epoch 90, Loss: 7.463232904672623 = 5.793510269373655 (normal) + 1.6697225794196129 (augmented)  
[FFN] Epoch 91, Loss: 7.083487883210182 = 5.327218841761351 (normal) + 1.756269033998251 (augmented)  
[FFN] Epoch 92, Loss: 7.613562174141407 = 6.032775912433863 (normal) + 1.5807862915098667 (augmented)  
[FFN] Epoch 93, Loss: 7.5433773547410965 = 5.868953343480825 (normal) + 1.674423985183239 (augmented)  
[FFN] Epoch 94, Loss: 7.23478252440691 = 5.853172622621059 (normal) + 1.3816098533570766 (augmented)  
[FFN] Epoch 95, Loss: 6.828717291355133 = 5.228927794843912 (normal) + 1.5997894704341888 (augmented)  
[FFN] Epoch 96, Loss: 7.110130287706852 = 5.305318389087915 (normal) + 1.8048118501901627 (augmented)  
[FFN] Epoch 97, Loss: 7.188512474298477 = 5.536770682781935 (normal) + 1.6517417915165424 (augmented)  
[FFN] Epoch 98, Loss: 6.8602325692772865 = 5.144012309610844 (normal) + 1.7162202447652817 (augmented)  
[FFN] Epoch 99, Loss: 8.11688606441021 = 6.447159305214882 (normal) + 1.669726774096489 (augmented)  
[FFN] Epoch 100, Loss: 8.136139757931232 = 6.513203460723162 (normal) + 1.6229363083839417 (augmented)  
Training complete.
Modular training for SA module, encoder 2
LOAD FROM SAVE
Encoder idx = 2
Batch count :  32
[MHA]Epoch 1, Loss: 5.974022880196571 = 3.02186768501997 (normal) + 2.9521551728248596 (augmented)  
[MHA]Epoch 2, Loss: 4.440863259136677 = 2.6133186668157578 (normal) + 1.8275445587933064 (augmented)  
[MHA]Epoch 3, Loss: 4.1135760843753815 = 2.5242648273706436 (normal) + 1.5893112011253834 (augmented)  
[MHA]Epoch 4, Loss: 3.8617665767669678 = 2.423178520053625 (normal) + 1.4385880418121815 (augmented)  
[MHA]Epoch 5, Loss: 3.6397057250142097 = 2.319511439651251 (normal) + 1.32019430026412 (augmented)  
[MHA]Epoch 6, Loss: 3.4778864085674286 = 2.233766544610262 (normal) + 1.2441198509186506 (augmented)  
[MHA]Epoch 7, Loss: 3.339176058769226 = 2.164480574429035 (normal) + 1.1746955029666424 (augmented)  
[MHA]Epoch 8, Loss: 3.231530524790287 = 2.1065710447728634 (normal) + 1.124959459528327 (augmented)  
[MHA]Epoch 9, Loss: 3.126233510673046 = 2.055414468050003 (normal) + 1.0708190277218819 (augmented)  
[MHA]Epoch 10, Loss: 3.0396443977952003 = 2.009991370141506 (normal) + 1.029653014615178 (augmented)  
[MHA]Epoch 11, Loss: 2.9604565501213074 = 1.9684623405337334 (normal) + 0.99199422262609 (augmented)  
[MHA]Epoch 12, Loss: 2.8910129070281982 = 1.9312417916953564 (normal) + 0.9597710929811001 (augmented)  
[MHA]Epoch 13, Loss: 2.8300327509641647 = 1.8965940289199352 (normal) + 0.9334387145936489 (augmented)  
[MHA]Epoch 14, Loss: 2.7641373351216316 = 1.8644467778503895 (normal) + 0.899690542370081 (augmented)  
[MHA]Epoch 15, Loss: 2.722394421696663 = 1.8376251086592674 (normal) + 0.884769294410944 (augmented)  
[MHA]Epoch 16, Loss: 2.671555958688259 = 1.8124778792262077 (normal) + 0.859078049659729 (augmented)  
[MHA]Epoch 17, Loss: 2.620327763259411 = 1.7889857180416584 (normal) + 0.8313420470803976 (augmented)  
[MHA]Epoch 18, Loss: 2.593117356300354 = 1.7695793844759464 (normal) + 0.8235379680991173 (augmented)  
[MHA]Epoch 19, Loss: 2.550627328455448 = 1.7479076460003853 (normal) + 0.8027196396142244 (augmented)  
[MHA]Epoch 20, Loss: 2.523370712995529 = 1.7322221882641315 (normal) + 0.7911485228687525 (augmented)  
[MHA]Epoch 21, Loss: 2.488745331764221 = 1.7125960253179073 (normal) + 0.7761492934077978 (augmented)  
[MHA]Epoch 22, Loss: 2.4609955176711082 = 1.6980476900935173 (normal) + 0.7629478201270103 (augmented)  
[MHA]Epoch 23, Loss: 2.4229790940880775 = 1.680042002350092 (normal) + 0.7429370731115341 (augmented)  
[MHA]Epoch 24, Loss: 2.3973056599497795 = 1.6661653332412243 (normal) + 0.7311403173953295 (augmented)  
[MHA]Epoch 25, Loss: 2.3749330155551434 = 1.6493215411901474 (normal) + 0.7256114594638348 (augmented)  
[MHA]Epoch 26, Loss: 2.3452581763267517 = 1.6367101520299911 (normal) + 0.7085480261594057 (augmented)  
[MHA]Epoch 27, Loss: 2.335357129573822 = 1.6256123594939709 (normal) + 0.7097447440028191 (augmented)  
[MHA]Epoch 28, Loss: 2.3096270225942135 = 1.61343190446496 (normal) + 0.6961950976401567 (augmented)  
[MHA]Epoch 29, Loss: 2.2796697802841663 = 1.5997988916933537 (normal) + 0.6798708904534578 (augmented)  
[MHA]Epoch 30, Loss: 2.25309482216835 = 1.5866114906966686 (normal) + 0.6664833407849073 (augmented)  
[MHA]Epoch 31, Loss: 2.237636100500822 = 1.575346201658249 (normal) + 0.6622898736968637 (augmented)  
[MHA]Epoch 32, Loss: 2.2273611314594746 = 1.5658287331461906 (normal) + 0.661532374098897 (augmented)  
[MHA]Epoch 33, Loss: 2.2069133557379246 = 1.5541085079312325 (normal) + 0.6528048533946276 (augmented)  
[MHA]Epoch 34, Loss: 2.1798089742660522 = 1.5431717447936535 (normal) + 0.6366372229531407 (augmented)  
[MHA]Epoch 35, Loss: 2.1618087477982044 = 1.533505879342556 (normal) + 0.628302869386971 (augmented)  
[MHA]Epoch 36, Loss: 2.1514012217521667 = 1.5285024270415306 (normal) + 0.6228987844660878 (augmented)  
[MHA]Epoch 37, Loss: 2.1334173381328583 = 1.5157206542789936 (normal) + 0.6176966726779938 (augmented)  
[MHA]Epoch 38, Loss: 2.1140429116785526 = 1.5085261091589928 (normal) + 0.6055168006569147 (augmented)  
[MHA]Epoch 39, Loss: 2.1084588430821896 = 1.5018647201359272 (normal) + 0.6065941136330366 (augmented)  
[MHA]Epoch 40, Loss: 2.0960537306964397 = 1.497336808592081 (normal) + 0.5987169090658426 (augmented)  
[MHA]Epoch 41, Loss: 2.074093274772167 = 1.4830019101500511 (normal) + 0.591091375797987 (augmented)  
[MHA]Epoch 42, Loss: 2.062368642538786 = 1.4790481328964233 (normal) + 0.583320515230298 (augmented)  
[MHA]Epoch 43, Loss: 2.0372181311249733 = 1.467428207397461 (normal) + 0.5697899125516415 (augmented)  
[MHA]Epoch 44, Loss: 2.0357970893383026 = 1.4637205190956593 (normal) + 0.5720765683799982 (augmented)  
[MHA]Epoch 45, Loss: 2.0281042493879795 = 1.4607403837144375 (normal) + 0.5673638498410583 (augmented)  
[MHA]Epoch 46, Loss: 2.007399369031191 = 1.4494025222957134 (normal) + 0.5579968476668 (augmented)  
[MHA]Epoch 47, Loss: 1.99936094135046 = 1.4439302757382393 (normal) + 0.5554306553676724 (augmented)  
[MHA]Epoch 48, Loss: 1.9898803122341633 = 1.4362637139856815 (normal) + 0.5536165740340948 (augmented)  
[MHA]Epoch 49, Loss: 1.9701007083058357 = 1.4313546307384968 (normal) + 0.5387460868805647 (augmented)  
[MHA]Epoch 50, Loss: 1.9607155993580818 = 1.4222053103148937 (normal) + 0.5385102899745107 (augmented)  
[MHA]Epoch 51, Loss: 1.9398260526359081 = 1.415012989193201 (normal) + 0.5248130634427071 (augmented)  
[MHA]Epoch 52, Loss: 1.9441706240177155 = 1.4142149612307549 (normal) + 0.5299556562677026 (augmented)  
[MHA]Epoch 53, Loss: 1.939254753291607 = 1.4063375145196915 (normal) + 0.5329172341153026 (augmented)  
[MHA]Epoch 54, Loss: 1.9065542668104172 = 1.3980945609509945 (normal) + 0.5084596993401647 (augmented)  
[MHA]Epoch 55, Loss: 1.9043301083147526 = 1.3959001079201698 (normal) + 0.5084299808368087 (augmented)  
[MHA]Epoch 56, Loss: 1.893172662705183 = 1.3892271183431149 (normal) + 0.5039455378428102 (augmented)  
[MHA]Epoch 57, Loss: 1.8798220418393612 = 1.386369340121746 (normal) + 0.4934526924043894 (augmented)  
[MHA]Epoch 58, Loss: 1.8734219707548618 = 1.3809938989579678 (normal) + 0.492428089492023 (augmented)  
[MHA]Epoch 59, Loss: 1.8706580139696598 = 1.3759003952145576 (normal) + 0.49475760385394096 (augmented)  
[MHA]Epoch 60, Loss: 1.850121658295393 = 1.3676749467849731 (normal) + 0.482446719892323 (augmented)  
[MHA]Epoch 61, Loss: 1.8447596803307533 = 1.365148689597845 (normal) + 0.4796109823510051 (augmented)  
[MHA]Epoch 62, Loss: 1.8356645293533802 = 1.3615378700196743 (normal) + 0.4741266518831253 (augmented)  
[MHA]Epoch 63, Loss: 1.8291120938956738 = 1.3554480522871017 (normal) + 0.4736640378832817 (augmented)  
[MHA]Epoch 64, Loss: 1.8181017488241196 = 1.3523230664432049 (normal) + 0.4657786600291729 (augmented)  
[MHA]Epoch 65, Loss: 1.820072416216135 = 1.3491846323013306 (normal) + 0.47088777646422386 (augmented)  
[MHA]Epoch 66, Loss: 1.796354815363884 = 1.3403477668762207 (normal) + 0.4560070317238569 (augmented)  
[MHA]Epoch 67, Loss: 1.7903121709823608 = 1.3389459066092968 (normal) + 0.4513662541285157 (augmented)  
[MHA]Epoch 68, Loss: 1.7743460983037949 = 1.3306987583637238 (normal) + 0.4436473362147808 (augmented)  
[MHA]Epoch 69, Loss: 1.7764066569507122 = 1.328939251601696 (normal) + 0.44746739231050014 (augmented)  
[MHA]Epoch 70, Loss: 1.7602365091443062 = 1.3230509795248508 (normal) + 0.43718551844358444 (augmented)  
[MHA]Epoch 71, Loss: 1.7493622973561287 = 1.3176844008266926 (normal) + 0.4316778862848878 (augmented)  
[MHA]Epoch 72, Loss: 1.7413096278905869 = 1.3127247095108032 (normal) + 0.4285849258303642 (augmented)  
[MHA]Epoch 73, Loss: 1.739364329725504 = 1.3104877024888992 (normal) + 0.42887662164866924 (augmented)  
[MHA]Epoch 74, Loss: 1.7396447621285915 = 1.3082625940442085 (normal) + 0.4313821541145444 (augmented)  
[MHA]Epoch 75, Loss: 1.7288502156734467 = 1.3042021710425615 (normal) + 0.42464803345501423 (augmented)  
[MHA]Epoch 76, Loss: 1.7255029566586018 = 1.3057668916881084 (normal) + 0.419736054725945 (augmented)  
[MHA]Epoch 77, Loss: 1.7308056242763996 = 1.3030451722443104 (normal) + 0.42776043992489576 (augmented)  
[MHA]Epoch 78, Loss: 1.7087496556341648 = 1.2963626198470592 (normal) + 0.41238703206181526 (augmented)  
[MHA]Epoch 79, Loss: 1.722898755222559 = 1.3011974766850471 (normal) + 0.4217012757435441 (augmented)  
[MHA]Epoch 80, Loss: 1.705688003450632 = 1.2931895833462477 (normal) + 0.41249841917306185 (augmented)  
[MHA]Epoch 81, Loss: 1.6944802775979042 = 1.28517284989357 (normal) + 0.4093074155971408 (augmented)  
[MHA]Epoch 82, Loss: 1.692327331751585 = 1.2840319257229567 (normal) + 0.40829539857804775 (augmented)  
[MHA]Epoch 83, Loss: 1.6842425167560577 = 1.2785544451326132 (normal) + 0.40568806417286396 (augmented)  
[MHA]Epoch 84, Loss: 1.6755604445934296 = 1.2750221863389015 (normal) + 0.40053824707865715 (augmented)  
[MHA]Epoch 85, Loss: 1.6717869341373444 = 1.2738525066524744 (normal) + 0.39793440885841846 (augmented)  
[MHA]Epoch 86, Loss: 1.6622778698801994 = 1.270922226831317 (normal) + 0.39135562162846327 (augmented)  
[MHA]Epoch 87, Loss: 1.6542585082352161 = 1.2657415214926004 (normal) + 0.3885169802233577 (augmented)  
[MHA]Epoch 88, Loss: 1.6516266614198685 = 1.2660415545105934 (normal) + 0.3855851013213396 (augmented)  
[MHA]Epoch 89, Loss: 1.635017056018114 = 1.2569782249629498 (normal) + 0.3780388319864869 (augmented)  
[MHA]Epoch 90, Loss: 1.624129131436348 = 1.2536274138838053 (normal) + 0.37050170358270407 (augmented)  
[MHA]Epoch 91, Loss: 1.626603089272976 = 1.25243179500103 (normal) + 0.37417128775268793 (augmented)  
[MHA]Epoch 92, Loss: 1.6206389404833317 = 1.250327568501234 (normal) + 0.370311358012259 (augmented)  
[MHA]Epoch 93, Loss: 1.6054565533995628 = 1.2436867710202932 (normal) + 0.3617697609588504 (augmented)  
[MHA]Epoch 94, Loss: 1.5995814129710197 = 1.2399773579090834 (normal) + 0.3596040513366461 (augmented)  
[MHA]Epoch 95, Loss: 1.5928314067423344 = 1.2360577434301376 (normal) + 0.35677365586161613 (augmented)  
[MHA]Epoch 96, Loss: 1.5864256657660007 = 1.2344497349113226 (normal) + 0.35197592712938786 (augmented)  
[MHA]Epoch 97, Loss: 1.5783470384776592 = 1.2300226800143719 (normal) + 0.3483243463560939 (augmented)  
[MHA]Epoch 98, Loss: 1.5820574015378952 = 1.2321664355695248 (normal) + 0.3498909492045641 (augmented)  
[MHA]Epoch 99, Loss: 1.56901765614748 = 1.223925705999136 (normal) + 0.34509195294231176 (augmented)  
[MHA]Epoch 100, Loss: 1.5688257180154324 = 1.2229741029441357 (normal) + 0.34585162438452244 (augmented)  
EC  0
Training complete.
Generating data for encoder 3
[nltk_data] Downloading package stopwords to /home/a40-ko-
[nltk_data]     lab/nltk_data...
[nltk_data]   Package stopwords is already up-to-date!
BertModel loaded from: transformers.models.bert.modeling_bert
Transformers module loaded from: /home/a40-ko-lab/miniconda3/envs/jiantenv/lib/python3.9/site-packages/transformers/__init__.py
06/12/2024 13:40:29 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False
06/12/2024 13:40:29 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
batch_eval_metrics=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_steps=None,
eval_strategy=no,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/tmp/cb/runs/Jun12_13-40-29_a40kolab,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=100.0,
optim=adamw_torch,
optim_args=None,
optim_target_modules=None,
output_dir=/tmp/cb/,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=/tmp/cb/,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
[INFO|configuration_utils.py:731] 2024-06-12 13:40:29,440 >> loading configuration file ./downloads/cb_config/config.json
[INFO|configuration_utils.py:800] 2024-06-12 13:40:29,441 >> Model config DistilBertConfig {
  "_name_or_path": "./downloads/cb_config",
  "activation": "gelu",
  "architectures": [
    "DistilBertForMaskedLM"
  ],
  "attention_dropout": 0.1,
  "dim": 768,
  "dropout": 0.1,
  "finetuning_task": "cb",
  "hidden_dim": 3072,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 512,
  "model_type": "distilbert",
  "n_heads": 12,
  "n_layers": 6,
  "pad_token_id": 0,
  "qa_dropout": 0.1,
  "seq_classif_dropout": 0.2,
  "sinusoidal_pos_embds": false,
  "tie_weights_": true,
  "transformers_version": "4.42.0.dev0",
  "vocab_size": 30522
}

[INFO|tokenization_utils_base.py:2107] 2024-06-12 13:40:29,442 >> loading file vocab.txt
[INFO|tokenization_utils_base.py:2107] 2024-06-12 13:40:29,442 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2107] 2024-06-12 13:40:29,443 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2107] 2024-06-12 13:40:29,443 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2107] 2024-06-12 13:40:29,443 >> loading file tokenizer_config.json
[INFO|configuration_utils.py:731] 2024-06-12 13:40:29,458 >> loading configuration file ./downloads/cb_model/config.json
[INFO|configuration_utils.py:800] 2024-06-12 13:40:29,459 >> Model config DistilBertConfig {
  "_name_or_path": "./downloads/cb_model",
  "activation": "gelu",
  "architectures": [
    "DistilBertForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "dim": 768,
  "dropout": 0.1,
  "finetuning_task": "cb",
  "hidden_dim": 3072,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 512,
  "model_type": "distilbert",
  "n_heads": 12,
  "n_layers": 6,
  "pad_token_id": 0,
  "qa_dropout": 0.1,
  "seq_classif_dropout": 0.2,
  "sinusoidal_pos_embds": false,
  "tie_weights_": true,
  "torch_dtype": "float32",
  "transformers_version": "4.42.0.dev0",
  "vocab_size": 30522
}

[INFO|modeling_utils.py:3474] 2024-06-12 13:40:29,484 >> loading weights file ./downloads/cb_model/model.safetensors
[INFO|modeling_utils.py:4283] 2024-06-12 13:40:29,521 >> All model checkpoint weights were used when initializing DistilBertForSequenceClassification.

[INFO|modeling_utils.py:4291] 2024-06-12 13:40:29,521 >> All the weights of DistilBertForSequenceClassification were initialized from the model checkpoint at ./downloads/cb_model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForSequenceClassification for predictions without further training.
Loading cached processed dataset at /media/a40-ko-lab/c61d4ea0-b8fe-4205-9815-f7d3d054043c/sayed/Sayed/transformers/weight_sharing/modular_superglue/downloads/cb/train/cache-ec9a28bf6cd38919.arrow
Loaded dataset from disk
06/12/2024 13:40:29 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /media/a40-ko-lab/c61d4ea0-b8fe-4205-9815-f7d3d054043c/sayed/Sayed/transformers/weight_sharing/modular_superglue/downloads/cb/train/cache-ec9a28bf6cd38919.arrow
Loading cached processed dataset at /media/a40-ko-lab/c61d4ea0-b8fe-4205-9815-f7d3d054043c/sayed/Sayed/transformers/weight_sharing/modular_superglue/downloads/cb/validation/cache-462a1cd6bd454452.arrow
06/12/2024 13:40:29 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /media/a40-ko-lab/c61d4ea0-b8fe-4205-9815-f7d3d054043c/sayed/Sayed/transformers/weight_sharing/modular_superglue/downloads/cb/validation/cache-462a1cd6bd454452.arrow
Loading cached processed dataset at /media/a40-ko-lab/c61d4ea0-b8fe-4205-9815-f7d3d054043c/sayed/Sayed/transformers/weight_sharing/modular_superglue/downloads/cb/test/cache-f44364b0f28c5bd9.arrow
06/12/2024 13:40:29 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /media/a40-ko-lab/c61d4ea0-b8fe-4205-9815-f7d3d054043c/sayed/Sayed/transformers/weight_sharing/modular_superglue/downloads/cb/test/cache-f44364b0f28c5bd9.arrow
06/12/2024 13:40:29 - INFO - __main__ - Sample 163 of the training set: {'input_ids': [101, 1037, 1024, 2031, 2017, 2628, 2008, 2200, 2172, 2030, 1010, 1038, 1024, 7910, 1010, 2025, 2428, 1012, 1045, 2123, 1005, 1056, 2228, 2505, 2097, 2412, 2202, 2058, 1996, 5088, 1012, 102, 2242, 2097, 2202, 2058, 1996, 5088, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
06/12/2024 13:40:29 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:3753] 2024-06-12 13:40:29,744 >> ***** Running Evaluation *****
[INFO|trainer.py:3755] 2024-06-12 13:40:29,745 >>   Num examples = 250
[INFO|trainer.py:3758] 2024-06-12 13:40:29,745 >>   Batch size = 8
Directory './saves/distilbert/distilbert-base-uncased/cb/mha/outputs/encoder_3' created successfully.
Directory './saves/distilbert/distilbert-base-uncased/cb/ffn/inputs/encoder_3' created successfully.
Directory './saves/distilbert/distilbert-base-uncased/cb/ffn/outputs/encoder_3' created successfully.
Directory './saves/distilbert/distilbert-base-uncased/cb/mha/inputs/encoder_3' created successfully.
Batch Idx  1
  0%|          | 0/32 [00:00<?, ?it/s] 16%|█▌        | 5/32 [00:00<00:00, 45.99it/s] 31%|███▏      | 10/32 [00:00<00:00, 43.88it/s] 47%|████▋     | 15/32 [00:00<00:00, 43.32it/s] 62%|██████▎   | 20/32 [00:00<00:00, 41.99it/s] 78%|███████▊  | 25/32 [00:00<00:00, 42.24it/s] 94%|█████████▍| 30/32 [00:00<00:00, 42.45it/s]100%|██████████| 32/32 [00:00<00:00, 43.48it/s]
Batch Idx  2
Batch Idx  3
Batch Idx  4
Batch Idx  5
Batch Idx  6
Batch Idx  7
Batch Idx  8
Batch Idx  9
Batch Idx  10
Batch Idx  11
Batch Idx  12
Batch Idx  13
Batch Idx  14
Batch Idx  15
Batch Idx  16
Batch Idx  17
Batch Idx  18
Batch Idx  19
Batch Idx  20
Batch Idx  21
Batch Idx  22
Batch Idx  23
Batch Idx  24
Batch Idx  25
Batch Idx  26
Batch Idx  27
Batch Idx  28
Batch Idx  29
Batch Idx  30
Batch Idx  31
Batch Idx  32
***** eval metrics *****
  eval_runtime            = 0:00:00.85
  eval_samples            =        250
  eval_samples_per_second =    294.073
  eval_steps_per_second   =     37.641
Data generated succesfully

 
 
 Beginning modular training of SA and BL modules
Encoder compression: 2
Modular training for BL module, encoder 3
LOAD FROM SAVE
Encoder idx = 3
Compression = 2
[FFN] Epoch 1, Loss: 5.6001351326704025 = 4.059566296637058 (normal) + 1.5405688136816025 (augmented)  
[FFN] Epoch 2, Loss: 4.4934509471058846 = 3.364527717232704 (normal) + 1.1289232186973095 (augmented)  
[FFN] Epoch 3, Loss: 4.054900668561459 = 3.011661283671856 (normal) + 1.0432393942028284 (augmented)  
[FFN] Epoch 4, Loss: 3.771470382809639 = 2.779017962515354 (normal) + 0.9924524109810591 (augmented)  
[FFN] Epoch 5, Loss: 3.5731568187475204 = 2.620521806180477 (normal) + 0.9526350107043982 (augmented)  
[FFN] Epoch 6, Loss: 3.427543632686138 = 2.5055012553930283 (normal) + 0.9220423139631748 (augmented)  
[FFN] Epoch 7, Loss: 3.3055241256952286 = 2.412143751978874 (normal) + 0.8933803457766771 (augmented)  
[FFN] Epoch 8, Loss: 3.2004536166787148 = 2.3320713862776756 (normal) + 0.8683822024613619 (augmented)  
[FFN] Epoch 9, Loss: 3.112744338810444 = 2.26380954682827 (normal) + 0.8489347472786903 (augmented)  
[FFN] Epoch 10, Loss: 3.0307550504803658 = 2.2018403112888336 (normal) + 0.828914737328887 (augmented)  
[FFN] Epoch 11, Loss: 2.960914969444275 = 2.1500369235873222 (normal) + 0.8108780160546303 (augmented)  
[FFN] Epoch 12, Loss: 2.901189461350441 = 2.1055100560188293 (normal) + 0.7956793829798698 (augmented)  
[FFN] Epoch 13, Loss: 2.844331316649914 = 2.0631407648324966 (normal) + 0.7811905257403851 (augmented)  
[FFN] Epoch 14, Loss: 2.798844389617443 = 2.025290112942457 (normal) + 0.7735542543232441 (augmented)  
[FFN] Epoch 15, Loss: 2.7491282746195793 = 1.9879368133842945 (normal) + 0.761191425845027 (augmented)  
[FFN] Epoch 16, Loss: 2.7089722007513046 = 1.9585431441664696 (normal) + 0.750429030507803 (augmented)  
[FFN] Epoch 17, Loss: 2.668003559112549 = 1.9286866299808025 (normal) + 0.7393168974667788 (augmented)  
[FFN] Epoch 18, Loss: 2.6339566111564636 = 1.9016695991158485 (normal) + 0.7322869896888733 (augmented)  
[FFN] Epoch 19, Loss: 2.6023863330483437 = 1.8774190805852413 (normal) + 0.7249672338366508 (augmented)  
[FFN] Epoch 20, Loss: 2.5735368952155113 = 1.858940239995718 (normal) + 0.7145966421812773 (augmented)  
[FFN] Epoch 21, Loss: 2.544602669775486 = 1.8366525284945965 (normal) + 0.7079501133412123 (augmented)  
[FFN] Epoch 22, Loss: 2.519855409860611 = 1.814822107553482 (normal) + 0.7050332836806774 (augmented)  
[FFN] Epoch 23, Loss: 2.496959187090397 = 1.798917569220066 (normal) + 0.6980415880680084 (augmented)  
[FFN] Epoch 24, Loss: 2.4748176261782646 = 1.7827207520604134 (normal) + 0.6920968685299158 (augmented)  
[FFN] Epoch 25, Loss: 2.453537590801716 = 1.7658259086310863 (normal) + 0.6877116840332747 (augmented)  
[FFN] Epoch 26, Loss: 2.432887479662895 = 1.7498191259801388 (normal) + 0.6830683369189501 (augmented)  
[FFN] Epoch 27, Loss: 2.417008765041828 = 1.7375468760728836 (normal) + 0.6794618833810091 (augmented)  
[FFN] Epoch 28, Loss: 2.4024634063243866 = 1.7266732826828957 (normal) + 0.6757901217788458 (augmented)  
[FFN] Epoch 29, Loss: 2.382050819694996 = 1.7116863876581192 (normal) + 0.6703644450753927 (augmented)  
[FFN] Epoch 30, Loss: 2.3660939410328865 = 1.6995619721710682 (normal) + 0.666531927883625 (augmented)  
[FFN] Epoch 31, Loss: 2.3558261804282665 = 1.6913458295166492 (normal) + 0.6644803453236818 (augmented)  
[FFN] Epoch 32, Loss: 2.3403053507208824 = 1.6771273724734783 (normal) + 0.6631779931485653 (augmented)  
[FFN] Epoch 33, Loss: 2.3277200162410736 = 1.6674399115145206 (normal) + 0.6602800861001015 (augmented)  
[FFN] Epoch 34, Loss: 2.314656265079975 = 1.6580086797475815 (normal) + 0.6566475685685873 (augmented)  
[FFN] Epoch 35, Loss: 2.3002135306596756 = 1.6471657380461693 (normal) + 0.6530477926135063 (augmented)  
[FFN] Epoch 36, Loss: 2.2913355380296707 = 1.6399004608392715 (normal) + 0.6514350641518831 (augmented)  
[FFN] Epoch 37, Loss: 2.281615301966667 = 1.6305911615490913 (normal) + 0.6510241255164146 (augmented)  
[FFN] Epoch 38, Loss: 2.271876499056816 = 1.6223039142787457 (normal) + 0.6495725940912962 (augmented)  
[FFN] Epoch 39, Loss: 2.2636165805161 = 1.6180045828223228 (normal) + 0.6456119883805513 (augmented)  
[FFN] Epoch 40, Loss: 2.255740098655224 = 1.6087846010923386 (normal) + 0.6469554845243692 (augmented)  
[FFN] Epoch 41, Loss: 2.245501834899187 = 1.6007849462330341 (normal) + 0.6447168923914433 (augmented)  
[FFN] Epoch 42, Loss: 2.232021525502205 = 1.591284353286028 (normal) + 0.6407371573150158 (augmented)  
[FFN] Epoch 43, Loss: 2.2244836017489433 = 1.5843480564653873 (normal) + 0.640135545283556 (augmented)  
[FFN] Epoch 44, Loss: 2.21529471129179 = 1.5797907374799252 (normal) + 0.6355039514601231 (augmented)  
[FFN] Epoch 45, Loss: 2.2143635526299477 = 1.5730657316744328 (normal) + 0.6412978284060955 (augmented)  
[FFN] Epoch 46, Loss: 2.2042098604142666 = 1.567042164504528 (normal) + 0.6371676791459322 (augmented)  
[FFN] Epoch 47, Loss: 2.1983504071831703 = 1.5604949407279491 (normal) + 0.6378554552793503 (augmented)  
[FFN] Epoch 48, Loss: 2.190650973469019 = 1.5533930249512196 (normal) + 0.6372579298913479 (augmented)  
[FFN] Epoch 49, Loss: 2.186000343412161 = 1.5486019104719162 (normal) + 0.6373984385281801 (augmented)  
[FFN] Epoch 50, Loss: 2.176229815930128 = 1.5449227280914783 (normal) + 0.631307065486908 (augmented)  
[FFN] Epoch 51, Loss: 2.171301655471325 = 1.5380709916353226 (normal) + 0.633230660110712 (augmented)  
[FFN] Epoch 52, Loss: 2.1682174392044544 = 1.5359599478542805 (normal) + 0.632257491350174 (augmented)  
[FFN] Epoch 53, Loss: 2.1583399437367916 = 1.5285710208117962 (normal) + 0.62976891733706 (augmented)  
[FFN] Epoch 54, Loss: 2.1510842069983482 = 1.5221329778432846 (normal) + 0.6289512235671282 (augmented)  
[FFN] Epoch 55, Loss: 2.1496076472103596 = 1.520555030554533 (normal) + 0.629052646458149 (augmented)  
[FFN] Epoch 56, Loss: 2.143684256821871 = 1.5154321230947971 (normal) + 0.6282521430402994 (augmented)  
[FFN] Epoch 57, Loss: 2.136985681951046 = 1.5088968239724636 (normal) + 0.6280888542532921 (augmented)  
[FFN] Epoch 58, Loss: 2.1310249492526054 = 1.5054224245250225 (normal) + 0.625602513551712 (augmented)  
[FFN] Epoch 59, Loss: 2.129212949424982 = 1.5020841024816036 (normal) + 0.6271288599818945 (augmented)  
[FFN] Epoch 60, Loss: 2.1205655559897423 = 1.4960117638111115 (normal) + 0.6245537754148245 (augmented)  
[FFN] Epoch 61, Loss: 2.1216126643121243 = 1.4939216002821922 (normal) + 0.6276910379528999 (augmented)  
[FFN] Epoch 62, Loss: 2.117622446268797 = 1.4905274659395218 (normal) + 0.6270949803292751 (augmented)  
[FFN] Epoch 63, Loss: 2.1079691350460052 = 1.4840024672448635 (normal) + 0.6239666808396578 (augmented)  
[FFN] Epoch 64, Loss: 2.1064365804195404 = 1.4831092208623886 (normal) + 0.6233273670077324 (augmented)  
[FFN] Epoch 65, Loss: 2.1079890727996826 = 1.4809316098690033 (normal) + 0.6270574666559696 (augmented)  
[FFN] Epoch 66, Loss: 2.0991896018385887 = 1.4740391112864017 (normal) + 0.625150490552187 (augmented)  
[FFN] Epoch 67, Loss: 2.097052987664938 = 1.472135353833437 (normal) + 0.6249176263809204 (augmented)  
[FFN] Epoch 68, Loss: 2.0886935740709305 = 1.4671352319419384 (normal) + 0.6215583346784115 (augmented)  
[FFN] Epoch 69, Loss: 2.0925935991108418 = 1.4672145396471024 (normal) + 0.6253790445625782 (augmented)  
[FFN] Epoch 70, Loss: 2.083579406142235 = 1.4619598537683487 (normal) + 0.6216195411980152 (augmented)  
[FFN] Epoch 71, Loss: 2.0802619606256485 = 1.4587485939264297 (normal) + 0.6215133629739285 (augmented)  
[FFN] Epoch 72, Loss: 2.077780809253454 = 1.455894809216261 (normal) + 0.6218860000371933 (augmented)  
[FFN] Epoch 73, Loss: 2.073361475020647 = 1.4533077999949455 (normal) + 0.6200536731630564 (augmented)  
[FFN] Epoch 74, Loss: 2.0795437544584274 = 1.453685648739338 (normal) + 0.6258580945432186 (augmented)  
[FFN] Epoch 75, Loss: 2.0756914764642715 = 1.448233138769865 (normal) + 0.6274583227932453 (augmented)  
[FFN] Epoch 76, Loss: 2.0691298581659794 = 1.4453538581728935 (normal) + 0.623775988817215 (augmented)  
[FFN] Epoch 77, Loss: 2.0705810636281967 = 1.445805635303259 (normal) + 0.6247754208743572 (augmented)  
[FFN] Epoch 78, Loss: 2.0633603371679783 = 1.4404626414179802 (normal) + 0.6228976845741272 (augmented)  
[FFN] Epoch 79, Loss: 2.0637910440564156 = 1.438954170793295 (normal) + 0.6248368918895721 (augmented)  
[FFN] Epoch 80, Loss: 2.0588786229491234 = 1.4358487725257874 (normal) + 0.6230298336595297 (augmented)  
[FFN] Epoch 81, Loss: 2.0570512674748898 = 1.4331129565835 (normal) + 0.6239383108913898 (augmented)  
[FFN] Epoch 82, Loss: 2.0619144812226295 = 1.4342737831175327 (normal) + 0.6276406943798065 (augmented)  
[FFN] Epoch 83, Loss: 2.0495773926377296 = 1.4273623749613762 (normal) + 0.6222150139510632 (augmented)  
[FFN] Epoch 84, Loss: 2.042544763535261 = 1.4249052591621876 (normal) + 0.6176394950598478 (augmented)  
[FFN] Epoch 85, Loss: 2.0435743741691113 = 1.424943171441555 (normal) + 0.6186312027275562 (augmented)  
[FFN] Epoch 86, Loss: 2.044092033058405 = 1.4226721823215485 (normal) + 0.6214198656380177 (augmented)  
[FFN] Epoch 87, Loss: 2.041325643658638 = 1.4183963313698769 (normal) + 0.6229293160140514 (augmented)  
[FFN] Epoch 88, Loss: 2.0381760001182556 = 1.4176862835884094 (normal) + 0.6204897202551365 (augmented)  
[FFN] Epoch 89, Loss: 2.035871844738722 = 1.4149626679718494 (normal) + 0.6209091525524855 (augmented)  
[FFN] Epoch 90, Loss: 2.031938821077347 = 1.4128418639302254 (normal) + 0.6190969496965408 (augmented)  
[FFN] Epoch 91, Loss: 2.0311407670378685 = 1.4101490005850792 (normal) + 0.6209917645901442 (augmented)  
[FFN] Epoch 92, Loss: 2.031826715916395 = 1.4111139066517353 (normal) + 0.6207128241658211 (augmented)  
[FFN] Epoch 93, Loss: 2.0279458425939083 = 1.4079755805432796 (normal) + 0.6199702713638544 (augmented)  
[FFN] Epoch 94, Loss: 2.0251846723258495 = 1.4052020385861397 (normal) + 0.6199826579540968 (augmented)  
[FFN] Epoch 95, Loss: 2.0243223272264004 = 1.4021678790450096 (normal) + 0.6221544370055199 (augmented)  
[FFN] Epoch 96, Loss: 2.019547175616026 = 1.3988366350531578 (normal) + 0.6207105405628681 (augmented)  
[FFN] Epoch 97, Loss: 2.015972789376974 = 1.39651308208704 (normal) + 0.6194597166031599 (augmented)  
[FFN] Epoch 98, Loss: 2.01692645996809 = 1.3965320251882076 (normal) + 0.6203944440931082 (augmented)  
[FFN] Epoch 99, Loss: 2.01806016638875 = 1.397883977741003 (normal) + 0.6201761960983276 (augmented)  
[FFN] Epoch 100, Loss: 2.0145978815853596 = 1.3962203934788704 (normal) + 0.6183774918317795 (augmented)  
Training complete.
Modular training for SA module, encoder 3
LOAD FROM SAVE
Encoder idx = 3
Batch count :  32
[MHA]Epoch 1, Loss: 9.376011908054352 = 4.327182546257973 (normal) + 5.0488293543457985 (augmented)  
[MHA]Epoch 2, Loss: 7.022477343678474 = 3.788960002362728 (normal) + 3.233517326414585 (augmented)  
[MHA]Epoch 3, Loss: 6.483994334936142 = 3.754508227109909 (normal) + 2.7294861003756523 (augmented)  
[MHA]Epoch 4, Loss: 6.169406160712242 = 3.6682786643505096 (normal) + 2.501127503812313 (augmented)  
[MHA]Epoch 5, Loss: 5.838867589831352 = 3.5320364609360695 (normal) + 2.3068311102688313 (augmented)  
[MHA]Epoch 6, Loss: 5.567400306463242 = 3.405623234808445 (normal) + 2.1617770940065384 (augmented)  
[MHA]Epoch 7, Loss: 5.334005996584892 = 3.304644174873829 (normal) + 2.0293618366122246 (augmented)  
[MHA]Epoch 8, Loss: 5.165789812803268 = 3.2254497036337852 (normal) + 1.9403400607407093 (augmented)  
[MHA]Epoch 9, Loss: 5.014330729842186 = 3.158030353486538 (normal) + 1.8563003428280354 (augmented)  
[MHA]Epoch 10, Loss: 4.890335574746132 = 3.0985944271087646 (normal) + 1.7917411923408508 (augmented)  
[MHA]Epoch 11, Loss: 4.770482152700424 = 3.0405033081769943 (normal) + 1.7299788296222687 (augmented)  
[MHA]Epoch 12, Loss: 4.67176990956068 = 2.990614838898182 (normal) + 1.681155014783144 (augmented)  
[MHA]Epoch 13, Loss: 4.578181803226471 = 2.9454822465777397 (normal) + 1.6326995491981506 (augmented)  
[MHA]Epoch 14, Loss: 4.4854665994644165 = 2.9023251831531525 (normal) + 1.5831414051353931 (augmented)  
[MHA]Epoch 15, Loss: 4.410450764000416 = 2.866826593875885 (normal) + 1.5436241775751114 (augmented)  
[MHA]Epoch 16, Loss: 4.339060947299004 = 2.8320329934358597 (normal) + 1.5070279352366924 (augmented)  
[MHA]Epoch 17, Loss: 4.262530662119389 = 2.7980906814336777 (normal) + 1.4644399918615818 (augmented)  
[MHA]Epoch 18, Loss: 4.220532692968845 = 2.773133009672165 (normal) + 1.4473996832966805 (augmented)  
[MHA]Epoch 19, Loss: 4.152654469013214 = 2.742330215871334 (normal) + 1.4103242084383965 (augmented)  
[MHA]Epoch 20, Loss: 4.116831175982952 = 2.7198977768421173 (normal) + 1.396933402866125 (augmented)  
[MHA]Epoch 21, Loss: 4.055138468742371 = 2.691322721540928 (normal) + 1.3638157844543457 (augmented)  
[MHA]Epoch 22, Loss: 3.9986662939190865 = 2.669485129415989 (normal) + 1.3291811738163233 (augmented)  
[MHA]Epoch 23, Loss: 3.9481868892908096 = 2.6451989710330963 (normal) + 1.3029878959059715 (augmented)  
[MHA]Epoch 24, Loss: 3.914011351764202 = 2.6252877339720726 (normal) + 1.2887235712260008 (augmented)  
[MHA]Epoch 25, Loss: 3.8678759410977364 = 2.5985693261027336 (normal) + 1.269306592643261 (augmented)  
[MHA]Epoch 26, Loss: 3.8097469583153725 = 2.579194091260433 (normal) + 1.2305528596043587 (augmented)  
[MHA]Epoch 27, Loss: 3.7829600498080254 = 2.560286231338978 (normal) + 1.22267378680408 (augmented)  
[MHA]Epoch 28, Loss: 3.7503478974103928 = 2.539603568613529 (normal) + 1.2107443138957024 (augmented)  
[MHA]Epoch 29, Loss: 3.6966720521450043 = 2.518604449927807 (normal) + 1.1780676022171974 (augmented)  
[MHA]Epoch 30, Loss: 3.650908909738064 = 2.496814288198948 (normal) + 1.1540946029126644 (augmented)  
[MHA]Epoch 31, Loss: 3.631603516638279 = 2.4803751334547997 (normal) + 1.1512283720076084 (augmented)  
[MHA]Epoch 32, Loss: 3.6088386550545692 = 2.465168982744217 (normal) + 1.143669644370675 (augmented)  
[MHA]Epoch 33, Loss: 3.5725360587239265 = 2.445631444454193 (normal) + 1.1269045677036047 (augmented)  
[MHA]Epoch 34, Loss: 3.522757910192013 = 2.4262071773409843 (normal) + 1.0965507291257381 (augmented)  
[MHA]Epoch 35, Loss: 3.49691279232502 = 2.4102890342473984 (normal) + 1.0866237469017506 (augmented)  
[MHA]Epoch 36, Loss: 3.477174997329712 = 2.3966460302472115 (normal) + 1.0805289465934038 (augmented)  
[MHA]Epoch 37, Loss: 3.442676417529583 = 2.376977302134037 (normal) + 1.065699115395546 (augmented)  
[MHA]Epoch 38, Loss: 3.3944149166345596 = 2.3653362914919853 (normal) + 1.0290785934776068 (augmented)  
[MHA]Epoch 39, Loss: 3.391510896384716 = 2.351765066385269 (normal) + 1.0397458132356405 (augmented)  
[MHA]Epoch 40, Loss: 3.3606042936444283 = 2.338822465389967 (normal) + 1.0217818040400743 (augmented)  
[MHA]Epoch 41, Loss: 3.30677929520607 = 2.31093505397439 (normal) + 0.9958442412316799 (augmented)  
[MHA]Epoch 42, Loss: 3.2819262593984604 = 2.3012857511639595 (normal) + 0.980640472844243 (augmented)  
[MHA]Epoch 43, Loss: 3.244184210896492 = 2.280556920915842 (normal) + 0.9636272583156824 (augmented)  
[MHA]Epoch 44, Loss: 3.243826910853386 = 2.273620206862688 (normal) + 0.9702066797763109 (augmented)  
[MHA]Epoch 45, Loss: 3.2250068560242653 = 2.262528907507658 (normal) + 0.9624779392033815 (augmented)  
[MHA]Epoch 46, Loss: 3.1819689497351646 = 2.2434330843389034 (normal) + 0.9385358486324549 (augmented)  
[MHA]Epoch 47, Loss: 3.166735425591469 = 2.2308090180158615 (normal) + 0.9359264075756073 (augmented)  
[MHA]Epoch 48, Loss: 3.1320964992046356 = 2.2143957428634167 (normal) + 0.917700719088316 (augmented)  
[MHA]Epoch 49, Loss: 3.0966755598783493 = 2.201286554336548 (normal) + 0.8953889850527048 (augmented)  
[MHA]Epoch 50, Loss: 3.070880874991417 = 2.18267160654068 (normal) + 0.8882092460989952 (augmented)  
[MHA]Epoch 51, Loss: 3.030060365796089 = 2.166891887784004 (normal) + 0.8631684482097626 (augmented)  
[MHA]Epoch 52, Loss: 3.0308628380298615 = 2.161481808871031 (normal) + 0.869381008669734 (augmented)  
[MHA]Epoch 53, Loss: 2.9998549818992615 = 2.143045410513878 (normal) + 0.8568095676600933 (augmented)  
[MHA]Epoch 54, Loss: 2.9536320343613625 = 2.1273824721574783 (normal) + 0.826249536126852 (augmented)  
[MHA]Epoch 55, Loss: 2.9578245654702187 = 2.1216500140726566 (normal) + 0.8361745178699493 (augmented)  
[MHA]Epoch 56, Loss: 2.933907985687256 = 2.1092060692608356 (normal) + 0.8247019145637751 (augmented)  
[MHA]Epoch 57, Loss: 2.9038849025964737 = 2.0997816249728203 (normal) + 0.804103247821331 (augmented)  
[MHA]Epoch 58, Loss: 2.887686111032963 = 2.088688213378191 (normal) + 0.7989978715777397 (augmented)  
[MHA]Epoch 59, Loss: 2.8911260962486267 = 2.0799208730459213 (normal) + 0.8112051840871572 (augmented)  
[MHA]Epoch 60, Loss: 2.8535123988986015 = 2.0648248493671417 (normal) + 0.7886875309050083 (augmented)  
[MHA]Epoch 61, Loss: 2.8346771746873856 = 2.0547641217708588 (normal) + 0.7799130547791719 (augmented)  
[MHA]Epoch 62, Loss: 2.809276632964611 = 2.0451297909021378 (normal) + 0.7641468457877636 (augmented)  
[MHA]Epoch 63, Loss: 2.783763103187084 = 2.029120024293661 (normal) + 0.7546430882066488 (augmented)  
[MHA]Epoch 64, Loss: 2.7695785090327263 = 2.0206445790827274 (normal) + 0.7489339057356119 (augmented)  
[MHA]Epoch 65, Loss: 2.7636217325925827 = 2.012228552252054 (normal) + 0.7513931840658188 (augmented)  
[MHA]Epoch 66, Loss: 2.7178129702806473 = 1.999672096222639 (normal) + 0.7181408777832985 (augmented)  
[MHA]Epoch 67, Loss: 2.70776829123497 = 1.991157978773117 (normal) + 0.7166103012859821 (augmented)  
[MHA]Epoch 68, Loss: 2.680725648999214 = 1.9799880161881447 (normal) + 0.7007376365363598 (augmented)  
[MHA]Epoch 69, Loss: 2.6883816719055176 = 1.9740002788603306 (normal) + 0.7143813651055098 (augmented)  
[MHA]Epoch 70, Loss: 2.6578593775629997 = 1.9618316553533077 (normal) + 0.6960276979953051 (augmented)  
[MHA]Epoch 71, Loss: 2.6305579096078873 = 1.9514929316937923 (normal) + 0.6790649443864822 (augmented)  
[MHA]Epoch 72, Loss: 2.615729920566082 = 1.945007335394621 (normal) + 0.6707225572317839 (augmented)  
[MHA]Epoch 73, Loss: 2.607081763446331 = 1.9364096336066723 (normal) + 0.670672107487917 (augmented)  
[MHA]Epoch 74, Loss: 2.6080094799399376 = 1.930527575314045 (normal) + 0.6774819009006023 (augmented)  
[MHA]Epoch 75, Loss: 2.5837500914931297 = 1.9214564822614193 (normal) + 0.6622935840860009 (augmented)  
[MHA]Epoch 76, Loss: 2.5691961273550987 = 1.9161699041724205 (normal) + 0.6530262231826782 (augmented)  
[MHA]Epoch 77, Loss: 2.5665594190359116 = 1.9068082943558693 (normal) + 0.6597511023283005 (augmented)  
[MHA]Epoch 78, Loss: 2.5281045213341713 = 1.893068939447403 (normal) + 0.6350355744361877 (augmented)  
[MHA]Epoch 79, Loss: 2.5390549078583717 = 1.894596192985773 (normal) + 0.6444587102159858 (augmented)  
[MHA]Epoch 80, Loss: 2.505764588713646 = 1.8822302408516407 (normal) + 0.6235343394801021 (augmented)  
[MHA]Epoch 81, Loss: 2.502022959291935 = 1.877969078719616 (normal) + 0.624053880572319 (augmented)  
[MHA]Epoch 82, Loss: 2.50063768774271 = 1.869998935610056 (normal) + 0.6306387493386865 (augmented)  
[MHA]Epoch 83, Loss: 2.477950006723404 = 1.8627128303050995 (normal) + 0.615237170830369 (augmented)  
[MHA]Epoch 84, Loss: 2.4757648184895515 = 1.8592401929199696 (normal) + 0.6165246227756143 (augmented)  
[MHA]Epoch 85, Loss: 2.482774242758751 = 1.8630033396184444 (normal) + 0.6197709096595645 (augmented)  
[MHA]Epoch 86, Loss: 2.4822216629981995 = 1.8596754185855389 (normal) + 0.6225462313741446 (augmented)  
[MHA]Epoch 87, Loss: 2.4707398787140846 = 1.8566246591508389 (normal) + 0.6141152186319232 (augmented)  
[MHA]Epoch 88, Loss: 2.47367525100708 = 1.8611981235444546 (normal) + 0.6124771144241095 (augmented)  
[MHA]Epoch 89, Loss: 2.45509222894907 = 1.8482925333082676 (normal) + 0.6067996909841895 (augmented)  
[MHA]Epoch 90, Loss: 2.4374323934316635 = 1.8377965427935123 (normal) + 0.599635829217732 (augmented)  
[MHA]Epoch 91, Loss: 2.4344911724328995 = 1.835759524255991 (normal) + 0.5987316258251667 (augmented)  
[MHA]Epoch 92, Loss: 2.422107048332691 = 1.8286991640925407 (normal) + 0.5934078795835376 (augmented)  
[MHA]Epoch 93, Loss: 2.394032798707485 = 1.815809242427349 (normal) + 0.5782235553488135 (augmented)  
[MHA]Epoch 94, Loss: 2.3738189563155174 = 1.8043130151927471 (normal) + 0.5695059467107058 (augmented)  
[MHA]Epoch 95, Loss: 2.3508234322071075 = 1.7943773008883 (normal) + 0.556446130387485 (augmented)  
[MHA]Epoch 96, Loss: 2.34781950712204 = 1.7903305739164352 (normal) + 0.5574889564886689 (augmented)  
[MHA]Epoch 97, Loss: 2.3249560929834843 = 1.7811709344387054 (normal) + 0.5437851455062628 (augmented)  
[MHA]Epoch 98, Loss: 2.32206716760993 = 1.781675323843956 (normal) + 0.5403918465599418 (augmented)  
[MHA]Epoch 99, Loss: 2.299906887114048 = 1.7673890329897404 (normal) + 0.5325178392231464 (augmented)  
[MHA]Epoch 100, Loss: 2.3002750612795353 = 1.7662773355841637 (normal) + 0.5339977238327265 (augmented)  
EC  0
Training complete.
Generating data for encoder 4
[nltk_data] Downloading package stopwords to /home/a40-ko-
[nltk_data]     lab/nltk_data...
[nltk_data]   Package stopwords is already up-to-date!
BertModel loaded from: transformers.models.bert.modeling_bert
Transformers module loaded from: /home/a40-ko-lab/miniconda3/envs/jiantenv/lib/python3.9/site-packages/transformers/__init__.py
06/12/2024 13:41:17 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False
06/12/2024 13:41:17 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
batch_eval_metrics=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_steps=None,
eval_strategy=no,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/tmp/cb/runs/Jun12_13-41-16_a40kolab,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=100.0,
optim=adamw_torch,
optim_args=None,
optim_target_modules=None,
output_dir=/tmp/cb/,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=/tmp/cb/,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
[INFO|configuration_utils.py:731] 2024-06-12 13:41:17,061 >> loading configuration file ./downloads/cb_config/config.json
[INFO|configuration_utils.py:800] 2024-06-12 13:41:17,062 >> Model config DistilBertConfig {
  "_name_or_path": "./downloads/cb_config",
  "activation": "gelu",
  "architectures": [
    "DistilBertForMaskedLM"
  ],
  "attention_dropout": 0.1,
  "dim": 768,
  "dropout": 0.1,
  "finetuning_task": "cb",
  "hidden_dim": 3072,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 512,
  "model_type": "distilbert",
  "n_heads": 12,
  "n_layers": 6,
  "pad_token_id": 0,
  "qa_dropout": 0.1,
  "seq_classif_dropout": 0.2,
  "sinusoidal_pos_embds": false,
  "tie_weights_": true,
  "transformers_version": "4.42.0.dev0",
  "vocab_size": 30522
}

[INFO|tokenization_utils_base.py:2107] 2024-06-12 13:41:17,064 >> loading file vocab.txt
[INFO|tokenization_utils_base.py:2107] 2024-06-12 13:41:17,064 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2107] 2024-06-12 13:41:17,064 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2107] 2024-06-12 13:41:17,064 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2107] 2024-06-12 13:41:17,064 >> loading file tokenizer_config.json
[INFO|configuration_utils.py:731] 2024-06-12 13:41:17,081 >> loading configuration file ./downloads/cb_model/config.json
[INFO|configuration_utils.py:800] 2024-06-12 13:41:17,082 >> Model config DistilBertConfig {
  "_name_or_path": "./downloads/cb_model",
  "activation": "gelu",
  "architectures": [
    "DistilBertForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "dim": 768,
  "dropout": 0.1,
  "finetuning_task": "cb",
  "hidden_dim": 3072,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 512,
  "model_type": "distilbert",
  "n_heads": 12,
  "n_layers": 6,
  "pad_token_id": 0,
  "qa_dropout": 0.1,
  "seq_classif_dropout": 0.2,
  "sinusoidal_pos_embds": false,
  "tie_weights_": true,
  "torch_dtype": "float32",
  "transformers_version": "4.42.0.dev0",
  "vocab_size": 30522
}

[INFO|modeling_utils.py:3474] 2024-06-12 13:41:17,108 >> loading weights file ./downloads/cb_model/model.safetensors
[INFO|modeling_utils.py:4283] 2024-06-12 13:41:17,145 >> All model checkpoint weights were used when initializing DistilBertForSequenceClassification.

[INFO|modeling_utils.py:4291] 2024-06-12 13:41:17,145 >> All the weights of DistilBertForSequenceClassification were initialized from the model checkpoint at ./downloads/cb_model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForSequenceClassification for predictions without further training.
Loading cached processed dataset at /media/a40-ko-lab/c61d4ea0-b8fe-4205-9815-f7d3d054043c/sayed/Sayed/transformers/weight_sharing/modular_superglue/downloads/cb/train/cache-ec9a28bf6cd38919.arrow
Loaded dataset from disk
06/12/2024 13:41:17 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /media/a40-ko-lab/c61d4ea0-b8fe-4205-9815-f7d3d054043c/sayed/Sayed/transformers/weight_sharing/modular_superglue/downloads/cb/train/cache-ec9a28bf6cd38919.arrow
Loading cached processed dataset at /media/a40-ko-lab/c61d4ea0-b8fe-4205-9815-f7d3d054043c/sayed/Sayed/transformers/weight_sharing/modular_superglue/downloads/cb/validation/cache-462a1cd6bd454452.arrow
06/12/2024 13:41:17 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /media/a40-ko-lab/c61d4ea0-b8fe-4205-9815-f7d3d054043c/sayed/Sayed/transformers/weight_sharing/modular_superglue/downloads/cb/validation/cache-462a1cd6bd454452.arrow
Loading cached processed dataset at /media/a40-ko-lab/c61d4ea0-b8fe-4205-9815-f7d3d054043c/sayed/Sayed/transformers/weight_sharing/modular_superglue/downloads/cb/test/cache-f44364b0f28c5bd9.arrow
06/12/2024 13:41:17 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /media/a40-ko-lab/c61d4ea0-b8fe-4205-9815-f7d3d054043c/sayed/Sayed/transformers/weight_sharing/modular_superglue/downloads/cb/test/cache-f44364b0f28c5bd9.arrow
06/12/2024 13:41:17 - INFO - __main__ - Sample 163 of the training set: {'input_ids': [101, 1037, 1024, 2031, 2017, 2628, 2008, 2200, 2172, 2030, 1010, 1038, 1024, 7910, 1010, 2025, 2428, 1012, 1045, 2123, 1005, 1056, 2228, 2505, 2097, 2412, 2202, 2058, 1996, 5088, 1012, 102, 2242, 2097, 2202, 2058, 1996, 5088, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
06/12/2024 13:41:17 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:3753] 2024-06-12 13:41:17,376 >> ***** Running Evaluation *****
[INFO|trainer.py:3755] 2024-06-12 13:41:17,376 >>   Num examples = 250
[INFO|trainer.py:3758] 2024-06-12 13:41:17,376 >>   Batch size = 8
Directory './saves/distilbert/distilbert-base-uncased/cb/mha/outputs/encoder_4' created successfully.
Directory './saves/distilbert/distilbert-base-uncased/cb/ffn/inputs/encoder_4' created successfully.
Directory './saves/distilbert/distilbert-base-uncased/cb/ffn/outputs/encoder_4' created successfully.
Directory './saves/distilbert/distilbert-base-uncased/cb/mha/inputs/encoder_4' created successfully.
Batch Idx  1
  0%|          | 0/32 [00:00<?, ?it/s] 16%|█▌        | 5/32 [00:00<00:00, 46.97it/s] 31%|███▏      | 10/32 [00:00<00:00, 43.85it/s] 47%|████▋     | 15/32 [00:00<00:00, 43.25it/s] 62%|██████▎   | 20/32 [00:00<00:00, 42.13it/s] 78%|███████▊  | 25/32 [00:00<00:00, 42.36it/s] 94%|█████████▍| 30/32 [00:00<00:00, 42.55it/s]100%|██████████| 32/32 [00:00<00:00, 43.61it/s]
Batch Idx  2
Batch Idx  3
Batch Idx  4
Batch Idx  5
Batch Idx  6
Batch Idx  7
Batch Idx  8
Batch Idx  9
Batch Idx  10
Batch Idx  11
Batch Idx  12
Batch Idx  13
Batch Idx  14
Batch Idx  15
Batch Idx  16
Batch Idx  17
Batch Idx  18
Batch Idx  19
Batch Idx  20
Batch Idx  21
Batch Idx  22
Batch Idx  23
Batch Idx  24
Batch Idx  25
Batch Idx  26
Batch Idx  27
Batch Idx  28
Batch Idx  29
Batch Idx  30
Batch Idx  31
Batch Idx  32
***** eval metrics *****
  eval_runtime            = 0:00:00.84
  eval_samples            =        250
  eval_samples_per_second =    294.599
  eval_steps_per_second   =     37.709
Data generated succesfully

 
 
 Beginning modular training of SA and BL modules
Encoder compression: 2
Modular training for BL module, encoder 4
LOAD FROM SAVE
Encoder idx = 4
Compression = 2
[FFN] Epoch 1, Loss: 641.7617225646973 = 523.184211730957 (normal) + 118.57751083374023 (augmented)  
[FFN] Epoch 2, Loss: 605.5349597930908 = 495.69612884521484 (normal) + 109.8388319015503 (augmented)  
[FFN] Epoch 3, Loss: 544.8666763305664 = 449.4168872833252 (normal) + 95.4497857093811 (augmented)  
[FFN] Epoch 4, Loss: 471.0031728744507 = 392.12163639068604 (normal) + 78.88153946399689 (augmented)  
[FFN] Epoch 5, Loss: 395.18054580688477 = 330.01398754119873 (normal) + 65.16656017303467 (augmented)  
[FFN] Epoch 6, Loss: 330.00173473358154 = 276.7776827812195 (normal) + 53.22405195236206 (augmented)  
[FFN] Epoch 7, Loss: 275.1105890274048 = 227.391854763031 (normal) + 47.71873486042023 (augmented)  
[FFN] Epoch 8, Loss: 237.9269723892212 = 192.83497095108032 (normal) + 45.09200209379196 (augmented)  
[FFN] Epoch 9, Loss: 211.21211004257202 = 169.9895634651184 (normal) + 41.22254717350006 (augmented)  
[FFN] Epoch 10, Loss: 188.55003929138184 = 146.97427105903625 (normal) + 41.575767159461975 (augmented)  
[FFN] Epoch 11, Loss: 173.83623456954956 = 133.8997962474823 (normal) + 39.936438381671906 (augmented)  
[FFN] Epoch 12, Loss: 171.38969707489014 = 131.5085985660553 (normal) + 39.88109880685806 (augmented)  
[FFN] Epoch 13, Loss: 163.16486477851868 = 120.95374894142151 (normal) + 42.211115300655365 (augmented)  
[FFN] Epoch 14, Loss: 153.86966156959534 = 113.89679861068726 (normal) + 39.972862899303436 (augmented)  
[FFN] Epoch 15, Loss: 146.54051613807678 = 107.25927662849426 (normal) + 39.28123962879181 (augmented)  
[FFN] Epoch 16, Loss: 150.64950299263 = 110.30986905097961 (normal) + 40.33963507413864 (augmented)  
[FFN] Epoch 17, Loss: 147.58001828193665 = 106.4941211938858 (normal) + 41.08589768409729 (augmented)  
[FFN] Epoch 18, Loss: 136.05936431884766 = 98.67571425437927 (normal) + 37.383651196956635 (augmented)  
[FFN] Epoch 19, Loss: 133.04408979415894 = 93.70999336242676 (normal) + 39.33409607410431 (augmented)  
[FFN] Epoch 20, Loss: 136.68992114067078 = 99.58859932422638 (normal) + 37.10132086277008 (augmented)  
[FFN] Epoch 21, Loss: 130.13597464561462 = 92.25244176387787 (normal) + 37.88353258371353 (augmented)  
[FFN] Epoch 22, Loss: 126.49841547012329 = 89.41199386119843 (normal) + 37.08642113208771 (augmented)  
[FFN] Epoch 23, Loss: 128.61994981765747 = 93.20378804206848 (normal) + 35.416161715984344 (augmented)  
[FFN] Epoch 24, Loss: 135.29710388183594 = 100.13237655162811 (normal) + 35.164726972579956 (augmented)  
[FFN] Epoch 25, Loss: 118.8102376461029 = 83.29493117332458 (normal) + 35.51530569791794 (augmented)  
[FFN] Epoch 26, Loss: 126.8474702835083 = 92.0541124343872 (normal) + 34.793358862400055 (augmented)  
[FFN] Epoch 27, Loss: 126.1958532333374 = 93.59553110599518 (normal) + 32.6003223657608 (augmented)  
[FFN] Epoch 28, Loss: 122.09025263786316 = 88.73759263753891 (normal) + 33.35266077518463 (augmented)  
[FFN] Epoch 29, Loss: 119.22525262832642 = 87.68007397651672 (normal) + 31.545179665088654 (augmented)  
[FFN] Epoch 30, Loss: 117.12182283401489 = 86.07729232311249 (normal) + 31.04453033208847 (augmented)  
[FFN] Epoch 31, Loss: 121.5236177444458 = 88.49983578920364 (normal) + 33.023780822753906 (augmented)  
[FFN] Epoch 32, Loss: 117.08842444419861 = 83.8304682970047 (normal) + 33.25795495510101 (augmented)  
[FFN] Epoch 33, Loss: 112.85611367225647 = 81.53828370571136 (normal) + 31.31782990694046 (augmented)  
[FFN] Epoch 34, Loss: 115.4860782623291 = 84.52661216259003 (normal) + 30.95946580171585 (augmented)  
[FFN] Epoch 35, Loss: 114.56686472892761 = 84.56224465370178 (normal) + 30.004621267318726 (augmented)  
[FFN] Epoch 36, Loss: 110.14011645317078 = 80.76784825325012 (normal) + 29.3722682595253 (augmented)  
[FFN] Epoch 37, Loss: 115.16347336769104 = 84.02974343299866 (normal) + 31.133729457855225 (augmented)  
[FFN] Epoch 38, Loss: 116.65730428695679 = 83.8721067905426 (normal) + 32.785197257995605 (augmented)  
[FFN] Epoch 39, Loss: 105.303307056427 = 75.46366047859192 (normal) + 29.839645981788635 (augmented)  
[FFN] Epoch 40, Loss: 106.14191460609436 = 74.98136556148529 (normal) + 31.160548746585846 (augmented)  
[FFN] Epoch 41, Loss: 109.70196056365967 = 80.42023003101349 (normal) + 29.28173005580902 (augmented)  
[FFN] Epoch 42, Loss: 112.93746662139893 = 83.33739340305328 (normal) + 29.600072383880615 (augmented)  
[FFN] Epoch 43, Loss: 101.24448442459106 = 73.33948087692261 (normal) + 27.905003130435944 (augmented)  
[FFN] Epoch 44, Loss: 102.42108178138733 = 75.77229619026184 (normal) + 26.64878559112549 (augmented)  
[FFN] Epoch 45, Loss: 99.32881665229797 = 72.69939577579498 (normal) + 26.629420191049576 (augmented)  
[FFN] Epoch 46, Loss: 101.58357799053192 = 74.5632649064064 (normal) + 27.020313143730164 (augmented)  
[FFN] Epoch 47, Loss: 88.2566157579422 = 64.19926345348358 (normal) + 24.057352006435394 (augmented)  
[FFN] Epoch 48, Loss: 97.50697982311249 = 73.60239624977112 (normal) + 23.904583632946014 (augmented)  
[FFN] Epoch 49, Loss: 88.90021514892578 = 65.32395958900452 (normal) + 23.576254844665527 (augmented)  
[FFN] Epoch 50, Loss: 99.83294308185577 = 75.55254411697388 (normal) + 24.28039926290512 (augmented)  
[FFN] Epoch 51, Loss: 95.05909609794617 = 72.89310389757156 (normal) + 22.165991872549057 (augmented)  
[FFN] Epoch 52, Loss: 98.41854190826416 = 76.2453578710556 (normal) + 22.173183292150497 (augmented)  
[FFN] Epoch 53, Loss: 98.73291385173798 = 77.45925784111023 (normal) + 21.273656457662582 (augmented)  
[FFN] Epoch 54, Loss: 94.28321588039398 = 74.41492247581482 (normal) + 19.868293821811676 (augmented)  
[FFN] Epoch 55, Loss: 91.32757079601288 = 70.11001026630402 (normal) + 21.217560052871704 (augmented)  
[FFN] Epoch 56, Loss: 87.89201653003693 = 65.91494047641754 (normal) + 21.97707685828209 (augmented)  
[FFN] Epoch 57, Loss: 91.94664597511292 = 72.26534593105316 (normal) + 19.681299954652786 (augmented)  
[FFN] Epoch 58, Loss: 88.46593976020813 = 67.86400067806244 (normal) + 20.601938277482986 (augmented)  
[FFN] Epoch 59, Loss: 92.36887860298157 = 73.73844665288925 (normal) + 18.630432426929474 (augmented)  
[FFN] Epoch 60, Loss: 85.95517861843109 = 66.56607210636139 (normal) + 19.389106273651123 (augmented)  
[FFN] Epoch 61, Loss: 77.74420034885406 = 59.24771595001221 (normal) + 18.496484220027924 (augmented)  
[FFN] Epoch 62, Loss: 88.24706447124481 = 69.51001799106598 (normal) + 18.737046152353287 (augmented)  
[FFN] Epoch 63, Loss: 86.43018245697021 = 67.73112499713898 (normal) + 18.69905772805214 (augmented)  
[FFN] Epoch 64, Loss: 82.68216967582703 = 64.74043136835098 (normal) + 17.941737830638885 (augmented)  
[FFN] Epoch 65, Loss: 86.8517529964447 = 69.65501892566681 (normal) + 17.19673451781273 (augmented)  
[FFN] Epoch 66, Loss: 83.97399055957794 = 67.24255383014679 (normal) + 16.731437176465988 (augmented)  
[FFN] Epoch 67, Loss: 83.33777058124542 = 65.81102365255356 (normal) + 17.526747077703476 (augmented)  
[FFN] Epoch 68, Loss: 87.10718750953674 = 69.88481509685516 (normal) + 17.222372472286224 (augmented)  
[FFN] Epoch 69, Loss: 75.8284740447998 = 58.92427182197571 (normal) + 16.904202044010162 (augmented)  
[FFN] Epoch 70, Loss: 82.75308358669281 = 65.7223487496376 (normal) + 17.030735284090042 (augmented)  
[FFN] Epoch 71, Loss: 75.94552564620972 = 60.7141175866127 (normal) + 15.231408014893532 (augmented)  
[FFN] Epoch 72, Loss: 80.15240120887756 = 63.97123348712921 (normal) + 16.181168287992477 (augmented)  
[FFN] Epoch 73, Loss: 81.69472455978394 = 67.19194161891937 (normal) + 14.502783685922623 (augmented)  
[FFN] Epoch 74, Loss: 85.66911959648132 = 68.83623957633972 (normal) + 16.832879066467285 (augmented)  
[FFN] Epoch 75, Loss: 81.29630690813065 = 64.90482276678085 (normal) + 16.391484200954437 (augmented)  
[FFN] Epoch 76, Loss: 73.76607060432434 = 58.47928786277771 (normal) + 15.286782652139664 (augmented)  
[FFN] Epoch 77, Loss: 74.05213928222656 = 58.86912578344345 (normal) + 15.183013260364532 (augmented)  
[FFN] Epoch 78, Loss: 78.84415471553802 = 63.738472163677216 (normal) + 15.105682462453842 (augmented)  
[FFN] Epoch 79, Loss: 74.99038934707642 = 59.236069321632385 (normal) + 15.754319846630096 (augmented)  
[FFN] Epoch 80, Loss: 78.69564878940582 = 62.98934531211853 (normal) + 15.706303477287292 (augmented)  
[FFN] Epoch 81, Loss: 77.29557490348816 = 60.8999724984169 (normal) + 16.39560240507126 (augmented)  
[FFN] Epoch 82, Loss: 71.61183965206146 = 56.729327499866486 (normal) + 14.8825121819973 (augmented)  
[FFN] Epoch 83, Loss: 75.55199259519577 = 62.373185992240906 (normal) + 13.178806558251381 (augmented)  
[FFN] Epoch 84, Loss: 78.67168962955475 = 63.096018850803375 (normal) + 15.575670078396797 (augmented)  
[FFN] Epoch 85, Loss: 79.40909600257874 = 66.31917482614517 (normal) + 13.089920699596405 (augmented)  
[FFN] Epoch 86, Loss: 70.58293908834457 = 55.50503033399582 (normal) + 15.077908337116241 (augmented)  
[FFN] Epoch 87, Loss: 73.07308685779572 = 58.86052221059799 (normal) + 14.212564617395401 (augmented)  
[FFN] Epoch 88, Loss: 83.11192870140076 = 70.51199567317963 (normal) + 12.599932700395584 (augmented)  
[FFN] Epoch 89, Loss: 83.98627525568008 = 71.3764778971672 (normal) + 12.609797820448875 (augmented)  
[FFN] Epoch 90, Loss: 73.13579136133194 = 60.53074663877487 (normal) + 12.605044826865196 (augmented)  
[FFN] Epoch 91, Loss: 77.62271785736084 = 63.70725256204605 (normal) + 13.915465205907822 (augmented)  
[FFN] Epoch 92, Loss: 80.62316942214966 = 65.13571572303772 (normal) + 15.48745334148407 (augmented)  
[FFN] Epoch 93, Loss: 71.56494611501694 = 58.7742745578289 (normal) + 12.790671348571777 (augmented)  
[FFN] Epoch 94, Loss: 72.95178401470184 = 58.72052317857742 (normal) + 14.231260374188423 (augmented)  
[FFN] Epoch 95, Loss: 71.46805429458618 = 58.60781380534172 (normal) + 12.86024035513401 (augmented)  
[FFN] Epoch 96, Loss: 69.90100079774857 = 57.00182229280472 (normal) + 12.899178847670555 (augmented)  
[FFN] Epoch 97, Loss: 66.89284580945969 = 54.307804346084595 (normal) + 12.585041120648384 (augmented)  
[FFN] Epoch 98, Loss: 76.26271855831146 = 64.38180810213089 (normal) + 11.880910485982895 (augmented)  
[FFN] Epoch 99, Loss: 76.5922120809555 = 63.570956349372864 (normal) + 13.021255612373352 (augmented)  
[FFN] Epoch 100, Loss: 72.96002960205078 = 61.12627360224724 (normal) + 11.833755820989609 (augmented)  
Training complete.
Modular training for SA module, encoder 4
LOAD FROM SAVE
Encoder idx = 4
Batch count :  32
[MHA]Epoch 1, Loss: 7.040161192417145 = 3.4995909929275513 (normal) + 3.5405702367424965 (augmented)  
[MHA]Epoch 2, Loss: 5.065101251006126 = 2.9690697640180588 (normal) + 2.0960315130650997 (augmented)  
[MHA]Epoch 3, Loss: 4.6021406054496765 = 2.8758464753627777 (normal) + 1.726294118911028 (augmented)  
[MHA]Epoch 4, Loss: 4.330759301781654 = 2.786623813211918 (normal) + 1.5441354513168335 (augmented)  
[MHA]Epoch 5, Loss: 4.0608891248703 = 2.665679231286049 (normal) + 1.3952098973095417 (augmented)  
[MHA]Epoch 6, Loss: 3.8421187698841095 = 2.55179014056921 (normal) + 1.290328634902835 (augmented)  
[MHA]Epoch 7, Loss: 3.6560180112719536 = 2.458531081676483 (normal) + 1.1974869314581156 (augmented)  
[MHA]Epoch 8, Loss: 3.5227170884609222 = 2.3846213929355145 (normal) + 1.1380956675857306 (augmented)  
[MHA]Epoch 9, Loss: 3.4073475301265717 = 2.3201407939195633 (normal) + 1.0872067362070084 (augmented)  
[MHA]Epoch 10, Loss: 3.3144580498337746 = 2.2660254277288914 (normal) + 1.048432607203722 (augmented)  
[MHA]Epoch 11, Loss: 3.2276390567421913 = 2.217285569757223 (normal) + 1.0103534758090973 (augmented)  
[MHA]Epoch 12, Loss: 3.1583261862397194 = 2.1744588799774647 (normal) + 0.9838673043996096 (augmented)  
[MHA]Epoch 13, Loss: 3.103360876441002 = 2.135994043201208 (normal) + 0.9673668369650841 (augmented)  
[MHA]Epoch 14, Loss: 3.033451795578003 = 2.0988708175718784 (normal) + 0.9345809277147055 (augmented)  
[MHA]Epoch 15, Loss: 2.981283187866211 = 2.066271983087063 (normal) + 0.9150111991912127 (augmented)  
[MHA]Epoch 16, Loss: 2.931223012506962 = 2.0346273444592953 (normal) + 0.8965956438332796 (augmented)  
[MHA]Epoch 17, Loss: 2.8792714923620224 = 2.0047685354948044 (normal) + 0.8745029475539923 (augmented)  
[MHA]Epoch 18, Loss: 2.844299353659153 = 1.9768377169966698 (normal) + 0.8674616292119026 (augmented)  
[MHA]Epoch 19, Loss: 2.792683705687523 = 1.9495267570018768 (normal) + 0.8431569170206785 (augmented)  
[MHA]Epoch 20, Loss: 2.7583628743886948 = 1.9246932975947857 (normal) + 0.8336695563048124 (augmented)  
[MHA]Epoch 21, Loss: 2.71848526597023 = 1.899121817201376 (normal) + 0.8193634487688541 (augmented)  
[MHA]Epoch 22, Loss: 2.6739013344049454 = 1.8763695135712624 (normal) + 0.7975318115204573 (augmented)  
[MHA]Epoch 23, Loss: 2.6343746408820152 = 1.852082185447216 (normal) + 0.782292453572154 (augmented)  
[MHA]Epoch 24, Loss: 2.6004741191864014 = 1.8312015496194363 (normal) + 0.7692725565284491 (augmented)  
[MHA]Epoch 25, Loss: 2.5578889548778534 = 1.8084258399903774 (normal) + 0.7494631242007017 (augmented)  
[MHA]Epoch 26, Loss: 2.5213106721639633 = 1.7884704358875751 (normal) + 0.7328402325510979 (augmented)  
[MHA]Epoch 27, Loss: 2.5018371120095253 = 1.7694325186312199 (normal) + 0.7324045971035957 (augmented)  
[MHA]Epoch 28, Loss: 2.4738001227378845 = 1.750778667628765 (normal) + 0.7230214439332485 (augmented)  
[MHA]Epoch 29, Loss: 2.4289892464876175 = 1.7306527867913246 (normal) + 0.698336448520422 (augmented)  
[MHA]Epoch 30, Loss: 2.39803496748209 = 1.7128376513719559 (normal) + 0.6851973021402955 (augmented)  
[MHA]Epoch 31, Loss: 2.3758021146059036 = 1.6971374340355396 (normal) + 0.6786646610125899 (augmented)  
[MHA]Epoch 32, Loss: 2.359010174870491 = 1.681648001074791 (normal) + 0.6773621831089258 (augmented)  
[MHA]Epoch 33, Loss: 2.331655830144882 = 1.6662520729005337 (normal) + 0.6654037339612842 (augmented)  
[MHA]Epoch 34, Loss: 2.3045251481235027 = 1.6515618078410625 (normal) + 0.6529633393511176 (augmented)  
[MHA]Epoch 35, Loss: 2.2768590822815895 = 1.6380734294652939 (normal) + 0.638785638846457 (augmented)  
[MHA]Epoch 36, Loss: 2.26099868491292 = 1.6274149976670742 (normal) + 0.6335836825892329 (augmented)  
[MHA]Epoch 37, Loss: 2.248332269489765 = 1.6136739514768124 (normal) + 0.6346582965925336 (augmented)  
[MHA]Epoch 38, Loss: 2.2166977673768997 = 1.6032496951520443 (normal) + 0.613448079675436 (augmented)  
[MHA]Epoch 39, Loss: 2.2050238139927387 = 1.5915642641484737 (normal) + 0.6134595535695553 (augmented)  
[MHA]Epoch 40, Loss: 2.1936894953250885 = 1.581680279225111 (normal) + 0.6120092123746872 (augmented)  
[MHA]Epoch 41, Loss: 2.16746499016881 = 1.5660843588411808 (normal) + 0.6013806089758873 (augmented)  
[MHA]Epoch 42, Loss: 2.1366017162799835 = 1.5529371201992035 (normal) + 0.5836645811796188 (augmented)  
[MHA]Epoch 43, Loss: 2.112606056034565 = 1.5391167774796486 (normal) + 0.5734892608597875 (augmented)  
[MHA]Epoch 44, Loss: 2.0970674231648445 = 1.5260058604180813 (normal) + 0.5710615674033761 (augmented)  
[MHA]Epoch 45, Loss: 2.088796593248844 = 1.5172428153455257 (normal) + 0.5715537806972861 (augmented)  
[MHA]Epoch 46, Loss: 2.0618450567126274 = 1.5037310346961021 (normal) + 0.5581140024587512 (augmented)  
[MHA]Epoch 47, Loss: 2.058155871927738 = 1.493938647210598 (normal) + 0.5642172340303659 (augmented)  
[MHA]Epoch 48, Loss: 2.048065721988678 = 1.483213510364294 (normal) + 0.5648521957919002 (augmented)  
[MHA]Epoch 49, Loss: 2.0123273096978664 = 1.4745729677379131 (normal) + 0.537754344753921 (augmented)  
[MHA]Epoch 50, Loss: 2.0024169385433197 = 1.4634594954550266 (normal) + 0.5389574402943254 (augmented)  
[MHA]Epoch 51, Loss: 1.9796813875436783 = 1.4542954079806805 (normal) + 0.5253859730437398 (augmented)  
[MHA]Epoch 52, Loss: 1.9816064201295376 = 1.4474519900977612 (normal) + 0.5341544346883893 (augmented)  
[MHA]Epoch 53, Loss: 1.9681518115103245 = 1.4393271207809448 (normal) + 0.5288246860727668 (augmented)  
[MHA]Epoch 54, Loss: 1.944390345364809 = 1.42899651825428 (normal) + 0.5153938317671418 (augmented)  
[MHA]Epoch 55, Loss: 1.9362810105085373 = 1.4225257523357868 (normal) + 0.5137552563101053 (augmented)  
[MHA]Epoch 56, Loss: 1.9245022162795067 = 1.4106760434806347 (normal) + 0.5138261755928397 (augmented)  
[MHA]Epoch 57, Loss: 1.9041191972792149 = 1.4042702242732048 (normal) + 0.49984895810484886 (augmented)  
[MHA]Epoch 58, Loss: 1.8867809548974037 = 1.3951620049774647 (normal) + 0.4916189396753907 (augmented)  
[MHA]Epoch 59, Loss: 1.8913314379751682 = 1.3893145509064198 (normal) + 0.502016874961555 (augmented)  
[MHA]Epoch 60, Loss: 1.868865743279457 = 1.3795325979590416 (normal) + 0.48933311831206083 (augmented)  
[MHA]Epoch 61, Loss: 1.8619719967246056 = 1.3732772693037987 (normal) + 0.4886947264894843 (augmented)  
[MHA]Epoch 62, Loss: 1.8510808683931828 = 1.365265615284443 (normal) + 0.4858152540400624 (augmented)  
[MHA]Epoch 63, Loss: 1.8319242596626282 = 1.3560201860964298 (normal) + 0.47590404748916626 (augmented)  
[MHA]Epoch 64, Loss: 1.8301702290773392 = 1.3520655184984207 (normal) + 0.47810470685362816 (augmented)  
[MHA]Epoch 65, Loss: 1.8179706148803234 = 1.343885075300932 (normal) + 0.47408553399145603 (augmented)  
[MHA]Epoch 66, Loss: 1.7932468615472317 = 1.335766140371561 (normal) + 0.4574807109311223 (augmented)  
[MHA]Epoch 67, Loss: 1.7872772626578808 = 1.332551471889019 (normal) + 0.4547257851809263 (augmented)  
[MHA]Epoch 68, Loss: 1.7788244970142841 = 1.32607227191329 (normal) + 0.45275220461189747 (augmented)  
[MHA]Epoch 69, Loss: 1.7846748419106007 = 1.3204699233174324 (normal) + 0.4642049092799425 (augmented)  
[MHA]Epoch 70, Loss: 1.7622075229883194 = 1.3150517269968987 (normal) + 0.44715577736496925 (augmented)  
[MHA]Epoch 71, Loss: 1.7487379722297192 = 1.308902069926262 (normal) + 0.43983589205890894 (augmented)  
[MHA]Epoch 72, Loss: 1.7403784319758415 = 1.3047774583101273 (normal) + 0.4356009569019079 (augmented)  
[MHA]Epoch 73, Loss: 1.7401553094387054 = 1.300754051655531 (normal) + 0.4394012372940779 (augmented)  
[MHA]Epoch 74, Loss: 1.7468364834785461 = 1.2963425926864147 (normal) + 0.450493885204196 (augmented)  
[MHA]Epoch 75, Loss: 1.7352397367358208 = 1.2912560421973467 (normal) + 0.4439836936071515 (augmented)  
[MHA]Epoch 76, Loss: 1.727681029587984 = 1.285561554133892 (normal) + 0.4421194735914469 (augmented)  
[MHA]Epoch 77, Loss: 1.7233544699847698 = 1.2794226799160242 (normal) + 0.44393178913742304 (augmented)  
[MHA]Epoch 78, Loss: 1.6954266652464867 = 1.270608115941286 (normal) + 0.4248185409232974 (augmented)  
[MHA]Epoch 79, Loss: 1.6999510265886784 = 1.2696711588650942 (normal) + 0.43027985841035843 (augmented)  
[MHA]Epoch 80, Loss: 1.6871991008520126 = 1.264377124607563 (normal) + 0.4228219538927078 (augmented)  
[MHA]Epoch 81, Loss: 1.6796556524932384 = 1.257990948855877 (normal) + 0.42166469525545835 (augmented)  
[MHA]Epoch 82, Loss: 1.6811566725373268 = 1.256305530667305 (normal) + 0.42485113721340895 (augmented)  
[MHA]Epoch 83, Loss: 1.6674822010099888 = 1.251973032951355 (normal) + 0.4155091494321823 (augmented)  
[MHA]Epoch 84, Loss: 1.6740463748574257 = 1.251808077096939 (normal) + 0.4222382949665189 (augmented)  
[MHA]Epoch 85, Loss: 1.6728161126375198 = 1.2488735970109701 (normal) + 0.4239425025880337 (augmented)  
[MHA]Epoch 86, Loss: 1.6672681272029877 = 1.242318358272314 (normal) + 0.4249497652053833 (augmented)  
[MHA]Epoch 87, Loss: 1.6574092544615269 = 1.24100754968822 (normal) + 0.41640169359743595 (augmented)  
[MHA]Epoch 88, Loss: 1.6405337564647198 = 1.2350404281169176 (normal) + 0.4054933125153184 (augmented)  
[MHA]Epoch 89, Loss: 1.6320415921509266 = 1.2286335602402687 (normal) + 0.4034080095589161 (augmented)  
[MHA]Epoch 90, Loss: 1.6234287060797215 = 1.2245720084756613 (normal) + 0.3988566789776087 (augmented)  
[MHA]Epoch 91, Loss: 1.6220198832452297 = 1.221164358779788 (normal) + 0.4008555095642805 (augmented)  
[MHA]Epoch 92, Loss: 1.6212538443505764 = 1.2149418089538813 (normal) + 0.40631202328950167 (augmented)  
[MHA]Epoch 93, Loss: 1.6124517545104027 = 1.213948991149664 (normal) + 0.39850274939090014 (augmented)  
[MHA]Epoch 94, Loss: 1.6031737104058266 = 1.208410371094942 (normal) + 0.3947633383795619 (augmented)  
[MHA]Epoch 95, Loss: 1.599102783948183 = 1.2054887171834707 (normal) + 0.39361406303942204 (augmented)  
[MHA]Epoch 96, Loss: 1.5961413942277431 = 1.2016659323126078 (normal) + 0.39447544887661934 (augmented)  
[MHA]Epoch 97, Loss: 1.5829626955091953 = 1.1978483274579048 (normal) + 0.3851143568754196 (augmented)  
[MHA]Epoch 98, Loss: 1.5802552849054337 = 1.1959379762411118 (normal) + 0.38431730307638645 (augmented)  
[MHA]Epoch 99, Loss: 1.5723323449492455 = 1.1905465833842754 (normal) + 0.38178574852645397 (augmented)  
[MHA]Epoch 100, Loss: 1.574652012437582 = 1.1881495509296656 (normal) + 0.3865024531260133 (augmented)  
EC  0
Training complete.
Generating data for encoder 5
[nltk_data] Downloading package stopwords to /home/a40-ko-
[nltk_data]     lab/nltk_data...
[nltk_data]   Package stopwords is already up-to-date!
BertModel loaded from: transformers.models.bert.modeling_bert
Transformers module loaded from: /home/a40-ko-lab/miniconda3/envs/jiantenv/lib/python3.9/site-packages/transformers/__init__.py
06/12/2024 13:41:55 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False
06/12/2024 13:41:55 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
batch_eval_metrics=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_steps=None,
eval_strategy=no,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/tmp/cb/runs/Jun12_13-41-55_a40kolab,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=100.0,
optim=adamw_torch,
optim_args=None,
optim_target_modules=None,
output_dir=/tmp/cb/,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=/tmp/cb/,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
[INFO|configuration_utils.py:731] 2024-06-12 13:41:55,602 >> loading configuration file ./downloads/cb_config/config.json
[INFO|configuration_utils.py:800] 2024-06-12 13:41:55,603 >> Model config DistilBertConfig {
  "_name_or_path": "./downloads/cb_config",
  "activation": "gelu",
  "architectures": [
    "DistilBertForMaskedLM"
  ],
  "attention_dropout": 0.1,
  "dim": 768,
  "dropout": 0.1,
  "finetuning_task": "cb",
  "hidden_dim": 3072,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 512,
  "model_type": "distilbert",
  "n_heads": 12,
  "n_layers": 6,
  "pad_token_id": 0,
  "qa_dropout": 0.1,
  "seq_classif_dropout": 0.2,
  "sinusoidal_pos_embds": false,
  "tie_weights_": true,
  "transformers_version": "4.42.0.dev0",
  "vocab_size": 30522
}

[INFO|tokenization_utils_base.py:2107] 2024-06-12 13:41:55,604 >> loading file vocab.txt
[INFO|tokenization_utils_base.py:2107] 2024-06-12 13:41:55,604 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2107] 2024-06-12 13:41:55,604 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2107] 2024-06-12 13:41:55,605 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2107] 2024-06-12 13:41:55,605 >> loading file tokenizer_config.json
[INFO|configuration_utils.py:731] 2024-06-12 13:41:55,620 >> loading configuration file ./downloads/cb_model/config.json
[INFO|configuration_utils.py:800] 2024-06-12 13:41:55,620 >> Model config DistilBertConfig {
  "_name_or_path": "./downloads/cb_model",
  "activation": "gelu",
  "architectures": [
    "DistilBertForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "dim": 768,
  "dropout": 0.1,
  "finetuning_task": "cb",
  "hidden_dim": 3072,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 512,
  "model_type": "distilbert",
  "n_heads": 12,
  "n_layers": 6,
  "pad_token_id": 0,
  "qa_dropout": 0.1,
  "seq_classif_dropout": 0.2,
  "sinusoidal_pos_embds": false,
  "tie_weights_": true,
  "torch_dtype": "float32",
  "transformers_version": "4.42.0.dev0",
  "vocab_size": 30522
}

[INFO|modeling_utils.py:3474] 2024-06-12 13:41:55,646 >> loading weights file ./downloads/cb_model/model.safetensors
[INFO|modeling_utils.py:4283] 2024-06-12 13:41:55,682 >> All model checkpoint weights were used when initializing DistilBertForSequenceClassification.

[INFO|modeling_utils.py:4291] 2024-06-12 13:41:55,682 >> All the weights of DistilBertForSequenceClassification were initialized from the model checkpoint at ./downloads/cb_model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForSequenceClassification for predictions without further training.
Loading cached processed dataset at /media/a40-ko-lab/c61d4ea0-b8fe-4205-9815-f7d3d054043c/sayed/Sayed/transformers/weight_sharing/modular_superglue/downloads/cb/train/cache-ec9a28bf6cd38919.arrow
Loaded dataset from disk
06/12/2024 13:41:55 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /media/a40-ko-lab/c61d4ea0-b8fe-4205-9815-f7d3d054043c/sayed/Sayed/transformers/weight_sharing/modular_superglue/downloads/cb/train/cache-ec9a28bf6cd38919.arrow
Loading cached processed dataset at /media/a40-ko-lab/c61d4ea0-b8fe-4205-9815-f7d3d054043c/sayed/Sayed/transformers/weight_sharing/modular_superglue/downloads/cb/validation/cache-462a1cd6bd454452.arrow
06/12/2024 13:41:55 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /media/a40-ko-lab/c61d4ea0-b8fe-4205-9815-f7d3d054043c/sayed/Sayed/transformers/weight_sharing/modular_superglue/downloads/cb/validation/cache-462a1cd6bd454452.arrow
Loading cached processed dataset at /media/a40-ko-lab/c61d4ea0-b8fe-4205-9815-f7d3d054043c/sayed/Sayed/transformers/weight_sharing/modular_superglue/downloads/cb/test/cache-f44364b0f28c5bd9.arrow
06/12/2024 13:41:55 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /media/a40-ko-lab/c61d4ea0-b8fe-4205-9815-f7d3d054043c/sayed/Sayed/transformers/weight_sharing/modular_superglue/downloads/cb/test/cache-f44364b0f28c5bd9.arrow
06/12/2024 13:41:55 - INFO - __main__ - Sample 163 of the training set: {'input_ids': [101, 1037, 1024, 2031, 2017, 2628, 2008, 2200, 2172, 2030, 1010, 1038, 1024, 7910, 1010, 2025, 2428, 1012, 1045, 2123, 1005, 1056, 2228, 2505, 2097, 2412, 2202, 2058, 1996, 5088, 1012, 102, 2242, 2097, 2202, 2058, 1996, 5088, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
06/12/2024 13:41:55 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:3753] 2024-06-12 13:41:55,906 >> ***** Running Evaluation *****
[INFO|trainer.py:3755] 2024-06-12 13:41:55,906 >>   Num examples = 250
[INFO|trainer.py:3758] 2024-06-12 13:41:55,906 >>   Batch size = 8
Directory './saves/distilbert/distilbert-base-uncased/cb/mha/outputs/encoder_5' created successfully.
Directory './saves/distilbert/distilbert-base-uncased/cb/ffn/inputs/encoder_5' created successfully.
Directory './saves/distilbert/distilbert-base-uncased/cb/ffn/outputs/encoder_5' created successfully.
Directory './saves/distilbert/distilbert-base-uncased/cb/mha/inputs/encoder_5' created successfully.
Batch Idx  1
  0%|          | 0/32 [00:00<?, ?it/s] 16%|█▌        | 5/32 [00:00<00:00, 46.97it/s] 31%|███▏      | 10/32 [00:00<00:00, 43.84it/s] 47%|████▋     | 15/32 [00:00<00:00, 42.99it/s] 62%|██████▎   | 20/32 [00:00<00:00, 41.59it/s] 78%|███████▊  | 25/32 [00:00<00:00, 41.73it/s] 94%|█████████▍| 30/32 [00:00<00:00, 41.88it/s]100%|██████████| 32/32 [00:00<00:00, 43.10it/s]
Batch Idx  2
Batch Idx  3
Batch Idx  4
Batch Idx  5
Batch Idx  6
Batch Idx  7
Batch Idx  8
Batch Idx  9
Batch Idx  10
Batch Idx  11
Batch Idx  12
Batch Idx  13
Batch Idx  14
Batch Idx  15
Batch Idx  16
Batch Idx  17
Batch Idx  18
Batch Idx  19
Batch Idx  20
Batch Idx  21
Batch Idx  22
Batch Idx  23
Batch Idx  24
Batch Idx  25
Batch Idx  26
Batch Idx  27
Batch Idx  28
Batch Idx  29
Batch Idx  30
Batch Idx  31
Batch Idx  32
***** eval metrics *****
  eval_runtime            = 0:00:00.85
  eval_samples            =        250
  eval_samples_per_second =    291.703
  eval_steps_per_second   =     37.338
Data generated succesfully

 
 
 Beginning modular training of SA and BL modules
Encoder compression: 2
Modular training for BL module, encoder 5
LOAD FROM SAVE
Encoder idx = 5
Compression = 2
[FFN] Epoch 1, Loss: 3.1601140797138214 = 1.9780828580260277 (normal) + 1.1820312459021807 (augmented)  
[FFN] Epoch 2, Loss: 2.3238221630454063 = 1.5112516954541206 (normal) + 0.8125704806298018 (augmented)  
[FFN] Epoch 3, Loss: 1.973636344075203 = 1.313605085015297 (normal) + 0.6600312534719706 (augmented)  
[FFN] Epoch 4, Loss: 1.6715332455933094 = 1.1367152146995068 (normal) + 0.5348180271685123 (augmented)  
[FFN] Epoch 5, Loss: 1.4776238091289997 = 1.028422987088561 (normal) + 0.4492007987573743 (augmented)  
[FFN] Epoch 6, Loss: 1.3723550587892532 = 0.9626189060509205 (normal) + 0.40973614528775215 (augmented)  
[FFN] Epoch 7, Loss: 1.3029597587883472 = 0.9072083532810211 (normal) + 0.3957513989880681 (augmented)  
[FFN] Epoch 8, Loss: 1.259974166750908 = 0.8780356291681528 (normal) + 0.3819385292008519 (augmented)  
[FFN] Epoch 9, Loss: 1.2103905826807022 = 0.8523894231766462 (normal) + 0.3580011548474431 (augmented)  
[FFN] Epoch 10, Loss: 1.1780556850135326 = 0.8238876424729824 (normal) + 0.35416802763938904 (augmented)  
[FFN] Epoch 11, Loss: 1.1501065343618393 = 0.8040486443787813 (normal) + 0.3460578825324774 (augmented)  
[FFN] Epoch 12, Loss: 1.1181136928498745 = 0.7836425453424454 (normal) + 0.3344711447134614 (augmented)  
[FFN] Epoch 13, Loss: 1.0933076180517673 = 0.7719130851328373 (normal) + 0.32139452639967203 (augmented)  
[FFN] Epoch 14, Loss: 1.0817860402166843 = 0.7592797465622425 (normal) + 0.3225062945857644 (augmented)  
[FFN] Epoch 15, Loss: 1.0549408998340368 = 0.7423940803855658 (normal) + 0.3125468175858259 (augmented)  
[FFN] Epoch 16, Loss: 1.05072838999331 = 0.7331456691026688 (normal) + 0.31758272740989923 (augmented)  
[FFN] Epoch 17, Loss: 1.0299299452453852 = 0.7189858220517635 (normal) + 0.31094411201775074 (augmented)  
[FFN] Epoch 18, Loss: 1.0184114202857018 = 0.7129712477326393 (normal) + 0.3054401706904173 (augmented)  
[FFN] Epoch 19, Loss: 1.0064200554043055 = 0.7097213603556156 (normal) + 0.2966986997053027 (augmented)  
[FFN] Epoch 20, Loss: 0.9948663227260113 = 0.6971595343202353 (normal) + 0.2977067995816469 (augmented)  
[FFN] Epoch 21, Loss: 0.9862353913486004 = 0.6876025665551424 (normal) + 0.29863282945007086 (augmented)  
[FFN] Epoch 22, Loss: 0.9829127378761768 = 0.6844826601445675 (normal) + 0.29843008052557707 (augmented)  
[FFN] Epoch 23, Loss: 0.9657061286270618 = 0.6781694181263447 (normal) + 0.2875367058441043 (augmented)  
[FFN] Epoch 24, Loss: 0.9645532928407192 = 0.6699960865080357 (normal) + 0.29455720307305455 (augmented)  
[FFN] Epoch 25, Loss: 0.9580768663436174 = 0.6646819915622473 (normal) + 0.29339487105607986 (augmented)  
[FFN] Epoch 26, Loss: 0.953113405033946 = 0.6649534720927477 (normal) + 0.28815993666648865 (augmented)  
[FFN] Epoch 27, Loss: 0.9445975441485643 = 0.6596627812832594 (normal) + 0.2849347619339824 (augmented)  
[FFN] Epoch 28, Loss: 0.9320123698562384 = 0.647344809025526 (normal) + 0.28466755896806717 (augmented)  
[FFN] Epoch 29, Loss: 0.9246391132473946 = 0.6467668451368809 (normal) + 0.2778722643852234 (augmented)  
[FFN] Epoch 30, Loss: 0.9144979529082775 = 0.6368252523243427 (normal) + 0.27767269872128963 (augmented)  
[FFN] Epoch 31, Loss: 0.9139546416699886 = 0.637607105076313 (normal) + 0.2763475300744176 (augmented)  
[FFN] Epoch 32, Loss: 0.9120978713035583 = 0.6302451398223639 (normal) + 0.2818527230992913 (augmented)  
[FFN] Epoch 33, Loss: 0.9102680310606956 = 0.634201243519783 (normal) + 0.2760667805559933 (augmented)  
[FFN] Epoch 34, Loss: 0.9043753761798143 = 0.6277234107255936 (normal) + 0.27665196312591434 (augmented)  
[FFN] Epoch 35, Loss: 0.9010106064379215 = 0.625343831256032 (normal) + 0.2756667770445347 (augmented)  
[FFN] Epoch 36, Loss: 0.8916256241500378 = 0.6155560482293367 (normal) + 0.2760695740580559 (augmented)  
[FFN] Epoch 37, Loss: 0.8923932462930679 = 0.6162669099867344 (normal) + 0.27612633863464 (augmented)  
[FFN] Epoch 38, Loss: 0.8916220478713512 = 0.6183832436800003 (normal) + 0.2732388009317219 (augmented)  
[FFN] Epoch 39, Loss: 0.8952395431697369 = 0.6143297851085663 (normal) + 0.2809097543358803 (augmented)  
[FFN] Epoch 40, Loss: 0.8900969233363867 = 0.6126171946525574 (normal) + 0.27747972728684545 (augmented)  
[FFN] Epoch 41, Loss: 0.8804743159562349 = 0.603892844170332 (normal) + 0.2765814666636288 (augmented)  
[FFN] Epoch 42, Loss: 0.8829592876136303 = 0.6038811728358269 (normal) + 0.2790781152434647 (augmented)  
[FFN] Epoch 43, Loss: 0.8747921604663134 = 0.603429926559329 (normal) + 0.2713622283190489 (augmented)  
[FFN] Epoch 44, Loss: 0.8777310997247696 = 0.6011135736480355 (normal) + 0.27661752235144377 (augmented)  
[FFN] Epoch 45, Loss: 0.8689443841576576 = 0.5998670496046543 (normal) + 0.269077334087342 (augmented)  
[FFN] Epoch 46, Loss: 0.8688047006726265 = 0.594949409365654 (normal) + 0.2738552875816822 (augmented)  
[FFN] Epoch 47, Loss: 0.866892484948039 = 0.5925073549151421 (normal) + 0.2743851291015744 (augmented)  
[FFN] Epoch 48, Loss: 0.8598886243999004 = 0.587530305609107 (normal) + 0.2723583155311644 (augmented)  
[FFN] Epoch 49, Loss: 0.8674532845616341 = 0.5942112812772393 (normal) + 0.2732420046813786 (augmented)  
[FFN] Epoch 50, Loss: 0.8673286996781826 = 0.5908685065805912 (normal) + 0.27646018750965595 (augmented)  
[FFN] Epoch 51, Loss: 0.8568345811218023 = 0.5855595488101244 (normal) + 0.27127503510564566 (augmented)  
[FFN] Epoch 52, Loss: 0.8553462699055672 = 0.5859604757279158 (normal) + 0.26938579231500626 (augmented)  
[FFN] Epoch 53, Loss: 0.8497069627046585 = 0.5803427435457706 (normal) + 0.2693642182275653 (augmented)  
[FFN] Epoch 54, Loss: 0.8513054605573416 = 0.5783258285373449 (normal) + 0.2729796292260289 (augmented)  
[FFN] Epoch 55, Loss: 0.8450932297855616 = 0.5753055941313505 (normal) + 0.2697876296006143 (augmented)  
[FFN] Epoch 56, Loss: 0.8503955956548452 = 0.5785644557327032 (normal) + 0.27183113899081945 (augmented)  
[FFN] Epoch 57, Loss: 0.8484206590801477 = 0.5806245096027851 (normal) + 0.26779614808037877 (augmented)  
[FFN] Epoch 58, Loss: 0.8368914555758238 = 0.5666992357000709 (normal) + 0.27019221521914005 (augmented)  
[FFN] Epoch 59, Loss: 0.8434497658163309 = 0.5712775867432356 (normal) + 0.27217217441648245 (augmented)  
[FFN] Epoch 60, Loss: 0.8514588288962841 = 0.577659877948463 (normal) + 0.27379895094782114 (augmented)  
[FFN] Epoch 61, Loss: 0.8398453556001186 = 0.5728200245648623 (normal) + 0.26702532451599836 (augmented)  
[FFN] Epoch 62, Loss: 0.8353331945836544 = 0.5659635923802853 (normal) + 0.26936960266903043 (augmented)  
[FFN] Epoch 63, Loss: 0.8393159210681915 = 0.5633392361924052 (normal) + 0.2759766881354153 (augmented)  
[FFN] Epoch 64, Loss: 0.8476764000952244 = 0.5743128005415201 (normal) + 0.2733635981567204 (augmented)  
[FFN] Epoch 65, Loss: 0.8369134068489075 = 0.5703903846442699 (normal) + 0.26652301708236337 (augmented)  
[FFN] Epoch 66, Loss: 0.833235714584589 = 0.5621681185439229 (normal) + 0.2710675881244242 (augmented)  
[FFN] Epoch 67, Loss: 0.8309177458286285 = 0.5629799487069249 (normal) + 0.2679377938620746 (augmented)  
[FFN] Epoch 68, Loss: 0.8341651726514101 = 0.5629290714859962 (normal) + 0.2712360955774784 (augmented)  
[FFN] Epoch 69, Loss: 0.8275108989328146 = 0.5563110914081335 (normal) + 0.27119980938732624 (augmented)  
[FFN] Epoch 70, Loss: 0.8261019866913557 = 0.5588472252711654 (normal) + 0.2672547595575452 (augmented)  
[FFN] Epoch 71, Loss: 0.8256717603653669 = 0.5578364990651608 (normal) + 0.26783525850623846 (augmented)  
[FFN] Epoch 72, Loss: 0.8317698445171118 = 0.5609920592978597 (normal) + 0.2707777828909457 (augmented)  
[FFN] Epoch 73, Loss: 0.8275706917047501 = 0.5556885218247771 (normal) + 0.2718821675516665 (augmented)  
[FFN] Epoch 74, Loss: 0.8209627531468868 = 0.553096073679626 (normal) + 0.26786667946726084 (augmented)  
[FFN] Epoch 75, Loss: 0.831976231187582 = 0.5602774331346154 (normal) + 0.27169879572466016 (augmented)  
[FFN] Epoch 76, Loss: 0.8286053370684385 = 0.5514525957405567 (normal) + 0.2771527371369302 (augmented)  
[FFN] Epoch 77, Loss: 0.8257435634732246 = 0.5573078924790025 (normal) + 0.2684356700628996 (augmented)  
[FFN] Epoch 78, Loss: 0.8295886609703302 = 0.555962841026485 (normal) + 0.2736258190125227 (augmented)  
[FFN] Epoch 79, Loss: 0.8147344011813402 = 0.5458355406299233 (normal) + 0.26889885077252984 (augmented)  
[FFN] Epoch 80, Loss: 0.8319365158677101 = 0.5564911188557744 (normal) + 0.27544539142400026 (augmented)  
[FFN] Epoch 81, Loss: 0.8311767112463713 = 0.5550518613308668 (normal) + 0.2761248452588916 (augmented)  
[FFN] Epoch 82, Loss: 0.8168636374175549 = 0.5459785936400294 (normal) + 0.27088504284620285 (augmented)  
[FFN] Epoch 83, Loss: 0.8169138208031654 = 0.547824383713305 (normal) + 0.26908942963927984 (augmented)  
[FFN] Epoch 84, Loss: 0.815875144675374 = 0.5448237750679255 (normal) + 0.2710513644851744 (augmented)  
[FFN] Epoch 85, Loss: 0.8260746896266937 = 0.552246836014092 (normal) + 0.27382785407826304 (augmented)  
[FFN] Epoch 86, Loss: 0.8184228334575891 = 0.543312044814229 (normal) + 0.2751107863150537 (augmented)  
[FFN] Epoch 87, Loss: 0.8244955576956272 = 0.5484834769740701 (normal) + 0.27601207746192813 (augmented)  
[FFN] Epoch 88, Loss: 0.8303321711719036 = 0.551871164701879 (normal) + 0.2784610101953149 (augmented)  
[FFN] Epoch 89, Loss: 0.8202779348939657 = 0.5490401536226273 (normal) + 0.2712377826683223 (augmented)  
[FFN] Epoch 90, Loss: 0.8193691801279783 = 0.5416025584563613 (normal) + 0.27776662074029446 (augmented)  
[FFN] Epoch 91, Loss: 0.8225726317614317 = 0.5425650831311941 (normal) + 0.280007547698915 (augmented)  
[FFN] Epoch 92, Loss: 0.8164650909602642 = 0.5401588240638375 (normal) + 0.2763062627054751 (augmented)  
[FFN] Epoch 93, Loss: 0.8143665790557861 = 0.5377189042046666 (normal) + 0.2766476762481034 (augmented)  
[FFN] Epoch 94, Loss: 0.8199781253933907 = 0.5445030890405178 (normal) + 0.2754750302992761 (augmented)  
[FFN] Epoch 95, Loss: 0.8138797078281641 = 0.5399672072380781 (normal) + 0.27391249872744083 (augmented)  
[FFN] Epoch 96, Loss: 0.828813724219799 = 0.5483712647110224 (normal) + 0.2804424590431154 (augmented)  
[FFN] Epoch 97, Loss: 0.8266188148409128 = 0.5458767544478178 (normal) + 0.28074205899611115 (augmented)  
[FFN] Epoch 98, Loss: 0.8183001130819321 = 0.5390239153057337 (normal) + 0.27927619591355324 (augmented)  
[FFN] Epoch 99, Loss: 0.8202590066939592 = 0.5486693987622857 (normal) + 0.27158960746601224 (augmented)  
[FFN] Epoch 100, Loss: 0.8276702426373959 = 0.5401166742667556 (normal) + 0.28755356557667255 (augmented)  
Training complete.
Modular training for SA module, encoder 5
LOAD FROM SAVE
Encoder idx = 5
Batch count :  32
[MHA]Epoch 1, Loss: 7.286457151174545 = 3.1965966299176216 (normal) + 4.089860588312149 (augmented)  
[MHA]Epoch 2, Loss: 4.334213808178902 = 2.231470175087452 (normal) + 2.1027436032891273 (augmented)  
[MHA]Epoch 3, Loss: 3.865131236612797 = 2.056125458329916 (normal) + 1.8090057633817196 (augmented)  
[MHA]Epoch 4, Loss: 3.6791057735681534 = 1.9786405973136425 (normal) + 1.700465191155672 (augmented)  
[MHA]Epoch 5, Loss: 3.4839722514152527 = 1.9080817811191082 (normal) + 1.5758904442191124 (augmented)  
[MHA]Epoch 6, Loss: 3.3099513202905655 = 1.8323293961584568 (normal) + 1.4776219259947538 (augmented)  
[MHA]Epoch 7, Loss: 3.136953204870224 = 1.7585576511919498 (normal) + 1.378395564854145 (augmented)  
[MHA]Epoch 8, Loss: 2.9848633110523224 = 1.691839162260294 (normal) + 1.2930241376161575 (augmented)  
[MHA]Epoch 9, Loss: 2.8641790822148323 = 1.6325229294598103 (normal) + 1.2316561341285706 (augmented)  
[MHA]Epoch 10, Loss: 2.760765455663204 = 1.5824046209454536 (normal) + 1.1783608347177505 (augmented)  
[MHA]Epoch 11, Loss: 2.6686520650982857 = 1.5386318899691105 (normal) + 1.1300201583653688 (augmented)  
[MHA]Epoch 12, Loss: 2.583705570548773 = 1.5007113069295883 (normal) + 1.082994258031249 (augmented)  
[MHA]Epoch 13, Loss: 2.5080683194100857 = 1.465332929044962 (normal) + 1.0427353866398335 (augmented)  
[MHA]Epoch 14, Loss: 2.4524141773581505 = 1.4341521672904491 (normal) + 1.0182620137929916 (augmented)  
[MHA]Epoch 15, Loss: 2.3871516063809395 = 1.4042991697788239 (normal) + 0.9828524235635996 (augmented)  
[MHA]Epoch 16, Loss: 2.33179697021842 = 1.377092882990837 (normal) + 0.9547040872275829 (augmented)  
[MHA]Epoch 17, Loss: 2.277215998619795 = 1.3514040932059288 (normal) + 0.9258118830621243 (augmented)  
[MHA]Epoch 18, Loss: 2.222621191293001 = 1.3275447860360146 (normal) + 0.8950764127075672 (augmented)  
[MHA]Epoch 19, Loss: 2.1713564954698086 = 1.303802140057087 (normal) + 0.8675543395802379 (augmented)  
[MHA]Epoch 20, Loss: 2.129942163825035 = 1.2815093249082565 (normal) + 0.848432831466198 (augmented)  
[MHA]Epoch 21, Loss: 2.0868495367467403 = 1.2609419114887714 (normal) + 0.825907614082098 (augmented)  
[MHA]Epoch 22, Loss: 2.0509206280112267 = 1.2415112890303135 (normal) + 0.8094093604013324 (augmented)  
[MHA]Epoch 23, Loss: 2.0040081292390823 = 1.2211111336946487 (normal) + 0.7828969862312078 (augmented)  
[MHA]Epoch 24, Loss: 1.9655421003699303 = 1.2026473097503185 (normal) + 0.7628947831690311 (augmented)  
[MHA]Epoch 25, Loss: 1.9231778047978878 = 1.183556454256177 (normal) + 0.7396213468164206 (augmented)  
[MHA]Epoch 26, Loss: 1.8992723934352398 = 1.1668010465800762 (normal) + 0.7324713505804539 (augmented)  
[MHA]Epoch 27, Loss: 1.860277682542801 = 1.1499044839292765 (normal) + 0.7103731902316213 (augmented)  
[MHA]Epoch 28, Loss: 1.8256579749286175 = 1.1342290192842484 (normal) + 0.6914289565756917 (augmented)  
[MHA]Epoch 29, Loss: 1.8026275970041752 = 1.1184800621122122 (normal) + 0.6841475237160921 (augmented)  
[MHA]Epoch 30, Loss: 1.7628711350262165 = 1.1047624852508307 (normal) + 0.6581086358055472 (augmented)  
[MHA]Epoch 31, Loss: 1.736737933009863 = 1.0909418370574713 (normal) + 0.6457960959523916 (augmented)  
[MHA]Epoch 32, Loss: 1.7135890014469624 = 1.076561126857996 (normal) + 0.6370278634130955 (augmented)  
[MHA]Epoch 33, Loss: 1.6879246644675732 = 1.0662539135664701 (normal) + 0.6216707518324256 (augmented)  
[MHA]Epoch 34, Loss: 1.669986791908741 = 1.052558958530426 (normal) + 0.6174278324469924 (augmented)  
[MHA]Epoch 35, Loss: 1.6466581039130688 = 1.0424837600439787 (normal) + 0.6041743252426386 (augmented)  
[MHA]Epoch 36, Loss: 1.6173260062932968 = 1.029937757179141 (normal) + 0.5873882360756397 (augmented)  
[MHA]Epoch 37, Loss: 1.6007040329277515 = 1.0223269183188677 (normal) + 0.5783771183341742 (augmented)  
[MHA]Epoch 38, Loss: 1.5862649269402027 = 1.0095766335725784 (normal) + 0.5766882942989469 (augmented)  
[MHA]Epoch 39, Loss: 1.5679744742810726 = 1.000904619693756 (normal) + 0.5670698545873165 (augmented)  
[MHA]Epoch 40, Loss: 1.5468424148857594 = 0.9932220429182053 (normal) + 0.553620358929038 (augmented)  
[MHA]Epoch 41, Loss: 1.545850533992052 = 0.9897935297340155 (normal) + 0.5560570005327463 (augmented)  
[MHA]Epoch 42, Loss: 1.523532424122095 = 0.9791912883520126 (normal) + 0.5443411250598729 (augmented)  
[MHA]Epoch 43, Loss: 1.5130717381834984 = 0.9739264398813248 (normal) + 0.5391452880576253 (augmented)  
[MHA]Epoch 44, Loss: 1.488528300076723 = 0.9668235443532467 (normal) + 0.5217047543264925 (augmented)  
[MHA]Epoch 45, Loss: 1.4812091328203678 = 0.9596069976687431 (normal) + 0.5216021225787699 (augmented)  
[MHA]Epoch 46, Loss: 1.4685107432305813 = 0.9512164983898401 (normal) + 0.5172942499630153 (augmented)  
[MHA]Epoch 47, Loss: 1.4541641771793365 = 0.9470267929136753 (normal) + 0.5071373851969838 (augmented)  
[MHA]Epoch 48, Loss: 1.4379924647510052 = 0.9335037581622601 (normal) + 0.5044887140393257 (augmented)  
[MHA]Epoch 49, Loss: 1.4077363274991512 = 0.9222176987677813 (normal) + 0.4855186156928539 (augmented)  
[MHA]Epoch 50, Loss: 1.402850590646267 = 0.9177361596375704 (normal) + 0.4851144151762128 (augmented)  
[MHA]Epoch 51, Loss: 1.3914502747356892 = 0.918217409402132 (normal) + 0.4732328625395894 (augmented)  
[MHA]Epoch 52, Loss: 1.37703150883317 = 0.9094895832240582 (normal) + 0.4675419251434505 (augmented)  
[MHA]Epoch 53, Loss: 1.3736532628536224 = 0.9066994413733482 (normal) + 0.46695380844175816 (augmented)  
[MHA]Epoch 54, Loss: 1.3631402105093002 = 0.9044847693294287 (normal) + 0.45865544117987156 (augmented)  
[MHA]Epoch 55, Loss: 1.3661840930581093 = 0.903664980083704 (normal) + 0.4625191008672118 (augmented)  
[MHA]Epoch 56, Loss: 1.3639057911932468 = 0.9019087310880423 (normal) + 0.461997062433511 (augmented)  
[MHA]Epoch 57, Loss: 1.3398776911199093 = 0.8947985041886568 (normal) + 0.44507918786257505 (augmented)  
[MHA]Epoch 58, Loss: 1.3380183801054955 = 0.8862971905618906 (normal) + 0.4517211872152984 (augmented)  
[MHA]Epoch 59, Loss: 1.331454798579216 = 0.883745014667511 (normal) + 0.4477097694762051 (augmented)  
[MHA]Epoch 60, Loss: 1.3288514874875546 = 0.8846014030277729 (normal) + 0.4442500751465559 (augmented)  
[MHA]Epoch 61, Loss: 1.3309913147240877 = 0.8876778353005648 (normal) + 0.4433134589344263 (augmented)  
[MHA]Epoch 62, Loss: 1.3367804177105427 = 0.8898502849042416 (normal) + 0.44693012069910765 (augmented)  
[MHA]Epoch 63, Loss: 1.339293047785759 = 0.8924095388501883 (normal) + 0.4468835103325546 (augmented)  
[MHA]Epoch 64, Loss: 1.3451046608388424 = 0.893080111593008 (normal) + 0.4520245464518666 (augmented)  
[MHA]Epoch 65, Loss: 1.3468517884612083 = 0.8903706669807434 (normal) + 0.45648112054914236 (augmented)  
[MHA]Epoch 66, Loss: 1.325677003711462 = 0.8806295674294233 (normal) + 0.44504743348807096 (augmented)  
[MHA]Epoch 67, Loss: 1.2977056317031384 = 0.863932067528367 (normal) + 0.43377357069402933 (augmented)  
[MHA]Epoch 68, Loss: 1.2599656507372856 = 0.8438535500317812 (normal) + 0.4161120974458754 (augmented)  
[MHA]Epoch 69, Loss: 1.2360686361789703 = 0.8319715484976768 (normal) + 0.40409708954393864 (augmented)  
[MHA]Epoch 70, Loss: 1.2241805531084538 = 0.8288533166050911 (normal) + 0.39532723324373364 (augmented)  
[MHA]Epoch 71, Loss: 1.2502644509077072 = 0.8431341033428907 (normal) + 0.40713033825159073 (augmented)  
[MHA]Epoch 72, Loss: 1.288827359676361 = 0.8608072567731142 (normal) + 0.42802010709419847 (augmented)  
[MHA]Epoch 73, Loss: 1.3212104104459286 = 0.8765539024025202 (normal) + 0.44465651363134384 (augmented)  
[MHA]Epoch 74, Loss: 1.3196254391223192 = 0.869925394654274 (normal) + 0.4497000342234969 (augmented)  
[MHA]Epoch 75, Loss: 1.2782595790922642 = 0.8470431733876467 (normal) + 0.4312163949944079 (augmented)  
[MHA]Epoch 76, Loss: 1.221079619601369 = 0.8171349074691534 (normal) + 0.4039447084069252 (augmented)  
[MHA]Epoch 77, Loss: 1.164779033511877 = 0.7945467215031385 (normal) + 0.3702323054894805 (augmented)  
[MHA]Epoch 78, Loss: 1.1456439923495054 = 0.7825787607580423 (normal) + 0.36306522879749537 (augmented)  
[MHA]Epoch 79, Loss: 1.1242991592735052 = 0.7748826406896114 (normal) + 0.3494165143929422 (augmented)  
[MHA]Epoch 80, Loss: 1.1215390302240849 = 0.771962059661746 (normal) + 0.34957696637138724 (augmented)  
[MHA]Epoch 81, Loss: 1.1165632903575897 = 0.7681615073233843 (normal) + 0.34840178256854415 (augmented)  
[MHA]Epoch 82, Loss: 1.1028514225035906 = 0.764345632866025 (normal) + 0.33850578032433987 (augmented)  
[MHA]Epoch 83, Loss: 1.093108605593443 = 0.7608123235404491 (normal) + 0.33229627925902605 (augmented)  
[MHA]Epoch 84, Loss: 1.0882039349526167 = 0.7561510987579823 (normal) + 0.332052830606699 (augmented)  
[MHA]Epoch 85, Loss: 1.0868693888187408 = 0.7546221166849136 (normal) + 0.3322472698055208 (augmented)  
[MHA]Epoch 86, Loss: 1.08086365647614 = 0.7501942832022905 (normal) + 0.33066937793046236 (augmented)  
[MHA]Epoch 87, Loss: 1.077139848843217 = 0.749189818277955 (normal) + 0.32795002683997154 (augmented)  
[MHA]Epoch 88, Loss: 1.0741935707628727 = 0.7449615336954594 (normal) + 0.3292320352047682 (augmented)  
[MHA]Epoch 89, Loss: 1.060519328340888 = 0.7400053963065147 (normal) + 0.3205139231868088 (augmented)  
[MHA]Epoch 90, Loss: 1.054842445999384 = 0.7387638911604881 (normal) + 0.3160785408690572 (augmented)  
[MHA]Epoch 91, Loss: 1.0573327112942934 = 0.7358169592916965 (normal) + 0.32151574827730656 (augmented)  
[MHA]Epoch 92, Loss: 1.0503871347755194 = 0.7317921668291092 (normal) + 0.3185949553735554 (augmented)  
[MHA]Epoch 93, Loss: 1.0444029681384563 = 0.7285855785012245 (normal) + 0.31581738917157054 (augmented)  
[MHA]Epoch 94, Loss: 1.0344937462359667 = 0.7267623357474804 (normal) + 0.30773141141980886 (augmented)  
[MHA]Epoch 95, Loss: 1.0290993191301823 = 0.7226382996886969 (normal) + 0.30646101385354996 (augmented)  
[MHA]Epoch 96, Loss: 1.0257540717720985 = 0.7218428011983633 (normal) + 0.3039112580008805 (augmented)  
[MHA]Epoch 97, Loss: 1.0248932354152203 = 0.7187792342156172 (normal) + 0.3061139998026192 (augmented)  
[MHA]Epoch 98, Loss: 1.0160865914076567 = 0.7157352492213249 (normal) + 0.30035134218633175 (augmented)  
[MHA]Epoch 99, Loss: 1.0154214594513178 = 0.7131607048213482 (normal) + 0.30226075928658247 (augmented)  
[MHA]Epoch 100, Loss: 1.0086067449301481 = 0.7094376590102911 (normal) + 0.2991690863855183 (augmented)  
EC  0
Training complete.

 
 
 Run glue for job cb100
Modularity : MF
[nltk_data] Downloading package stopwords to /home/a40-ko-
[nltk_data]     lab/nltk_data...
[nltk_data]   Package stopwords is already up-to-date!
06/12/2024 13:42:38 - INFO - __main__ - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: no

06/12/2024 13:42:38 - INFO - __main__ - 


 SEED MANUALLY SET TO 42 



loading configuration file ./downloads/cb_config/config.json
Model config DistilBertConfig {
  "_name_or_path": "./downloads/cb_config",
  "activation": "gelu",
  "architectures": [
    "DistilBertForMaskedLM"
  ],
  "attention_dropout": 0.1,
  "dim": 768,
  "dropout": 0.1,
  "finetuning_task": "cb",
  "hidden_dim": 3072,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 512,
  "model_type": "distilbert",
  "n_heads": 12,
  "n_layers": 6,
  "pad_token_id": 0,
  "qa_dropout": 0.1,
  "seq_classif_dropout": 0.2,
  "sinusoidal_pos_embds": false,
  "tie_weights_": true,
  "transformers_version": "4.42.0.dev0",
  "vocab_size": 30522
}

loading file vocab.txt
loading file tokenizer.json
loading file added_tokens.json
loading file special_tokens_map.json
loading file tokenizer_config.json
loading configuration file ./downloads/cb_model/config.json
Model config DistilBertConfig {
  "_name_or_path": "./downloads/cb_model",
  "activation": "gelu",
  "architectures": [
    "DistilBertForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "dim": 768,
  "dropout": 0.1,
  "finetuning_task": "cb",
  "hidden_dim": 3072,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 512,
  "model_type": "distilbert",
  "n_heads": 12,
  "n_layers": 6,
  "pad_token_id": 0,
  "qa_dropout": 0.1,
  "seq_classif_dropout": 0.2,
  "sinusoidal_pos_embds": false,
  "tie_weights_": true,
  "torch_dtype": "float32",
  "transformers_version": "4.42.0.dev0",
  "vocab_size": 30522
}

loading weights file ./downloads/cb_model/model.safetensors
All model checkpoint weights were used when initializing DistilBertForSequenceClassification.

All the weights of DistilBertForSequenceClassification were initialized from the model checkpoint at ./downloads/cb_model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForSequenceClassification for predictions without further training.
06/12/2024 13:42:38 - INFO - __main__ - Sample 163 of the training set: {'input_ids': [101, 1037, 1024, 2031, 2017, 2628, 2008, 2200, 2172, 2030, 1010, 1038, 1024, 7910, 1010, 2025, 2428, 1012, 1045, 2123, 1005, 1056, 2228, 2505, 2097, 2412, 2202, 2058, 1996, 5088, 1012, 102, 2242, 2097, 2202, 2058, 1996, 5088, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 1}.
06/12/2024 13:42:38 - INFO - __main__ - Sample 28 of the training set: {'input_ids': [101, 2002, 2018, 2464, 2242, 1045, 2323, 2031, 1010, 2029, 2001, 1037, 2482, 3810, 1999, 2013, 23771, 2675, 1998, 2746, 2039, 2369, 2033, 1012, 2026, 2157, 3329, 18190, 2058, 1996, 23468, 15749, 1998, 1045, 12042, 9143, 2006, 1996, 15357, 1012, 1045, 2347, 1005, 1056, 2004, 6427, 2004, 15451, 15194, 2008, 11265, 14762, 2001, 2041, 1997, 7386, 1005, 1055, 2126, 1012, 102, 11265, 14762, 2001, 2041, 1997, 7386, 1005, 1055, 2126, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 1}.
06/12/2024 13:42:38 - INFO - __main__ - Sample 6 of the training set: {'input_ids': [101, 2059, 1996, 4223, 1999, 1996, 9201, 2150, 3143, 1012, 24185, 4014, 3592, 2105, 2032, 1998, 2059, 3402, 2007, 1037, 5245, 1997, 2010, 4777, 2992, 2370, 2046, 1996, 2250, 1010, 2357, 1010, 1998, 5565, 2702, 2519, 2185, 2006, 1996, 2067, 1997, 1037, 2665, 6847, 1012, 13675, 13910, 5289, 2071, 2156, 2008, 2002, 2001, 4452, 1998, 2008, 2010, 3571, 2001, 2437, 2032, 16668, 9662, 1012, 102, 24185, 4014, 2001, 4452, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 0}.
/home/a40-ko-lab/miniconda3/envs/jiantenv/lib/python3.9/site-packages/accelerate/accelerator.py:595: FutureWarning: The `use_fp16` property is deprecated and will be removed in version 1.0 of Accelerate use `Accelerator.mixed_precision == 'fp16'` instead.
  warnings.warn(
06/12/2024 13:42:38 - INFO - __main__ - ***** Running training *****
06/12/2024 13:42:38 - INFO - __main__ -   Num examples = 250
06/12/2024 13:42:38 - INFO - __main__ -   Num Epochs = 18
06/12/2024 13:42:38 - INFO - __main__ -   Instantaneous batch size per device = 32
06/12/2024 13:42:38 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 32
06/12/2024 13:42:38 - INFO - __main__ -   Gradient Accumulation steps = 1
06/12/2024 13:42:38 - INFO - __main__ -   Total optimization steps = 144
Loaded dataset from disk
LOAD FROM SAVE





	Original model param count :  66955779
	NEw model param count      :  45705987
31.737054392272846






  0%|          | 0/144 [00:00<?, ?it/s]  1%|          | 1/144 [00:00<00:39,  3.59it/s]  2%|▏         | 3/144 [00:00<00:18,  7.45it/s]  3%|▎         | 5/144 [00:00<00:15,  9.16it/s]  5%|▍         | 7/144 [00:00<00:13, 10.07it/s]06/12/2024 13:42:39 - INFO - __main__ - [EVAL] epoch 0: {'accuracy': 0.6428571428571429, 'f1': 0.4362934362934363}
  6%|▋         | 9/144 [00:01<00:14,  9.17it/s]  8%|▊         | 11/144 [00:01<00:13,  9.92it/s]  9%|▉         | 13/144 [00:01<00:12, 10.43it/s] 10%|█         | 15/144 [00:01<00:11, 10.77it/s]06/12/2024 13:42:40 - INFO - __main__ - [EVAL] epoch 1: {'accuracy': 0.6785714285714286, 'f1': 0.47391812865497074}
 12%|█▏        | 17/144 [00:01<00:12, 10.24it/s] 13%|█▎        | 19/144 [00:01<00:11, 10.64it/s] 15%|█▍        | 21/144 [00:02<00:11, 10.90it/s] 16%|█▌        | 23/144 [00:02<00:10, 11.08it/s]06/12/2024 13:42:41 - INFO - __main__ - [EVAL] epoch 2: {'accuracy': 0.7142857142857143, 'f1': 0.4997635933806146}
 17%|█▋        | 25/144 [00:02<00:11, 10.46it/s] 19%|█▉        | 27/144 [00:02<00:10, 10.79it/s] 20%|██        | 29/144 [00:02<00:10, 11.01it/s] 22%|██▏       | 31/144 [00:03<00:10, 11.16it/s]06/12/2024 13:42:42 - INFO - __main__ - [EVAL] epoch 3: {'accuracy': 0.6964285714285714, 'f1': 0.4872881355932203}
 23%|██▎       | 33/144 [00:03<00:10, 10.50it/s] 24%|██▍       | 35/144 [00:03<00:10, 10.82it/s] 26%|██▌       | 37/144 [00:03<00:09, 11.02it/s] 27%|██▋       | 39/144 [00:03<00:09, 11.16it/s]06/12/2024 13:42:42 - INFO - __main__ - [EVAL] epoch 4: {'accuracy': 0.6964285714285714, 'f1': 0.4868139700641483}
 28%|██▊       | 41/144 [00:03<00:09, 10.55it/s] 30%|██▉       | 43/144 [00:04<00:09, 10.84it/s] 31%|███▏      | 45/144 [00:04<00:08, 11.05it/s] 33%|███▎      | 47/144 [00:04<00:08, 11.20it/s]06/12/2024 13:42:43 - INFO - __main__ - [EVAL] epoch 5: {'accuracy': 0.6964285714285714, 'f1': 0.48581048581048575}
 34%|███▍      | 49/144 [00:04<00:09, 10.53it/s] 35%|███▌      | 51/144 [00:04<00:08, 10.86it/s] 37%|███▋      | 53/144 [00:05<00:08, 11.07it/s] 38%|███▊      | 55/144 [00:05<00:07, 11.22it/s]06/12/2024 13:42:44 - INFO - __main__ - [EVAL] epoch 6: {'accuracy': 0.6785714285714286, 'f1': 0.47356161192639173}
 40%|███▉      | 57/144 [00:05<00:08, 10.57it/s] 41%|████      | 59/144 [00:05<00:07, 10.88it/s] 42%|████▏     | 61/144 [00:05<00:07, 11.08it/s] 44%|████▍     | 63/144 [00:05<00:07, 11.22it/s]06/12/2024 13:42:45 - INFO - __main__ - [EVAL] epoch 7: {'accuracy': 0.6964285714285714, 'f1': 0.48434886499402624}
 45%|████▌     | 65/144 [00:06<00:07, 10.56it/s] 47%|████▋     | 67/144 [00:06<00:07, 10.84it/s] 48%|████▊     | 69/144 [00:06<00:06, 11.06it/s] 49%|████▉     | 71/144 [00:06<00:06, 11.20it/s]06/12/2024 13:42:45 - INFO - __main__ - [EVAL] epoch 8: {'accuracy': 0.6964285714285714, 'f1': 0.48557919621749407}
 51%|█████     | 73/144 [00:06<00:06, 10.54it/s] 52%|█████▏    | 75/144 [00:07<00:06, 10.86it/s] 53%|█████▎    | 77/144 [00:07<00:06, 11.06it/s] 55%|█████▍    | 79/144 [00:07<00:05, 11.20it/s]06/12/2024 13:42:46 - INFO - __main__ - [EVAL] epoch 9: {'accuracy': 0.6964285714285714, 'f1': 0.48557919621749407}
 56%|█████▋    | 81/144 [00:07<00:05, 10.56it/s] 58%|█████▊    | 83/144 [00:07<00:05, 10.84it/s] 59%|█████▉    | 85/144 [00:07<00:05, 11.05it/s] 60%|██████    | 87/144 [00:08<00:05, 11.21it/s]06/12/2024 13:42:47 - INFO - __main__ - [EVAL] epoch 10: {'accuracy': 0.6785714285714286, 'f1': 0.4726718273516303}
 62%|██████▏   | 89/144 [00:08<00:05, 10.54it/s] 63%|██████▎   | 91/144 [00:08<00:04, 10.86it/s] 65%|██████▍   | 93/144 [00:08<00:04, 11.07it/s] 66%|██████▌   | 95/144 [00:08<00:04, 11.20it/s]06/12/2024 13:42:47 - INFO - __main__ - [EVAL] epoch 11: {'accuracy': 0.7142857142857143, 'f1': 0.4977429318127821}
 67%|██████▋   | 97/144 [00:09<00:04, 10.56it/s] 69%|██████▉   | 99/144 [00:09<00:04, 10.86it/s] 70%|███████   | 101/144 [00:09<00:03, 11.07it/s] 72%|███████▏  | 103/144 [00:09<00:03, 11.16it/s]06/12/2024 13:42:48 - INFO - __main__ - [EVAL] epoch 12: {'accuracy': 0.6964285714285714, 'f1': 0.48557919621749407}
 73%|███████▎  | 105/144 [00:09<00:03, 10.55it/s] 74%|███████▍  | 107/144 [00:10<00:03, 10.84it/s] 76%|███████▌  | 109/144 [00:10<00:03, 11.05it/s] 77%|███████▋  | 111/144 [00:10<00:02, 11.21it/s]06/12/2024 13:42:49 - INFO - __main__ - [EVAL] epoch 13: {'accuracy': 0.6428571428571429, 'f1': 0.4488888888888889}
 78%|███████▊  | 113/144 [00:10<00:02, 10.56it/s] 80%|███████▉  | 115/144 [00:10<00:02, 10.87it/s] 81%|████████▏ | 117/144 [00:10<00:02, 11.08it/s] 83%|████████▎ | 119/144 [00:11<00:02, 11.21it/s]06/12/2024 13:42:50 - INFO - __main__ - [EVAL] epoch 14: {'accuracy': 0.7142857142857143, 'f1': 0.4977429318127821}
 84%|████████▍ | 121/144 [00:11<00:02, 10.55it/s] 85%|████████▌ | 123/144 [00:11<00:01, 10.87it/s] 87%|████████▋ | 125/144 [00:11<00:01, 11.06it/s] 88%|████████▊ | 127/144 [00:11<00:01, 11.21it/s]06/12/2024 13:42:50 - INFO - __main__ - [EVAL] epoch 15: {'accuracy': 0.6964285714285714, 'f1': 0.48557919621749407}
 90%|████████▉ | 129/144 [00:12<00:01, 10.56it/s] 91%|█████████ | 131/144 [00:12<00:01, 10.86it/s] 92%|█████████▏| 133/144 [00:12<00:00, 11.07it/s] 94%|█████████▍| 135/144 [00:12<00:00, 11.20it/s]06/12/2024 13:42:51 - INFO - __main__ - [EVAL] epoch 16: {'accuracy': 0.6428571428571429, 'f1': 0.4488888888888889}
 95%|█████████▌| 137/144 [00:12<00:00, 10.55it/s] 97%|█████████▋| 139/144 [00:12<00:00, 10.87it/s] 98%|█████████▊| 141/144 [00:13<00:00, 11.06it/s] 99%|█████████▉| 143/144 [00:13<00:00, 11.19it/s]06/12/2024 13:42:52 - INFO - __main__ - [EVAL] epoch 17: {'accuracy': 0.6428571428571429, 'f1': 0.4488888888888889}
Configuration saved in /tmp/cb/config.json
Model weights saved in /tmp/cb/model.safetensors
tokenizer config file saved in /tmp/cb/tokenizer_config.json
Special tokens file saved in /tmp/cb/special_tokens_map.json
100%|██████████| 144/144 [00:13<00:00, 10.41it/s]
